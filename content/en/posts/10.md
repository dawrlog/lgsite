---
title: "BigQuery Data Types Examined & Explained"
date: 2022-01-12
# weight: 1
# aliases: ["/first"]
tags: ["Data Warehouse", "GCP", "BigQuery"]
categories: ["Data warehouse", "Data Modeling", "Cloud"]
# series: ["Data Stack"]
author: "Daniel Paes"
# author: ["Me", "You"] # multiple authors
showToc: true
TocOpen: false
draft: false
hidemeta: false
comments: false
canonicalURL: "https://blog.panoply.io/bigquery-data-types"
disableHLJS: true # to disable highlightjs
disableShare: false
disableHLJS: false
hideSummary: false
searchHidden: false
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: false
ShowRssButtonInSectionTermList: true
UseHugoToc: true
cover:
    image: "https://user-images.githubusercontent.com/78096758/217090752-4965c510-831d-40c4-9049-c46428495a81.png" 
---

Hey everyone, today I would like to talk about Bigquery, the datawarehouse managed service by Google Cloud. And I would like to explore its datatypes.

Google Cloud Platform has long been the cloud provider of choice for web analytics, with the impressive BigQuery being Google's [massively parallel processing](https://searchdatamanagement.techtarget.com/definition/MPP-database-massively-parallel-processing-database) (aka MPP) option. MPP databases are very common on [Hadoop ecosystems](https://hadoop.apache.org/).

BigQuery is very friendly to its users (let's agree that BigQuery is as pleasant as its SQL knowledge), which fosters new users. In part, thanks to a more detailed exploration of Google Analytics data, as described on [this example](https://support.google.com/analytics/answer/3437618?hl=en).

This article aims to present BigQuery and its existing data types per SQL categories. I'll also give one practical use case where knowing these data types can be helpful: data verification when ingesting data.

Knowing which kind of BigQuery data type to use will help you create your data pipelines more easily. Once you better understand what data types are available, the quality of your reports will drastically increase in no time.
<br /><br />


## <h2>SQL types in BigQuery</h2><br />
BigQuery operates using structured query language (SQL), subdivided into legacy and standard categories.

It's good to point out that each subgroup has different data types and ways of questioning the database. When the hint is not informed, BigQuery will use standard SQL functions and methods. Check out what I am talking about at [SQL hints](https://en.wikipedia.org/wiki/Hint_(SQL)) if you are new to it and want to learn a little more about them.

To present what a legacy query looks like, I'll show a running example of a SQL command using the legacy category type.

<br />

**SQL hint example**

```sql
#legacySQL -- This hint informs that the query will use Legacy SQL  
SELECT
weight_pounds,
state,
year,
gestation_weeks
FROM [bigquery-public-data:samples.natality]
ORDER BY weight_pounds DESC;
``` 

Standard SQL is the type that you should be aiming to use when you don't have any constraints, such as legacy code on your [lift-and-shift](https://aws.amazon.com/blogs/architecture/optimizing-a-lift-and-shift-for-performance/) deployment.

Standard SQL also allows very nifty features like window functions and a more robust user-defined functions structure, and it's more flexible to create on standard SQL rather than legacy SQL.

It's also recommended, as best practice, to migrate your existing legacy SQL into standard SQL.

Google even offers a helpful page with guidance on this conversion. You can check out how to start it correctly at [this page](https://cloud.google.com/bigquery/docs/reference/standard-sql/migrating-from-legacy-sql).

## <h2>Standard SQL data types</h2> <br />
The below table presents the possible data types when using the standard SQL category. I like to point out that standard SQL is the preferred category by Google, which means you can correctly assume that it has better performance and more options when compared to the legacy SQL category.

![datatypes](https://user-images.githubusercontent.com/78096758/217090962-60ccc9db-b597-4ec0-86d2-8a82ceafa801.png)
_Bigquery Standard SQL Datatypes_
<br /><br />

## <h2>Standard SQL data types</h2><br />

The standard SQL category accepts **more complex data types** such as **ARRAY**, **STRUCT**, and **GEOGRAPHY** data types. Any of the mentioned data types can order (or group) the results of any dataset; one of its severe limitations as the data types **STRUCT** and **ARRAY** are heavily used on streaming [data ingestions](https://whatis.techtarget.com/definition/data-ingestion).

Another example comes from the **ARRAY** data type not comparing its values with any field, which happens in **STRUCT** and **GEOGRAPHY**.

We can use [ST_EQUALS](https://cloud.google.com/bigquery/docs/reference/standard-sql/geography_functions#st_equals) as a "workaround" of comparing geographical values directly.

All other data types can filter **SQL JOIN** clauses and be used to order the results; always remember to cast the columns used to avoid unwanted effects explicitly.

Standard SQL also allows the usage of what is called ***stored procedures***.

The stored procedure allows the execution of **repeatable functions**—very useful for shareable business logic transformations between different departments.

For example, how the human resources department calculates profit benefits could be useful for the marketing department for calculating campaigns.

The benefit of **well-defined data formats** starts with your stored procedures—as options on the earlier stage of your analytics pipelines empower your analysts with a shorter reaction time to analyze your data.


## <h2>Legacy SQL data types</h2><br />

Legacy SQL uses the same data types as those used with standard SQL, excluding **ARRAY**, **STRUCT**, and **GEOGRAPHY**.

The **NUMERIC** type fields provide limited support, making it necessary to use an explicit conversion using the [cast function](https://cloud.google.com/bigquery/docs/reference/legacy-sql#cast) when interacting with these fields.

The table below lists all the possible data types available when using the legacy SQL query category.

You can still access nested data using the [dot notation](https://en.wikipedia.org/wiki/Property_(programming)#Dot_notation), but it doesn't allow nice tricks like generating your array on the runtime.

Legacy SQL will enable you to create reusable, shareable functions between different queries. It grants this possibility with [user-defined functions](https://cloud.google.com/bigquery/user-defined-functions) (or UDFs); below, you have an example of a simple one.

UDF on legacy SQL
```js
// UDF definition
function urlDecode(row, emit) {
emit({title: decodeHelper(row.title),
       requests: row.num_requests});
}
 
// Helper function with error handling
function decodeHelper(s) {
try {
   return decodeURI(s);
} catch (ex) {
   return s;
}
}

// UDF registration

bigquery.defineFunction('urlDecode', // Name used to call the function from SQL

['title', 'num_requests'], // Input column names

// JSON representation of the output schema
[{name: 'title', type: 'string'},{name: 'requests', type: 'integer'}],

urlDecode // The function reference
);
``` 

So, since it has fewer available data types and some limitations like not being able to create shareable business logic as the standard SQL category does, the legacy SQL category is not a viable option.
<br /><br />

## <h2>Validating the target data type when inserting.</h2><br />

To have a better understanding of the data types, let's look at some code.

We'll tackle the `insert` statement in the standard SQL category since it's the suggested category by the documentation, with a focus on the `STRUCT` data type. This data type can be challenging and very common when ingesting data from REST API payloads.

Also, I believe you might get bored with manipulations with `Integers` and `Strings` only. The following command inserts some data into the table `DetailedInventory` under the schema `dataset`.

The following SQL statement, written using standard SQL, will insert values on the mentioned table with some **STRUCT** types.

Insert statement

```sql
INSERT dataset.DetailedInventory VALUES
('top load washer', 10, FALSE, [(CURRENT_DATE, "comment1")], ("white","1 year",(30,40,28))),
('front load washer', 20, FALSE, [(CURRENT_DATE, "comment1")], ("beige","1 year",(35,45,30)));
```

As simple as demonstrated, it inserts without any complexity (you can see how the inserted records look below).

Insert results
![ins-res](https://user-images.githubusercontent.com/78096758/217091354-471e4a7d-72f2-49b8-9db6-8608fc442d3d.png)

When interacting with your data, you need to be aware of ***handling each data type properly***.

One common mistake comes from comparing the time and time stamp data formats without the correct care. Although the two data types might resemble one another a lot, this mistake can result in inaccurate datasets.

Also, confirm that the function that you're using is under the ***right Bigquery SQL category***.

One good example is the `cast` function under [legacy SQL](https://cloud.google.com/bigquery/docs/reference/legacy-sql#cast) and its reference under [standard SQL](https://cloud.google.com/bigquery/docs/reference/standard-sql/functions-and-operators#cast), so know your terrain before making any changes to your code.

## <h2>Conclusion</h2><br />
As we saw, the type of SQL used can block the usage of some data types.

Due to different ways of processing the data, there is a significant difference between ***legacy SQL*** and ***standard SQL***, making things far more complex than a simple "hint" at the beginning of the SQL.

Utilizing the correct data type can help you audit your data in its earlier stages. This could mean adding an automated reaction to insufficiently formatted data, saving your production support team some investigation. As the same response could take hours to identify the root problem, in addition to the time it takes to solve it once identified.

Sometimes you may even need to apply some predefined rules to treat your data based on learned processing problems.

***Standard SQL should be preferred over the use of legacy SQL.***

The latter doesn't have cool tricks such as [windowing functions](https://cloud.google.com/bigquery/docs/reference/standard-sql/analytic-function-concepts) or [better lexical support](https://cloud.google.com/bigquery/docs/reference/standard-sql/lexical) when creating your SQL statements (way better than the simple dot notation from legacy SQL).

This intel is killer when analyzing why your data still has bottlenecks during its ingestion.

For further help transforming your data, you can [reach out to us](https://calendly.com/dawrlog) with any questions you have and even discover new info you weren't aware of with a better insight from better understanding your data.

See you guys next time! 

Article originally posted at [Panoply blog](https://blog.panoply.io/bigquery-data-types)