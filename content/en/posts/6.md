---
title: "Top 10 Argo Workflows Examples."
date: 2022-07-12
# weight: 1
# aliases: ["/first"]
tags: ["Kubernetes", "Argo", "Development", "Infrastructure Observability"]
categories: ["Kubernetes", "Development", "Data Warehouse"]
# series: ["Data Stack"]
author: "Daniel Paes"
ShowCodeCopyButtons: true
showToc: true
TocOpen: false
draft: false
hidemeta: false
comments: false
canonicalURL: "https://pipekit.io/blog/top-10-argo-workflows-examples"
disableHLJS: true # to disable highlightjs
# disableHLJS: false
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: false
ShowRssButtonInSectionTermList: true
UseHugoToc: true
cover:
    image: "https://user-images.githubusercontent.com/78096758/217081520-1f3b6771-8cf8-434b-b8fe-d11c17d380b5.png"

---
We all know how [Argo Workflows](https://argoproj.github.io/argo-workflows/) makes it easy to orchestrate parallel jobs on Kubernetes. While it’s most often associated with data processing and ETL projects, it’s useful for a lot more! These 10 workflows will change the way you see this Kubernetes orchestrator. 

Let’s dive in!

## <h2>Argo Workflows Setup</h2><br />
If you don't currently have a workflow running, I suggest you create your first Argo Workflow to understand what we'll discuss in this post. To do so, follow the instructions [here](https://argoproj.github.io/argo-workflows/quick-start/) to create a local Argo Workflows deployment on your cluster. I also suggest using k3d for your local Kubernetes control plane; this tutorial uses a [k3d](https://k3d.io/v5.3.0/#install-script) cluster named argo. Feel free to reproduce the command below to create it in your environment:

```sh
k3d cluster create argo
kubectl create -n argo -f https://raw.githubusercontent.com/argoproj/argo-workflows/master/manifests/quick-start-postgres.yaml
kubectl -n argo port-forward deployment/argo-server 2746:2746 & 
```
Now let's jump into looking at our first example! 

## <h2>1. Enhancing Your Workflow Using Parameters</h2><br />
Argo uses [custom resource definitions](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) stored on YAML files to manage its deployments. So no need to learn new specs to manage your infrastructure; you can follow the same pattern used on your Kubernetes and [Kustomize](https://kustomize.io/) scripts, which helps you remain consistent. Below we can see how to use parameters on your workflows, and passing parameters is handy when your configuration uses runtime values. As a result, you will know some components only after creating them, such as [access tokens](https://en.wikipedia.org/wiki/Access_token).

```sh
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: hello-world-parameters-
spec:
  entrypoint: whalesay
  arguments:
    parameters:
    - name: message
      value: Message string default value 
  templates:
  - name: whalesay
    inputs:
      parameters:
      - name: message
    container:
      image: docker/whalesay
      command: [cowsay]
      args: ["{{inputs.parameters.message}}"] 
```

In our template, the parameter `message` will have the default value of `Message string default value`. However, this value can be overwritten at runtime, as we can see by running the command below:


```sh 
argo submit -n argo param.yml -p message="This ran from your container" --watch 
```

We can validate the output from the Argo Workflows Logs UI. (You can access the UI by default at `https://localhost:2746/` if you quickly follow the port forwarding instructions while creating your cluster.)


## <h2>2. Pulling Images From Your Secured Repository</h2><br />
One of the features I like when automating an ecosystem is using rotational access keys while managing my services' access. This is useful in cases where your company uses private container repositories to host your container images. Argo Workflows helps you achieve this with the native support of Kubernetes secrets. In our example, we can see that the secret `docker-registry-secret` will pull the image `docker/whalesay:latest`.

```sh
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: hello-world-
spec:
  entrypoint: whalesay
  imagePullSecrets:
  - name: docker-registry-secret
  templates:
  - name: whalesay
    container:
      image: docker/whalesay:latest
      command: [cowsay]
      args: ["hello world"] 
```

## <h2>3. Using Sidecar Containers </h2><br />
One of my favorite things to do is to use sidecars while starting my pods. Kubernetes sidecars are useful helpers that can handle recurring tasks, such as syncing your Git repositories with `git-sync` as shown [here](https://github.com/kubernetes/git-sync). Argo Workflows has this covered with neat support for sidecar containers out of the box.

```sh 
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: sidecar-nginx-
spec:
  entrypoint: sidecar-nginx-example
  templates:
  - name: sidecar-nginx-example
    container:
      image: appropriate/curl
      command: [sh, -c]
      args: ["until `curl -G 'http://127.0.0.1/' >& /tmp/out`; do echo sleep && sleep 1; done && cat /tmp/out"]
    sidecars:
    - name: nginx
      image: nginx:1.13 
```

To deploy it, save the above code as `sidecar-nginx.yml` and submit it:

```sh
argo submit -n argo sidecar-nginx.yml --watch 
```

And as a result, you'll deploy an NGINX's reverse proxy sidecar instance.

_**Pro-tip: You might need to pay some extra attention to your workflows if you’re using [Istio](https://istio.io/) as Service mesh. Check out at this GitHub thread if you're thinking of using it. **_

## <h2>4. Archiving Your Current Workflow State on Persistent Storage</h2><br />
Workflow Archive is a nice feature that Argo Workflows provides so you can have previous workflow states stored on a relational database (Postgres or MySQL for now). However, Argo's archive won't keep detailed execution logs; you'll need to configure an Object storage as [artifact repository](https://argoproj.github.io/argo-workflows/configure-artifact-repository/), it can be an opensource option like [MinIO](https://min.io/), or [AWS S3](https://aws.amazon.com/s3/); to mention a cloud provider option from AWS. 

To use the archive feature, you'll first need to configure your Argo server's persistent storage option. You'll need more information about how to configure it to do so. Following this link will help you with the authentication piece required for the Argo archive; then base your configuration on [Controller configmap](https://argoproj.github.io/argo-workflows/workflow-controller-configmap/). You'll need to have them appropriately configured with your Argo server to benefit from this feature. Once it's configured, you can store your workflows with the `spec.archiveLocation.archiveLogs` as demonstrated below.

```sh 
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: archive-location-
spec:
  entrypoint: whalesay
  templates:
  - name: whalesay
    container:
      image: docker/whalesay:latest
      command: [cowsay]
      args: ["hello world"]
    archiveLocation:
      archiveLogs: true 
```
## <h2>5. Passing a Git Repository as an Input Artifact</h2><br />
Another cool feature that Argo Workflows provides out of the box is the possibility to sync your Git repository without the need for extra sidecars or init containers. The code below connects to the [Argo repository](https://github.com/argoproj/argo-workflows.git) on Github. You can choose from HTTP or SSH pull requests for the authentication piece. In the first template, git-clone, you'll need to use the combination of usernameSecret and passwordSecret Kubernetes secrets to access a URL in its HTTP format. You can see an example of an HTTP Git configuration in the code below.

```sh
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: input-artifact-git-
spec:
  entrypoint: git-clone
  templates:
  - name: git-clone
    inputs:
      artifacts:
      - name: argo-source
        path: /src
        git:
          repo: https://github.com/argoproj/argo-workflows.git
          revision: "v2.1.1"
          usernameSecret:
            name: github-creds
            key: username
          passwordSecret:
            name: github-creds
            key: password
    container:
      image: golang:1.10
      command: [sh, -c]
      args: ["git status && ls && cat VERSION"]
      workingDir: /src 
```

Argo Workflows also supports SSH connectivity (e.g., `git@github.com:argoproj/argo-workflows.git`). However, it needs the URL format following the SSH connectivity and the `sshPrivateKeySecret` Kubernetes secret instead of the `usernameSecret` and `passwordSecret` ones. 


## <h2>6. Creating Directed Acyclic Graph Workflows</h2><br />

I feel the directed acyclic graph (DAG) is now getting the attention it deserves on the analytics domains because of how it impressively handles data processing workload steps on [Apache Spark](https://spark.apache.org/docs/3.2.1/index.html) and its use as a common data orchestration pattern with Apache Airflow. With Argo Workflows, you'll have a Kubernetes-friendly interface instead of the need to configure a Kubernetes executor for Airflow which is less stable. 

I suggest checking this [link](https://www.techopedia.com/definition/5739/directed-acyclic-graph-dag) to learn more about how a DAG works. Below, you can see how Argo Workflows instantiates it.

```sh 
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: dag-target-
spec:
  entrypoint: dag-target
  arguments:
    parameters:
    - name: target
      value: E

  templates:
  - name: dag-target
    dag:
      target: "{{workflow.parameters.target}}"

      tasks:
      - name: A
        template: echo
        arguments:
          parameters: [{name: message, value: A}]
      - name: B
        depends: "A"
        template: echo
        arguments:
          parameters: [{name: message, value: B}]
      - name: C
        depends: "A"
        template: echo
        arguments:
          parameters: [{name: message, value: C}]
      - name: D
        depends: "B && C"
        template: echo
        arguments:
          parameters: [{name: message, value: D}]
      - name: E
        depends: "C"
        template: echo
        arguments:
          parameters: [{name: message, value: E}]

  - name: echo
    inputs:
      parameters:
      - name: message
    container:
      image: alpine:3.7
      command: [echo, "{{inputs.parameters.message}}"] 
```

Each task will be passed to the Argo server using the `target` parameter name, with the target names separated by spaces. Argo Workflows will execute only the ones you specify; however, it'll run each dependency until it reaches the informed targets. In plain English, say we save our file as `dag-targets.yml`  and execute using the following command: 

```sh
argo submit -n argo dag-targets.yml -p target="B E" --watch
``` 

It will skip only **target D**, as demonstrated below:

![argo workflow examples](/img/posts/argo-dag-results.png)
_Argo Workflows DAG execution results_

## <h2>7. Execute Python Scripts</h2><br />
Containers already make it easy to manage runtime environments. So, it’s easy to build a Python container with the libraries and version you need for your Python-based workflow steps.

With Argo Workflows you can call a Python script that’s already installed on the container by name, or pass in code via a `source` field in workflow description. You can specify any valid code in the source block. 

Here’s an example:

```sh
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  name: python-random-number-generator
  namespace: argo
spec:
  entrypoint: generator
  arguments:
    parameters:
      - name: min
        value: 0
      - name: max
        value: 10
  templates:
  - name: generator
    inputs:
      parameters:
        - name: min
        - name: max
    script:
      image: python:3.8
      command: [python]
      source: |
        from random import randrange
        range_min = {{inputs.parameters.min}}
        range_max = {{inputs.parameters.max}}
        random_number = randrange(range_min, range_max)
        print("Random number: {}".format(random_number)) 
```

## <h2>8. Implementing a Retry Strategy</h2><br />
Sometimes, multiple targets can implement some retry logic, and Argo Workflows configures your [retry strategy](https://argoproj.github.io/argo-workflows/retries/) on the Workflow level.

```sh
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: retry-container-
spec:
  entrypoint: retry-container
  templates:
  - name: retry-container
    retryStrategy:
      limit: "3"
      retryPolicy: "OnError"
    container:
      image: python:alpine3.6
      command: ["python", -c]
      # fail with a 66% probability
      args: ["import random; import sys; exit_code = random.choice([0, 1, 1]); sys.exit(exit_code)"] 
```
In our example, the target `retry-container` will try to restart three times in the cases that it finishes with an Error status on Kubernetes. 

## <h2>9. Adding Conditional Workflows </h2><br />
Conditional workflows are also among my favorites and are so simple to implement. You can deploy your architecture based on the return statuses of previous steps, which is very handy when orchestrating a set of containers. Argo Workflows grants you the possibility of executing targets based on a boolean condition. Under the hood, it uses [govaluate](https://github.com/Knetic/govaluate) to allow you to use [Golang's expr statements](https://github.com/antonmedv/expr). 

So you'll be able to orchestrate your conditions in the same way you handle your Golang helpers on your Kubernetes ecosystem—another nice extra benefit of using Kubernetes CRDs.

```sh 
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: conditional-parameter-
  labels:
    workflows.argoproj.io/test: "true"
  annotations:
    workflows.argoproj.io/version: '>= 3.1.0'
spec:
  entrypoint: main
  templates:
    - name: main
      steps:
        - - name: flip-coin
            template: flip-coin
        - - name: heads
            template: heads
            when: "{{steps.flip-coin.outputs.result}} == heads"
          - name: tails
            template: tails
            when: "{{steps.flip-coin.outputs.result}} == tails"
      outputs:
        parameters:
          - name: stepresult
            valueFrom:
              expression: "steps['flip-coin'].outputs.result == 'heads' ? steps.heads.outputs.result : steps.tails.outputs.result"

    - name: flip-coin
      script:
        image: python:alpine3.6
        command: [ python ]
        source: |
          import random
          print("heads" if random.randint(0,1) == 0 else "tails")
    - name: heads
      script:
        image: python:alpine3.6
        command: [ python ]
        source: |
          print("heads")
    - name: tails
      script:
        image: python:alpine3.6
        command: [ python ]
        source: |
          print("tails") 
```
Saving the above code as `cond.yml` and executing with `argo submit` will give the following output:

```sh
argo submit -n argo cond.yml --watch 
```
<br />

![Argo Workflows conditional execution results](https://user-images.githubusercontent.com/78096758/217082060-b565c064-74fb-457a-b7ae-6035e81019bb.png#center)

## <h2> 10. Managing Kubernetes Resources From Your Workflow </h2><br />
Argo Workflows can create Kubernetes components; this is very handy when you need to develop temporary kubelet actions in a declarative way. This feature follows the same principle of the inline scripts to deploy Kubernetes components responsible for applying patches to your environment. However, Argo Workflows handles this code's [Kubernetes CRD](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#customresourcedefinitions) YAML inline files.

```sh 
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: k8s-jobs-
spec:
  entrypoint: pi-tmpl
  templates:
  - name: pi-tmpl
    resource:                  
      action: create           
      successCondition: status.succeeded > 0
      failureCondition: status.failed > 3
      manifest: |               
        apiVersion: batch/v1
        kind: Job
        metadata:
          generateName: pi-job-
        spec:
          template:
            metadata:
              name: pi
            spec:
              containers:
              - name: pi
                image: perl
                command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
              restartPolicy: Never
          backoffLimit: 4 
```

This feature covers you as you directly run all kubectl actions, which allows you to create/update/delete any Kubernetes resource on your cluster using inline [Kubernetes API groups](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/) definitions. 

## <h2>Conclusion</h2><br />
The advances we’ve seen in systems management and development give us many reasons to be optimistic. For instance, [infrastructure as code](https://en.wikipedia.org/wiki/Infrastructure_as_code) allows you to have the same infrastructure on your scalable servers and your local workstation. Tools like Argo Workflows help us create scalable production-ready infrastructure on our local workstation, and that by itself is something to cherish. 

With constant infrastructure requirement changes such as [dynamic DNS](https://en.wikipedia.org/wiki/Dynamic_DNS#:~:text=Dynamic%20DNS%20(DDNS)%20is%20a,hostnames%2C%20addresses%20or%20other%20information.), you need to adapt your deployments to a more modular approach. These workflows are the must-haves for any DevOps admin. But this list is only the beginning. I would highly suggest implementing these scripts in your development and data pipelines. 

Until next time! 

Article originally posted at [Pipekit blog](https://pipekit.io/blog/top-10-argo-workflows-examples)