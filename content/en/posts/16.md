---
title: "How to optimizing your BigQuery results when using nested data structures like JSON files."
date: 2023-02-19
# weight: 1
# aliases: ["/first"]
tags: ["Data Warehouse", "GCP", "BigQuery"]
categories: ["Data warehouse", "Data Modeling", "Cloud"]
author: "Daniel Paes"
showToc: true
TocOpen: false
draft: false
hidemeta: false
comments: false
disableHLJS: true # to disable highlightjs
disableShare: false
disableHLJS: false
hideSummary: false
searchHidden: false
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: false
ShowRssButtonInSectionTermList: true
UseHugoToc: true
cover:
  image: "https://unsplash.com/photos/yiwEvZu7wbQ" 
---

As data can be used as a deal changer to check how healthy your company is, your storage option must reflect your business needs rather than the convenience of how you used to handle things. With more data, you can make better decisions; storage management must be done correctly. Otherwise, you might have storage worse than your previous choice because your team was better ready to maintain it. 

Today we will present some options to be taken in place while working with BigQuery. BigQuery has gained popularity due to its friendly interface to explore data from Google Analytics and Adwords. Enabling cold data from Object storage options like [Google Cloud Storage](https://cloud.google.com/storage) to be referenced by your users.

The presented code can be executed on the console and from the command line. The section **Enabling BQ from the command line** can be skipped for those who prefer the console UI. Extract the SQL statements from the provided code in this tutorial and execute them from the BigQuery Console UI at `console.google.com`.

![](https://user-images.githubusercontent.com/78096758/220182163-cb38ef43-c2e7-495b-87f9-71aceccbae17.png)
_Accessing the Bigquery editor from the console UI_


## Enabling BQ from the command line
<br />

If you prefer to run the presented code on the command line, please do the following; otherwise, you might be unable to follow along. The commands demonstrated here can be executed by the BigQuery console UI or a sub tool from the gcloud suite called `bq`. 

This tool allows us to run SQL statements from the command line, fetching our results without using the console UI. Check out how to install the BigQuery client by checking [Recommended Installation](https://cloud.google.com/sdk/docs/install) of the `gcloud` suite. 

Configuring the google project requires your BigQuery to run the `gcloud` commands line, such as the `bq`. This is quickly done by executing the following command from a terminal from your working machine.

```sh
gcloud config set project google-project-id
```

Once you have those configured, let's start by checking how to differentiate streaming from batch load jobs and how this can help your costs.
<br /><br />

## Prefer batch loading over streaming inserts if possible.
<br />

Here we start by saying to avoid streaming ingestion when possible and prefer using asynchronous methods instead. Let's picture an e-commerce platform to sell some goods which provides honest reviews from blog posts from a blogging platform. Both are accessible by your current clients, while some of your blog readers seem to buy from your platform after checking your comments. 
<br />

By checking on their customer base interests and how many of those blog posts led to sales, one can focus on extracting it and checking your selling trends based on what starts with a simple google search on one of your blog posts.
<br />

Storing the organic events that originated that led might have extra insights that aren't, but the aggregated topics from the most checked issues that created your organic conversions might be.
<br />

In this scenario, one can be biased to do the analysis in real-time, handling the data in a state called `hot data`. It is good to remember that running analytic functions with data at this stage is usually more expensive and should only be done if the return on investment explains its needs.
<br />

This data can be exported into landing storage; this data will then be loaded into your BigQuery tables. By running the command below, data stored on a Google Cloud Storage bucket in JSON format will be accessible from your BigQuery instance. 
<br />

```sh
bq load \
--autodetect \
--source_format=NEWLINE_DELIMITED_JSON \
bigquery-target-dataset.target-table \
gs://google-cloud-storage-bucket/source-data.json 
```

And this data can have enhanced quality procedures, such as **archiving policies**, which automatically move the data into different storage classes, reducing costs on both BigQuery and Google Cloud Storage buckets. 
<br /><br />

## How to enforce control over your query limits.
<br />

Bigquery has its charm due to being SQL friendly, which could lead us to have bad habits without even knowing. You can perceive cost reductions and performance enhancement with small changes while creating your queries. 

<br />
To demonstrate how partition tables work, we will be working with the public data available about GitHub repositories; let's check on its schema. 

![](https://user-images.githubusercontent.com/78096758/220181496-eadb112f-5fff-4787-8a92-5df8f07fc6b9.png)

_Bigquery nested table structure_ 
<br />

Tricks like `limit` and wildcards might not provide the expected results. And it would be best if you fetched only the desired fields from your table and used tricks such as the `maximun_bytes_billed` tag to limit the query bytes results instead of your bq command. You will also benefit from having all your results in a flattened structure in case you have nested fields in your table; check our article about [Bigquery data types examined and explained](https://dawrlog.com/posts/10/) if you need a refresher on the optimized data types per your query type.  
<br />

By informing a number on an `integer` datatype, we also tell the number of bytes that this query can consume without any decimal format. You will not be able to run the query if the threshold data amount fetched by your query surpasses the one informed, and your code **will fail** without returning any results if this **threshold is achieved**.

```sh
bq query --maximum_bytes_billed=1000000 \
--use_legacy_sql=false \
'SELECT
  field1,
  field2,
  .
  .
  .
  fieldN
FROM
  `PROJECT-ID.DATASET.TABLE`;'
```
<br />

Google also applies the same configuration from their graphical user interface (or UI). The `query settings` page has this option alongside other options to better tune your query; these options are under `more` from your BigQuery console UI.   

![](https://user-images.githubusercontent.com/78096758/219798792-961cb274-842f-43ad-9d04-05a534aba9ca.png)
_BigQuery Console UI Filter._
<br />

The **maximum bytes billed** helps, but up to a specific limit. As with this trick, we will control the total data returned. However, you can have better results by narrowing the data based on domains that make sense to your business. 
<br />

Here is where having **partitioned or clustered tables** will help; let's start by understanding how the data is stored in them.
<br /><br />

## In what consists of partitioned and clustered tables.
<br />

In event-based data platforms, it is common to have scattered data everywhere. BigQuery allows you to cluster your data by keys defined by you, which increases the performance while inquiring about your table. The gains are increased if the same structure is segregated by its event creation, creating a partitioned table. Let's think about an `order` table and how its data could be stored.
<br />

![](https://user-images.githubusercontent.com/78096758/220184794-e56b539a-7a7e-4eb8-af70-6c5a1279b903.png)
_Cluster tables per data tags (Source: [BigQuery Official doc on clustered tables](https://cloud.google.com/bigquery/docs/clustered-tables))._
<br />

This table type can increase retrieval performance while reducing costs, as you will run your queries on a subset of data. While this is a simple filter, the data is stored following an optimal structure. 
<br />

Using the dot notation mentioned before, let's see how to create both **partitioned and non-partitioned tables** from nested structures in BigQuery.  
<br />

## Creating partitioned tables in Bigquery.
<br />

Another best practice is to inform all the desired fields on your queries, avoiding the usage of wildcards when possible. Which can be tricky when nested structures are used, below we can see the table `github_nested` might not be. Below, we explore some of the nested fields of the table by using what is called **dot notation**. In addition, another table can work as a cache. By doing so, the computation required for every query will be reduced. The command below will create a nonpartitioned table. 
<br />

```sh
bq query --maximum_bytes_billed=1000000 \
--use_legacy_sql=false \
'SELECT
  repository.organization,
  actor_attributes.company,
  actor_attributes.location,
  repository.open_issues,
  repository.watchers,
  repository.has_downloads,
  SUBSTRING(repository.pushed_at, 0, 10) as date_push
FROM
  `bigquery-public-data.samples.github_nested`;'
```
<br />

To check on each performance, we will create a partitioned structure by executing the command below to create a partitioned table. 

```sql
bq query --maximum_bytes_billed=1000000 \
--use_legacy_sql=false \
'CREATE TABLE
  `samples.github_nested_partitioned`
PARTITION BY
  date_push
CLUSTER BY location
 AS
SELECT
  repository.organization,
  actor_attributes.company,
  actor_attributes.location,
  repository.open_issues,
  repository.watchers,
  repository.has_downloads,
  cast(PARSE_DATE("%Y/%m/%d",SUBSTRING(repository.pushed_at, 0, 10)) as date) as date_push
FROM
  `bigquery-public-data.samples.github_nested`;'
```

To check on each performance, we will create a partitioned structure by executing the command below to create a partitioned table. 
<br />

And this can be perceived from the console UI too. The `github_nested` shows what a standard table looks like, whereas the `github_nested_partitioned` shows how a partitioned table is. 

![](https://user-images.githubusercontent.com/78096758/220184872-74fa4cbd-b253-4b20-a6f9-91838435d620.png)


Now let us check if the is any difference while working with it.
<br />

## Comparing query performances 
<br />

For the sake of our example, ranking our results by the most downloaded repositories over watchers and open issues. Our results must be split by the company, its GitHub organization, and location, as informed on its page. The below query will interrogate the nonpartitioned object.
<br />

```sql
bq query --maximum_bytes_billed=1000000 \
--use_legacy_sql=false \
'SELECT 
repository.organization,
actor_attributes.company,			
actor_attributes.location,			
repository.open_issues,			
repository.watchers,			
SUBSTRING(repository.pushed_at, 0, 10) as date_push,
  RANK() OVER (PARTITION BY COUNT(repository.has_downloads)
  ORDER BY 
    COUNT(repository.open_issues), COUNT(repository.watchers), COUNT(repository.has_downloads) desc) 
 FROM `YOUR_GCP_PROJECT_ID.tutorial.github_nested`
 group by 
 repository.organization,
actor_attributes.company,			
actor_attributes.location,			
repository.open_issues,			
repository.watchers,			
SUBSTRING(repository.pushed_at, 0, 10)'
```

Let's see how the query performance was by looking at its query plan:

![](https://user-images.githubusercontent.com/78096758/220202269-d5343497-b572-4ea1-88ba-9456477def4a.png)
_Aggregation query plan nested table_


And the below one will query the partitioned one.

```sql
bq query --maximum_bytes_billed=1000000 \
--use_legacy_sql=false \
'SELECT 
organization,
company,			
location,			
open_issues,			
watchers,			
date_push,
  RANK() OVER (PARTITION BY COUNT(has_downloads)
  ORDER BY 
    COUNT(open_issues), COUNT(watchers), COUNT(has_downloads) desc) 
 FROM `YOUR_GCP_PROJECT_ID.tutorial.github_nested_partitioned`
 group by 
 organization,
company,			
location,			
open_issues,			
watchers,			
date_push'
```

And its query plan:

![](https://user-images.githubusercontent.com/78096758/220202450-ecaab7ed-f49d-4551-ac74-239a512a24a8.png)
_Aggregation query plan partitioned table_
<br />

By checking on the execution details of each query plan, we can perceive that the same aggregation takes less time and compute on the partitioned table when compared to a nonpartitioned one. 
<br />

We can see an improvement even on a small table like the one used in our example, leading to even more significance on tables with more rows. Let's now sum up what we saw.
<br />


## Wrapping up.

Today we checked how to optimize your BigQuery queries while avoiding unnecessary costs by applying small code changes. A proposed solution would be the well-balanced use of compute slots for your BigQuery clusters and avoiding any extra incurred costs for unused computing. 

[Book a time with our consultants](https://calendly.com/dawrlog) to have a tailored solution for your business based on the nature of your generated data. 

See you next time!