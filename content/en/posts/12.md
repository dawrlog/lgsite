---
title: "How to Value Stream DataOps?"
date: 2021-11-24
# weight: 1
# aliases: ["/first"]
tags: ["DataOps", "Lean Agile", "Best Practices"]
categories: ["DataOps", "Lean Agile", "Best Practices"]
# series: ["Best Practices"]
author: "Daniel Paes"
# author: ["Me", "You"] # multiple authors
showToc: true
TocOpen: false
draft: false
hidemeta: false
comments: false
canonicalURL: "https://www.enov8.com/blog/how-to-value-stream-dataops-2/"
disableHLJS: true # to disable highlightjs
disableShare: false
disableHLJS: false
hideSummary: false
searchHidden: false
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: false
ShowRssButtonInSectionTermList: true
UseHugoToc: true
cover:
    image: "https://user-images.githubusercontent.com/78096758/217092056-02f22c71-de93-4223-88c0-fc60f8f662b1.png" 
---

Hello everyone, today we will check how to properly value stream your DataOps initiative.

Enhancements on data ingestion made evident the amount of data lost when generating insights. However, without guidance from methodologies like [The DataOps Manifesto](https://www.dataopsmanifesto.org/), some companies are still struggling to blend data pipelines from legacy databases, such as an ADABAS database, with new ones, such as MongoDB or Neo4j. 

And it’s always good to point out that these legacy systems aren’t easy to let go of. Most of them are responsible for core processing (like the bank transactions in Cobol, for example). As such, it’s not attractive to change them because it’s too problematic. 

With that said, I believe the integration of new applications and legacy databases is one of the [biggest challenges](https://www.docshifter.com/blog/what-to-do-with-legacy-data/) for data-driven companies. However, there have been significant advancements in the methodologies and frameworks for data ingestion over the last few years.
<br /><br />

## <h2> Why Is It So Essential to Have Precise Data? </h2><br />
With optimized data ingestion pipelines, you can become better at decision-making. At the same time, updating your subject-focused legacy systems into modular applications sounds promising, right? 

The good news is, doing so isn’t a far-fetched dream! Thanks to advancements around computing power, it’s now possible to do way more with very little. We can now deconstruct bloated applications into slimmer versions. 

Using concepts that used to be possible only in theory, we can now ingest data from unusual places. For example, we can have an automated pipeline to ingest data from digitized documents thanks to [Optical Character Recognition](https://en.wikipedia.org/wiki/Optical_character_recognition), or OCR. How cool is that? 

In this post, I want to help you understand how your organization will benefit from a mature DataOps environment. First, I’ll present what DataOps is and why it’s not only DevOps for data. Then, I’ll explain some significant benefits that come from mapping the value streams for your own DataOps practice. Last, I’ll present some considerations when applying the DataOps framework to your team.
<br /><br /> 

## <h2> What Is DataOps? </h2><br />
It’s hard to provide your users with the most accurate metrics for existing dashboards with multiple data suppliers. It gets worse when your legacy database has so much technical debt that carrying out something as simple as creating a [Key Process Indicator](https://kpi.org/KPI-Basics), or KPI, is an absolute nightmare. 

And here’s where DataOps comes to the rescue! It makes it possible for your users to consume your data faster and in an automated environment integrated with the most updated version of your data suppliers. Pretty neat, right? 

I like to remember that this isn’t something new, as the [decision support system](https://www.investopedia.com/terms/d/decision-support-system.asp) (DSS) consists of an automated environment for your analytics pipelines. And I believe that DSSs always played an essential part in any company. The benefits for stakeholders include getting a complete understanding of your supply chain process or knowing faster which department or product is more profitable, to list a few. 

The data suppliers load these systems, which can be any source of information for your needs. Some examples of information provided by data suppliers include 

- Human resources data
- Agency campaign data
- Supply chain IoT devices
- Billing systems
- Server and traffic logs
<br /><br /> 

## <h2> DataOps vs. Decision Support Systems. </h2><br />
You often see orchestrated data pipelines from different data suppliers in legacy DSSs. So, each data supplier loads its data into this centralized database. But these independent data flows cause inconsistent data, making it hard to trust the DSS’s presented results. 

What DataOps facilitates is **better results with an optimized DSS**! Thanks to the agile approach, DataOps reinforces a more customer-centric approach when compared to DSS because of its modular architecture. In addition, DataOps eases constraints on scalability, automation, and security thanks to the reuse of components used by other data pipelines. 

These components can vary from simple database connectivity to a business rule used by the finance department that human resources could benefit from. All of this is thanks to repositories that are centralized, standardized, and reusable. 
<br /><br /> 

## <h2> Where DataOps Intersects and Where It Strays From DevOps</h2><br />
While DataOps seems like DevOps for data, I like to remember that the goals of DevOps and DataOps are different, although the methodologies share the same principles. 

DevOps focuses on loosening development to include operations. On the other hand, the goal of DataOps is to make analytics development more functional. DataOps uses [statistical process control](https://en.wikipedia.org/wiki/Statistical_process_control), the mathematical approach used in [lean manufacturing](https://en.wikipedia.org/wiki/Lean_manufacturing), DevOps disciplines, and best practices from the [Agile Manifesto](https://en.wikipedia.org/wiki/Agile_software_development#The_Manifesto_for_Agile_Software_Development). 

With these strategies in place, you can decouple your environment; DataOps makes it easier to detach your business logic and technical constraints from your data pipelines. As a result, you’re more confident in your data while enabling your teams to react better to data trends. 

You’ll benefit from the efficiency and results faster while designing your data pipeline components. And your teams will be more focused on delivering value rather than being tied down by technical decisions made in the past. 

Also, I always like to bring up the security gains resulting from proper deployment of DataOps: robust, secure systems that are less prone to breaches and leaks! 
<br /><br /> 

## <h2>Benefits to Consider While Mapping Your Value Streams</h2><br />
Now that we know what DataOps is about, I want to present the benefits of correctly mapping your [value streams](https://www.atlassian.com/continuous-delivery/principles/value-stream-mapping). 

My focus here is the benefits for your data pipelines, although they can also apply to your application deployments. From the business perspective, the principal value added by DataOps adoption is the ability to explore new data sources rapidly. 

Here are the advantages of the [value added](https://berteig.com/how-to-apply-agile/value-added-work-basic-lean-concepts/) to the customer when mapping your value streams. 
<br /><br /> 

## <h3>Enhanced Control Thanks to Technical Decoupling.</h3><br />
As I mentioned before, data suppliers are any source applications with relevant data for your analysis. In other words, they’re data entry points that feed your data pipeline. 

These applications produce data in a raw state. And as the name implies, this data should be left as untouched as possible. It’s beneficial in cases of reprocessing or data lineage analysis. 

This data needs additional processing steps to remove unnecessary noise from it, as its original form might not be a good fit for your needs. However, from this output, you can extract the necessary metrics to cover the needs of your users. 

I also want to bring up one of the values from conscious decoupling: its robust automated control of each component’s data pipeline. This orchestration brings up extra quality measures, making it possible to increase productivity given that your users won’t need to perform repetitive tasks. 
<br /><br /> 

## <h3>Fast Exploration of Existing and New Data Suppliers.</h3><br />
On legacy systems, the development of new pipelines is chaotic, as previously mentioned. The DataOps approach also enables fast exploration of your data suppliers. 

DataOps makes it easier to create conformed components thanks to its modular deployment. What I mean by that is that you can reuse already deployed and tested components. 

In other words, DataOps enables a mindset of continuous improvement. Therefore, it will drastically lower your development costs and increase your return on investment at the same time. 

Your team will handle more challenging tasks on bringing business value, not with daily data wrangling activities. As a result, you get an instant productivity boost thanks to the automated deployment of your application. 
<br /><br /> 

## <h3>Reliable Data Governance.</h3><br />
As a result of the automated data pipelines deployed by DataOps, it becomes easier to trace how your users consume your data. This information can help you quickly answer recurring questions. 

Your users can see where the business logic is in a blink of an eye. Moreover, the reference between its canonical and technical implementation name becomes easy to absorb since new analysts getting onboarded to your projects makes it an appealing source for your data profiling. 

As a result, a solid datalog of your data suppliers is a mandatory step to think about when mapping your value streams, in my opinion. It becomes easy to manage your data pipelines when you create an enterprise-level conformed business catalogue. All this structured information about your data suppliers creates an intuitive data catalogue. 

This information, also known as metadata, provides the business context where this data gives its value. So, in other words, your insights would become more accurate. 
<br /><br /> 

## <h2>Important Considerations About Your Own DataOps Deployment.</h2><br />
What we’ve seen so far shows how conformed detached modules enable a more robust data catalogue for your company. In addition, the coherence among your analytics components clarifies where you can enhance your data ingestion. 

I always like to remember that these improvements don’t come for free. As you’ll react faster and more wisely, be ready to reshape some of your company’s internal processes. Bear in mind that it’s hard to teach an old dog new tricks. So, expect some resistance from more experienced teammates. 

A self-healing data pipeline scales horizontally or vertically when needed. So, you can add more units for processing power (known as horizontal scaling) or enhance your clusters (known as vertical scaling) when your sets start to have some bottleneck issues while processing your data; **_Remember the rule of thumb_**: 

> It gets easier to enrich your outputs when you’re fully aware of your data scope. So, in addition to its modular components, DataOps enables actions like masking and obfuscating your data to occur more fluidly in the early stages of its processing. 

With DataOps, you’re building up a **trustworthy data pipeline** capable of reacting to new concepts and technologies at the **_same speed_** as your **_business’s evolution_**. 
<br /><br /> 

## <h2>Final Thoughts.</h2><br />
In this post, I gave an overview of what you’ll gain by correctly deploying DataOps. The result is a mix of business and technical added value because you’ll have slim and robust orchestrated data pipelines.  

I started by presenting what DataOps is and what business value it brings. Then, I explained where it intersects and where its goals differ from agile and DevOps methodologies. 

We also took a quick look at what I believe to be the short-term benefits from the correct deployment of a mature DataOps implementation and how automated deployments can add technical value. 

Last, we saw some challenges that your team may face when you adopt DataOps. For example, your unit may be **resistant** to **_adopting new technologies and methodologies_**. However, we also saw the benefits of a correct deployment as a **_concise data catalogue_**. 

Just remember that you must completely implement DataOps’s requirements. So, don’t expect to have a reliable DataOps implementation with partial deployments of agile or DevOps disciplines. 

For further help transforming your data, you can [reach out to us](https://calendly.com/dawrlog) with any questions you have and even discover new info you weren't aware of with a better insight from better understanding your data.

See you next time!

Article originally posted at [Enov blog](https://www.enov8.com/blog/how-to-value-stream-dataops-2/)
