---
title: "Archiving Argo Workflows: Postgres Database Setup"
date: 2022-04-11
# weight: 1
# aliases: ["/first"]
tags: ["Kubernetes", "Argo", "Development", "Data Warehouse", "Best Practices"]
categories: ["Data Modeling", "Kubernetes"]
# series: ["Kubernetes"]
author: "Daniel Paes"
# author: ["Me", "You"] # multiple authors
showToc: true
TocOpen: false
draft: false
hidemeta: false
comments: false
canonicalURL: "https://pipekit.io/blog/archiving-argo-workflows-postgres-database-setup"
disableHLJS: true # to disable highlightjs
disableShare: false
disableHLJS: false
hideSummary: false
searchHidden: false
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: false
ShowRssButtonInSectionTermList: true
UseHugoToc: true
cover:
    image: "https://assets.website-files.com/6238902a290688c9a1ad26ea/6293bd541bf0c5e5cc82d1e7_1.jpg" 

---

Hello everyone, I hope you are doing well!

If you’re familiar with [Argo Workflows](https://argoproj.github.io/workflows/), you already know that it can drive your CI/CD pipelines, manage your ETL processes, and orchestrate any set of tasks you can imagine for a Kubernetes cluster. But did you know that Argo knows how to archive the results of its workflows to a SQL database, too?

In this post, I'll show how Argo Workflows archives workflow state into persistent storage using a Postgres database. To do so, I'll present a quick summary of Argo components while showing what it means to have your workflow archived. We'll deploy Argo Workflows alongside a Postgres database on a local Kubernetes instance using [k3d](https://k3d.io/v5.3.0/). Finally, we’ll discuss some important security considerations for your Argo Workflows deployment. 

So, let's get started. 

## <h2>What is the Archive Option in Argo Workflows?</h2> <br />

The ability to store past workflow runs provides you with an accurate record of your past workflow states. This blueprint is a game changer, as it makes it possible to provision your task sets based on real-time metrics, such as spikes in processing needs from past deployments. 

The workflow archive option stores your workflow states in either MySQL or Postgres. Once you have archives configured, you can use them to better understand how your jobs run and where you might be able to improve. 

For example, they can help you know when it's a good idea to scale your traffic with the help of temporary instances, which will also have their states stored on the same database. With all your states held over time, you can apply rules to adjust your cluster size based on previous usage; a good time series analysis could even save you some money in the end.

![postgres archiving](https://user-images.githubusercontent.com/78096758/217069095-b10d7ff5-01f1-4213-98c4-02a35869a736.png)

<br />
The archive stores only the previous workflow states, not their detailed instance logs. Another thing to keep in mind is detailed audit logs. The artifact store option handles the detailed logs persistent option, storing it locally by [MinIO](https://min.io/). But you can also configure any other object storage option. This is covered in the [Argo official documetation](https://argoproj.github.io/argo-workflows/configure-artifact-repository/), where you can see how to use options such as [Google Cloud Storage](https://cloud.google.com/storage) buckets or [AWS S3](https://aws.amazon.com/s3/). 

But before we start on the technical implementation, let's have a quick refresher on the components of Argo Workflows. It's necessary to know how they correlate with the persistent storage for your archived workflows; this image from the Argo Workflows documentation presents an overview of the environment where a workflow resides:

![Argo Workflow Diagram](https://user-images.githubusercontent.com/78096758/217069401-e63ad934-46ae-4561-91ca-262089dd0131.png)
_Source: Argo Workflow Github repository_

## <h2>How to Deploy Argo Workflows with Persistent Storage</h2><br />
Now that we know what's in store for us let's get started. We'll be using k3d to manage our local Kubernetes environment (instead of minikube and VirtualBox). In addition to k3d, you'll need to install Docker as an additional dependency. Using kubectl to interact with your Kubernetes cluster works fine, too. As for our tutorial, we'll be using local Kubernetes deployment scripts. 

First, we'll start our local control plane with the following command:

```sh
k3d cluster create cluster-demo 
```

The successful creation will provide a log similar to this one:

![Archive Info](https://user-images.githubusercontent.com/78096758/217070632-b181c8a9-f684-4cf5-a58a-9b50d79d18a6.png)
_Creating the cluster_

<br/>
Once we have our `cluster-demo`, we'll deploy our Argo Workflows instance. To install Argo Workflows, you'll need to execute the following commands:

```sh
kubectl create ns argo
kubectl apply -n argo -f https://raw.githubusercontent.com/argoproj/argo-workflows/master/manifests/quick-start-postgres.yaml 
```
The first one creates a namespace called argo in your cluster, and the following line will deploy the Argo Workflows components on your cluster, as you can see below: 

![postgres archiving](https://user-images.githubusercontent.com/78096758/217072204-32b27544-798d-489a-8e67-02994c386620.png)
_Deployments made on the cluster_ 

## <h2>Creating the workflow-controller </h2><br />
To run the workflow with the archive option, you must first change the persistence configuration to `archive:true` on your Argo server deployment. Changing it will tell your Argo server to store your workflow execution states into the database reported by the key `postgresql`. 

We'll apply a new `ConfigMap` into our current Kubernetes argo namespace with the Postgres instance to store your archived workflows. You can then archive your workflows by using the `archiveLogs` option. 

We had a Postgres instance deployed with the Quickstart YAML we used earlier. You'll need it only to apply the following configuration to your deployment. Changing this configuration enables your Argo server deployment to accept the `archiveLocation.archiveLogs` notation while creating your workflows. We'll start by creating a new `workflow-controller-configmap.yml` with the following content and saving it locally:

```sh 
apiVersion: v1
kind: ConfigMap
metadata:
  name: workflow-controller-configmap
data:
  persistence: |
    connectionPool:
      maxIdleConns: 100
      maxOpenConns: 0
      connMaxLifetime: 0s
    nodeStatusOffLoad: true
    archive: true
    archiveTTL: 7d
    postgresql:
      host: postgres
      port: 5432
      database: postgres
      tableName: argo_workflows
      userNameSecret:
        name: argo-postgres-config
        key: username
      passwordSecret:
        name: argo-postgres-config
        key: password
  retentionPolicy: |
    completed: 10
    failed: 3
    errored: 3 
```

Deploy Your Environment with kubectl
We'll expose the Argo Workflows web UI using a load balancer on our argo namespace. The load balancer will expose the pod executing the web-facing component to connections made from outside Kubernetes.

```sh
kubectl apply -n argo -f workflow-controller-configmap.yml 
```

Your Argo server will restart with the new configuration in a couple of minutes. Feel free to check its status by running `kubectl get -n argo svc,pod` on your Kubernetes cluster. 

```sh
kubectl get -n argo svc,pod 
```

You can then bind your Kubernetes cluster and your host to port 2746 by running the following on your cluster:

```sh
kubectl -n argo port-forward deployment/argo-server 2746:2746 & 
```

Congratulations, you just deployed Argo Workflows on a k3d cluster. To confirm that your local instance is up and running, go to ```https://localhost:2746```. 

![Argo Workflow User info UI](https://user-images.githubusercontent.com/78096758/217072659-01210f43-c734-49d3-8282-0dfbc8b5c49f.png)
_Argo Workflows user info UI page_
<br />

## <h2>Testing Your Deployment</h2><br />
Congratulations on installing your Argo Workflows instance on your local Kubernetes cluster with the archive option. And now that we have checked that off our list, let’s archive our workflows. Adding the `archiveLogs` annotation lets you specify which ones you want to archive, as demonstrated in the following template, which we'll call `workflow-archive.yml`.

```sh
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: archive-location-
spec:
  archiveLogs: true # enables logs for this workflow
  entrypoint: whalesay
  templates:
  - name: whalesay
    container:
      image: docker/whalesay:latest
      command: [cowsay]
      args: ["hello world"] 
```

We need to execute `argo submit -n argo --watch -f workflow-archive.yml` on a terminal to deploy it.

```sh 
argo submit -n argo --watch -f workflow-archive.yml 
```

By doing so, you'll start the `archive-location` workflow under the `argo` namespace; the following output confirms that our example ran successfully: 

![Argo Workflow Archive example](https://user-images.githubusercontent.com/78096758/217072867-ae971ca3-0776-462a-8b8b-1209498873d6.png)
_Argo Workflows archive example run output log_ <br />

It doesn't change on the command line; however, as we have persistent storage for our workflows, you can see their previous states on the console UI. That'll give you the previous workflow states that ran with the archive options enabled—and going to the Argo Workflows console UI at `https://localhost:2746`, as we saw before, you can access the archived workflow UI option from the left menu bar’s icons. Once you are there, you can see all the past executions of a workflow. Your workflow history can be found in the UI under “Archived Workflows” (see below). 

![Argo Workflow Archive Workflow Console](https://user-images.githubusercontent.com/78096758/217073174-b3762346-615e-4b90-8752-b2521acf769d.png)
_Argo Workflows archived workflow console_
<br />
<br />

## <h2>Security Best Practices for Archiving Argo Workflows in Postgres</h2><br />
In our work, we deployed an Argo instance with the archive option configured with a Postgres database. As mentioned previously, this code isn't production-ready. As a next step, I suggest managing your access tokens to secure your Argo instance. 

A good practice is to avoid hardcoded values for server runtime information when possible. Your infrastructure should generate data like your Postgres hostname on runtime instead of having it hardcoded. Your infrastructure should use secrets to store sensitive information like repository access keys.

![postgres archiving](https://user-images.githubusercontent.com/78096758/217073499-91ffe20f-6625-42ff-9688-b1b436fa6f1f.png)
_Postgres Archiving_

Take a look at this [introduction about Secrets and configmaps in Kubernetes](https://opensource.com/article/19/6/introduction-kubernetes-secrets-and-configmaps), for more details on what Kubernetes information should be discreet. Adopting security best practices like this in the early stages is easier for both your users and developers as you start to scale. In addition, having your configuration automated ends up narrowing the [attack surface](https://csrc.nist.gov/glossary/term/attack_surface) of your environment while also reducing infrastructure management tasks. 

Here’s a [helpful blog post](https://blog.argoproj.io/practical-argo-workflows-hardening-dd8429acc1ce) with more Argo security best practices from the Argo Workflows maintainer, Alex Collins.

## <h2>Conclusion</h2><br />
We deployed Argo Workflows locally and archived a workflow using a Postgres database in this post. The scripts here are good starting points to understand and experiment with the archive option of Argo Workflows, but keep in mind that some critical factors are missing for a fully cloud native environment. 

See you guys next time!

Article originally posted at [Panoply blog](https://pipekit.io/blog/archiving-argo-workflows-postgres-database-setup)
