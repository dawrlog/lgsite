---
title: "Le connaissance de base pour l'exploration des données utilisant AWS Redshift Spectrum, Athena et S3."
date: 2021-07-08
# weight: 1
# aliases: ["/first"]
tags: ["Data Warehouse", "AWS", "Data Modeling", "Redshift", "Athena", "S3"]
categories: ["Data warehouse", "Data Modeling", "Cloud"]
# series: ["Data Stack"]
author: "Daniel Paes"
# author: ["Me", "You"] # multiple authors
showToc: true
TocOpen: false
draft: false
hidemeta: false
comments: false
# canonicalURL: "https://blog.panoply.io/the-spectrum-of-redshift-athena-and-s3"
disableHLJS: true # to disable highlightjs
disableShare: false
disableHLJS: false
hideSummary: false
searchHidden: false
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: false
ShowRssButtonInSectionTermList: true
UseHugoToc: true
cover:
    image: "https://user-images.githubusercontent.com/78096758/217091701-9777080f-4d8b-4bf9-b9fe-5dead25dd10f.png" 
---

Salut tout le monde, aujourd'hui je voudrais vous présenter comment explorer vos données brutes stockées par [AWS S3](https://aws.amazon.com/s3/) en utilisant les services analytiques gérés par AWS. Aujourd'hui, nous allons couvrir à la fois [AWS Redshift Spectrum](https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html#c-spectrum-overview) et [AWS Athena](https://aws.amazon.com/athena/) dans notre tutoriel.

L'un des principaux défis auxquels sont confrontées les entreprises axées sur les données est l'intégration de différents systèmes d'application.

Les principaux fournisseurs de clouds publics, tels que [Amazon Web Services](https://aws.amazon.com/) ou [Google Cloud Platform](https://cloud.google.com/docs/overview), proposent des produits robustes prêts à répondre à vos besoins en matière d'analyse. Mais lorsqu'il s'agit d'explorer vos données, les choses ne sont pas toujours aussi simples.

Vos données sources proviennent souvent de fichiers dont les formats de données sont inconnus, ce qui fait du travail de l'analyste un cauchemar.

Certains cas pourraient être plus fluides dans cette intégration, comme lorsque vos données ont des valeurs imbriquées. L'explosion de structures complexes, comme les fichiers JSON, en un format tabulaire, peut consommer la majeure partie de votre temps lorsque vous explorez de nouvelles données.

C'est là que ***AWS Redshift Spectrum*** et ***AWS Athena*** se distinguent. Elles vous permettent d'utiliser SQL pour analyser les données sans les modifier à la source. Il n'y a pas besoin de code Python complexe si vous ne voulez pas l'utiliser pour les tâches initiales de profilage des données.

Pas mal, non ?

Cet article vous montrera comment explorer vos données sur Amazon S3 à l'aide d'Athena et de Redshift Spectrum. Vous trouverez ci-dessous les étapes nécessaires pour créer une table sur le [catalogue AWS Glue](https://docs.aws.amazon.com/prescriptive-guidance/latest/serverless-etl-aws-glue/aws-glue-data-catalog.html) et l'utiliser pour accéder à vos données sur Amazon S3.
<br /><br />

## <h2>Comment Redshift Spectrum, AWS Athena & AWS S3 s'accordent-ils ? </h2><br />

Si les produits de données d'Amazon ne sont pas aussi étendus que sa célèbre boutique de commerce électronique, il y a quand même beaucoup de choses à faire.
<br /><br />

## <h3> Redshift Spectrum </h4> <br />
Spectrum est un composant Amazon Redshift qui vous permet d'interroger des fichiers stockés dans Amazon S3. Pour ce faire, il suffit de créer une nouvelle base de données pointant vers votre seau de stockage AWS S3. 

Votre équipe peut affiner sa recherche en interrogeant uniquement les colonnes nécessaires à votre analyse. Il est également possible de consulter les tables existantes de votre cluster Redshift, ce qui signifie qu'au lieu d'interroger la table complète en permanence, vous pouvez sélectionner les colonnes requises pour votre rapport à l'aide de SQL.

Ainsi, lorsque vous interrogez vos données, vous obtenez uniquement les colonnes nécessaires au lieu de renvoyer des champs et des lignes inutiles. Cette fonction offre également la possibilité d'interroger des données stockées directement sur Amazon S3.
<br /><br />

## <h3> AWS Athena </h4> <br />
Athena facilite la création de requêtes SQL partageables entre vos équipes, contrairement à Spectrum, qui nécessite Redshift. Vous pouvez ensuite créer et exécuter vos classeurs sans aucune configuration de cluster.

Athena permet de faire plus avec moins, et il est plus économique d'explorer vos données avec moins de gestion que Redshift Spectrum.

## <h3> AWS S3  </h4> <br />
AWS S3 est l'option de [stockage d'objets géré](https://www.techopedia.com/definition/29510/object-storage) proposée par Amazon. C'est la meilleure option pour stocker vos données semi-structurées, comme les journaux de serveur de vos applications.

S3 permet également la **_protection contre la suppression_** et le **_contrôle de version_** de vos objets, ce qui rend vos données plus sûres et plus faciles à retracer jusqu'à leur source d'origine.

## <h2>Comment créer des tables à partir de fichiers </h2><br />
Maintenant que vous avez une idée générale de chaque produit, il est temps de mettre la main à la pâte et de créer quelques tableaux !

Nous utiliserons un exemple de données CSV pour notre tutoriel, que vous pouvez [télécharger ici](https://people.sc.fsu.edu/~jburkardt/data/csv/csv.html).


Nous supposons également que votre cluster Redshift est prêt et que les [rôles IAM]() nécessaires y sont attachés (si vous utilisez Redshift Spectrum).

Vous devez également avoir votre [AWS bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html) configuré avec vos données et les autorisations requises pour créer votre catalogue de données ; vous trouverez plus de détails sur la page [Athena IAM Data Catalog policy](https://docs.aws.amazon.com/athena/latest/ug/datacatalogs-iam-policy.html).

Bon, jusqu'ici tout va bien ! Passons à la création des tables.

Tout d'abord, vous devez créer la base de données dans laquelle les tables seront stockées.

Pour ce tutoriel, nous comptons sur AWS Glue Data Catalog pour cette tâche. N'oubliez pas que d'autres options sont disponibles, comme le [Hive metastore](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-metastore-external-hive.html).

AWS Glue Data Catalog est une meilleure option si vous souhaitez avoir une intégration fluide avec des sources de données supplémentaires sans avoir à démarrer des services supplémentaires.

Voici comment créer une **base de données sur AWS Athena** :

```sql
CREATE DATABASE IF NOT EXISTS ext_data_suppliers
  COMMENT 'Landing Zone for S3 buckets loaded by external Data Suppliers'
  LOCATION 's3://test-12343210/';
```

Voici la version **Redshift Spectrum** de celui-ci :

```sql
create external schema ext_data_suppliers from data catalog 
database 'ext_data_suppliers'
iam_role 'arn:aws:iam::123456789012:role/RSRoleApiData'
create external database if not exists;
```

Comme vous pouvez le voir dans les deux cas, votre code créera une base de données de catalogue Glue si elle n'existe pas déjà.

Une fois que vous l'avez, vous aurez besoin de la définition de la table, qui vous permettra d'interroger les données directement à partir du fichier.

A ce stade, je recommande de ne pas faire de transformations sur les données car une modification mineure. Comme sur cette couche de données nous voulons être aussi proche que possible de sa source de données, même une simple conversion de type de données peut entraîner la perte de données. Évitons donc cela, surtout dans les premières étapes.

Maintenant, créons une définition de table qui contiendra les données.

Ci-dessous, vous pouvez voir la version Athena :
```sql
CREATE EXTERNAL TABLE IF NOT EXISTS ext_data_suppliers.zillow_sample_file 
( `index` int, 
  `liv_space_in_sqft` int, 
  `beds` int, 
  `baths` int, 
  `zip` int, 
  `year` int, 
  `list_price_in_usd` int )
ROW FORMAT SERDE 
  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
WITH SERDEPROPERTIES 
  ( 'serialization.format' = ',', 'field.delim' = ',') 
LOCATION 's3://test-12343210/'
TBLPROPERTIES ('has_encrypted_data'='false');
```

Et voici la version Spectrum :
```sql
CREATE EXTERNAL TABLE ext_data_suppliers.zillow_sample_file 
( index int, 
liv_space_in_sqft int, 
beds int, 
baths int, 
zip int, 
year int, 
list_price_in_usd int )
ROW FORMAT SERDE 
   'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
WITH SERDEPROPERTIES 
   ( 'serialization.format' = ',', 'field.delim' = ',') 
LOCATION 's3://test-12343210/';
```
<br />

## <h2>Alors, lequel choisir : Spectrum ou Athena ? </h2><br />
La plupart des données provenant des fournisseurs de données ne sont pas optimisées pour répondre aux questions que vous vous posez. Comme ces données sont à l'état brut, des outils comme AWS Athena ou AWS Redshift Spectrum rendront vos tâches d'exploration beaucoup plus simples.

Ces deux outils vous permettent d'explorer vos données sans les charger dans une base de données. Tout ce que vous avez à faire est de dire quelle est votre structure de données et où elle réside. Après cela, vous êtes prêt à partir - plus de retard sur vos pipelines de données pour commencer à créer vos tableaux de bord. Dès que vos données sont disponibles sur votre seau Amazon S3, votre équipe peut les consommer immédiatement.

Vous pouvez exécuter des requêtes fédérées sur les deux services. Ces requêtes permettent d'utiliser le même script SQL pour corréler des données provenant de sources relationnelles structurées, telles que MySQL et Postgres.

Mon conseil ? **_Choisissez Athena si vous n'avez pas déjà un cluster Redshift en place._**

Avec Athena, il devient plus facile de créer des requêtes partageables au sein de votre équipe sans gérer de services supplémentaires et augmenter inutilement votre facture de cloud.
<br /><br />

## <h3>Résumé et approfondi</h3><br />
Pour résumer, nous avons abordé deux sujets importants :

Les avantages de disposer d'un outil d'exploration des données qui permet à vos analystes d'exécuter des commandes SQL au-dessus de votre solution de type stockage objet.
Exécution de commandes SQL sur des fichiers stockés dans Amazon S3, en utilisant Athena et Redshift Spectrum.

Comme vous l'avez vu, les deux scripts sont très similaires. Dans les deux, nous avons utilisé [serialization/deserialization](https://docs.aws.amazon.com/athena/latest/ug/supported-serdes.html) (SerDe en abrégé) pour créer correctement une structure de type tableau. Cette structure vous permet d'accéder aux données au format CSV sans les charger dans des tableaux natifs.

Voici une autre chose dont j'aimerais que vous vous souveniez : N'accordez que les [permissions nécessaires](https://docs.aws.amazon.com/athena/latest/ug/datacatalogs-example-policies.html) aux services. En limitant l'accès à vos services, vous pourrez mieux dormir. 

Si vous avez besoin d'aide pour transformer vos données, vous pouvez [nous contacter](https://calendly.com/dawrlog) pour toute question et même découvrir de nouvelles informations dont vous n'étiez pas conscient grâce à une meilleure compréhension de vos données.

À+!