---
title: "Les types de données BigQuery examinés et expliqués"
date: 2022-01-12
# weight: 1
# aliases: ["/first"]
tags: ["Data Warehouse", "GCP", "BigQuery"]
categories: ["Data warehouse", "Data Modeling", "Cloud"]
# series: ["Data Stack"]
author: "Daniel Paes"
# author: ["Me", "You"] # multiple authors
showToc: true
TocOpen: false
draft: false
hidemeta: false
comments: false
# canonicalURL: "https://blog.panoply.io/bigquery-data-types"
disableHLJS: true # to disable highlightjs
disableShare: false
disableHLJS: false
hideSummary: false
searchHidden: false
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: false
ShowRssButtonInSectionTermList: true
UseHugoToc: true
cover:
    image: "https://user-images.githubusercontent.com/78096758/217090752-4965c510-831d-40c4-9049-c46428495a81.png" 
---

Bonjour à tous, aujourd'hui je voudrais vous parler de Bigquery, le service géré par Google Cloud pour les entrepôts de données. Et j'aimerais explorer ses types de données.

Google Cloud Platform est depuis longtemps le fournisseur de clouds de prédilection pour l'analyse Web, l'impressionnant BigQuery étant l'option de [traitement massivement parallèle](https://searchdatamanagement.techtarget.com/definition/MPP-database-massively-parallel-processing-database) (alias MPP) de Google. Les bases de données MPP sont très courantes dans les [écosystèmes Hadoop](https://hadoop.apache.org/).

BigQuery est très convivial pour ses utilisateurs (convenons que BigQuery est aussi agréable que sa connaissance de SQL), ce qui favorise l'arrivée de nouveaux utilisateurs. En partie, grâce à une exploration plus détaillée des données Google Analytics, comme décrit sur [cet exemple](https://support.google.com/analytics/answer/3437618?hl=en).

Cet article a pour but de présenter BigQuery et ses types de données existants par catégories SQL. Je donnerai également un cas d'utilisation pratique où la connaissance de ces types de données peut être utile : la vérification des données lors de l'ingestion des données.

En sachant quel type de données BigQuery utiliser, vous pourrez créer plus facilement vos pipelines de données. Une fois que vous aurez mieux compris quels types de données sont disponibles, la qualité de vos rapports augmentera considérablement en un rien de temps.
<br /><br />


## <h2>Types SQL dans BigQuery</h2><br />
BigQuery fonctionne à l'aide d'un langage d'interrogation structuré (SQL), subdivisé en catégories anciennes et standard.

Il est bon de préciser que chaque sous-groupe a des types de données et des façons d'interroger la base de données différents. Lorsque l'indice n'est pas renseigné, BigQuery utilise les fonctions et méthodes SQL standard. Jetez un coup d'œil à la rubrique [Indices SQL](https://en.wikipedia.org/wiki/Hint_(SQL)) si vous êtes novice et souhaitez en savoir un peu plus à leur sujet.

Pour présenter ce à quoi ressemble une requête patrimoniale, je vais montrer un exemple d'exécution d'une commande SQL utilisant le type de catégorie patrimoniale.

<br />

**Exemple d'astuce SQL**

```sql
#legacySQL -- This hint informs that the query will use Legacy SQL  
SELECT
weight_pounds,
state,
year,
gestation_weeks
FROM [bigquery-public-data:samples.natality]
ORDER BY weight_pounds DESC;
``` 
Le SQL standard est le type que vous devriez chercher à utiliser lorsque vous n'avez pas de contraintes, comme le code hérité sur votre déploiement [lift-and-shift](https://aws.amazon.com/blogs/architecture/optimizing-a-lift-and-shift-for-performance/).

Le SQL standard permet également des fonctionnalités très intéressantes comme les fonctions de fenêtre et une structure de fonctions définies par l'utilisateur plus robuste, et il est plus souple de créer sur le SQL standard plutôt que sur le SQL hérité.

Il est également recommandé, en tant que meilleure pratique, de migrer votre SQL patrimonial existant vers le SQL standard.

Google propose même une page utile contenant des conseils sur cette conversion. Vous pouvez vérifier comment la lancer correctement sur [cette page](https://cloud.google.com/bigquery/docs/reference/standard-sql/migrating-from-legacy-sql).

## <h2>Types de données SQL standard</h2><br />
Le tableau ci-dessous présente les types de données possibles en utilisant la catégorie SQL standard. Je tiens à souligner que la catégorie SQL standard est la catégorie préférée de Google, ce qui signifie que vous pouvez supposer à juste titre qu'elle offre de meilleures performances et davantage d'options par rapport à la catégorie SQL traditionnelle.

![datatypes](https://user-images.githubusercontent.com/78096758/217090962-60ccc9db-b597-4ec0-86d2-8a82ceafa801.png)
_Bigquery Standard SQL Datatypes_ (en anglais)
<br /><br />

## <h2>Types de données SQL standardTypes de données SQL standard</h2><br />
La catégorie SQL standard accepte des **types de données plus complexes** tels que les types de données **ARRAY**, **STRUCT** et **GEOGRAPHY**. Tous les types de données mentionnés peuvent ordonner (ou grouper) les résultats de n'importe quel ensemble de données ; l'une de ses graves limitations est que les types de données **STRUCT** et **ARRAY** sont fortement utilisés pour les [entrées de données] en continu (https://whatis.techtarget.com/definition/data-ingestion).

Un autre exemple vient du fait que le type de données **ARRAY** ne compare pas ses valeurs avec un champ quelconque, ce qui se produit dans **STRUCT** et **GEOGRAPHY**.

Nous pouvons utiliser [ST_EQUALS](https://cloud.google.com/bigquery/docs/reference/standard-sql/geography_functions#st_equals) comme "solution de rechange" pour comparer directement les valeurs géographiques.

Tous les autres types de données peuvent filtrer les clauses **SQL JOIN** et être utilisés pour ordonner les résultats ; n'oubliez jamais de caster les colonnes utilisées pour éviter explicitement les effets indésirables.

Le langage SQL standard permet également l'utilisation de ce que l'on appelle les ***procédures stockées***.

La procédure stockée permet l'exécution de **fonctions reproductibles** - très utile pour les transformations de logique métier partageables entre différents départements.

Par exemple, la façon dont le département des ressources humaines calcule les bénéfices pourrait être utile au département du marketing pour le calcul des campagnes.

L'avantage des **formats de données bien définis** commence avec vos procédures stockées - car les options sur l'étape précédente de vos pipelines d'analyse donnent à vos analystes un temps de réaction plus court pour analyser vos données.


## <h2>Types de données SQL hérité (Legacy SQL)</h2><br />

Legacy SQL utilise les mêmes types de données que ceux utilisés avec SQL standard, à l'exception de **ARRAY**, **STRUCT** et **GEOGRAPHY**.

Les champs de type **NUMERIC** offrent un support limité, ce qui rend nécessaire une conversion explicite à l'aide de la [fonction cast](https://cloud.google.com/bigquery/docs/reference/legacy-sql#cast) lors de l'interaction avec ces champs.

Le tableau ci-dessous énumère tous les types de données possibles disponibles lors de l'utilisation de la catégorie de requête SQL héritée.

Vous pouvez toujours accéder aux données imbriquées à l'aide de la [notation point](https://en.wikipedia.org/wiki/Property_(programming)#Dot_notation), mais elle ne permet pas de réaliser de belles astuces comme la génération de votre tableau au moment de l'exécution.

Legacy SQL vous permettra de créer des fonctions réutilisables et partageables entre différentes requêtes. Cette possibilité est offerte par les [fonctions définies par l'utilisateur](https://cloud.google.com/bigquery/user-defined-functions) (ou UDF) ; vous trouverez ci-dessous un exemple d'une fonction simple.

UDF sur legacy SQL
```js
// UDF definition
function urlDecode(row, emit) {
emit({title: decodeHelper(row.title),
       requests: row.num_requests});
}
 
// Helper function with error handling
function decodeHelper(s) {
try {
   return decodeURI(s);
} catch (ex) {
   return s;
}
}

// UDF registration

bigquery.defineFunction('urlDecode', // Name used to call the function from SQL

['title', 'num_requests'], // Input column names

// JSON representation of the output schema
[{name: 'title', type: 'string'},{name: 'requests', type: 'integer'}],

urlDecode // The function reference
);
``` 

Ainsi, étant donné que les types de données disponibles sont moins nombreux et qu'il existe certaines limitations, comme le fait de ne pas pouvoir créer une logique d'entreprise partageable comme le fait la catégorie SQL standard, la catégorie SQL patrimoniale n'est pas une option viable.<br /><br />

## <h2>Validation du type de données cible lors de l'insertion.</h2><br />

Pour avoir une meilleure compréhension des types de données, regardons un peu de code.

Nous allons nous attaquer à l'instruction `insert` dans la catégorie SQL standard puisque c'est la catégorie suggérée par la documentation, en nous concentrant sur le type de données `STRUCT`. Ce type de données peut être un défi et est très courant lors de l'ingestion de données provenant de charges utiles d'API REST.

De plus, je pense que vous pourriez vous lasser des manipulations avec les seuls `Integers` et `Strings`. La commande suivante insère des données dans la table `DetailedInventory` sous le schéma `dataset`.

L'instruction SQL suivante, écrite en utilisant le langage SQL standard, va insérer des valeurs dans la table mentionnée avec certains types **STRUCT**.

Instruction d'insertion

```sql
INSERT dataset.DetailedInventory VALUES
('top load washer', 10, FALSE, [(CURRENT_DATE, "comment1")], ("white","1 year",(30,40,28))),
('front load washer', 20, FALSE, [(CURRENT_DATE, "comment1")], ("beige","1 year",(35,45,30)));
```

Aussi simple que démontré, il insère sans aucune complexité (vous pouvez voir à quoi ressemblent les enregistrements insérés ci-dessous).

Résultats de l'insertion
![ins-res](https://user-images.githubusercontent.com/78096758/217091354-471e4a7d-72f2-49b8-9db6-8608fc442d3d.png)

Lorsque vous interagissez avec vos données, vous devez être conscient de ***manipuler correctement chaque type de données***.

Une erreur courante consiste à comparer les formats de données de l'heure et de l'horodatage sans prendre les précautions nécessaires. Bien que les deux types de données puissent se ressembler beaucoup, cette erreur peut entraîner des ensembles de données inexacts.

Vérifiez également que la fonction que vous utilisez appartient à la bonne catégorie SQL Bigquery***.

Un bon exemple est la fonction `cast` sous [legacy SQL](https://cloud.google.com/bigquery/docs/reference/legacy-sql#cast) et sa référence sous [standard SQL](https://cloud.google.com/bigquery/docs/reference/standard-sql/functions-and-operators#cast), donc connaissez votre terrain avant de faire des changements à votre code.

## <h2>Conclusion</h2><br />
Comme nous l'avons vu, le type de SQL utilisé peut bloquer l'utilisation de certains types de données.

En raison des différentes manières de traiter les données, il existe une différence significative entre ***legacy SQL*** et ***standard SQL***, ce qui rend les choses bien plus complexes qu'un simple "indice" au début du SQL.

L'utilisation du bon type de données peut vous aider à contrôler vos données à un stade précoce. Cela pourrait signifier l'ajout d'une réaction automatisée aux données insuffisamment formatées, ce qui épargnerait à votre équipe de support de production quelques investigations. En effet, la même réaction pourrait prendre des heures pour identifier le problème de fond, sans compter le temps nécessaire pour le résoudre une fois identifié.

Parfois, vous pouvez même avoir besoin d'appliquer certaines règles prédéfinies pour traiter vos données en fonction des problèmes de traitement appris.

***Le SQL standard doit être préféré à l'utilisation du SQL hérité (Legacy SQL).***

Ce dernier ne dispose pas d'astuces sympas telles que [les fonctions de fenêtrage](https://cloud.google.com/bigquery/docs/reference/standard-sql/analytic-function-concepts) ou [un meilleur support lexical](https://cloud.google.com/bigquery/docs/reference/standard-sql/lexical) lors de la création de vos instructions SQL (bien meilleures que la simple notation par points du SQL hérité).

Ces informations sont précieuses lorsqu'il s'agit d'analyser les raisons pour lesquelles vos données présentent toujours des goulots d'étranglement lors de leur ingestion.

Si vous avez besoin d'aide pour transformer vos données, vous pouvez [nous contacter](https://calendly.com/dawrlog) pour toute question et même découvrir de nouvelles informations dont vous n'étiez pas conscient grâce à une meilleure compréhension de vos données.

À+!