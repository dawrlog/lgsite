---
title: "Les 10 optimales workflows Argo; avec examples."
date: 2022-07-12
# weight: 1
# aliases: ["/first"]
tags: ["Kubernetes", "Argo", "Development", "Infrastructure Observability"]
categories: ["Kubernetes", "Development", "Data Warehouse"]
# series: ["Data Stack"]
author: "Daniel Paes"
ShowCodeCopyButtons: true
showToc: true
TocOpen: false
draft: false
hidemeta: false
comments: false
# canonicalURL: "https://pipekit.io/blog/top-10-argo-workflows-examples"
disableHLJS: true # to disable highlightjs
# disableHLJS: false
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: false
ShowRssButtonInSectionTermList: true
UseHugoToc: true
cover:
    image: "https://user-images.githubusercontent.com/78096758/217081520-1f3b6771-8cf8-434b-b8fe-d11c17d380b5.png"

---
Bonjour à tous, 

Nous savons tous comment [Argo Workflows](https://argoproj.github.io/argo-workflows/) permet d'orchestrer facilement des travaux parallèles sur Kubernetes. Bien qu'il soit le plus souvent associé aux projets de traitement de données et d'ETL, il est utile pour bien d'autres choses ! Ces 10 workflows vont changer la façon dont vous voyez cet orchestrateur Kubernetes. 

Allons-y !

## <h2>Configuration des Workflows Argo</h2><br />
Si vous n'avez pas actuellement de workflow en cours d'exécution, je vous suggère de créer votre premier Argo Workflow pour comprendre ce dont nous allons parler dans ce post. Pour ce faire, suivez les instructions [ici](https://argoproj.github.io/argo-workflows/quick-start/) pour créer un déploiement local de Workflows Argo sur votre cluster. Je vous suggère également d'utiliser k3d pour votre plan de contrôle Kubernetes local ; ce tutoriel utilise un cluster [k3d](https://k3d.io/v5.3.0/#install-script) nommé argo. N'hésitez pas à reproduire la commande ci-dessous pour le créer dans votre environnement :

```sh
k3d cluster create argo
kubectl create -n argo -f https://raw.githubusercontent.com/argoproj/argo-workflows/master/manifests/quick-start-postgres.yaml
kubectl -n argo port-forward deployment/argo-server 2746:2746 & 
```
Maintenant, regardons notre premier exemple !

## <h2>1. Améliorer votre flux de travail à l'aide de paramètres</h2><br />
Argo utilise des [custom resource definitions](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) stockées dans des fichiers YAML pour gérer ses déploiements. Il n'est donc pas nécessaire d'apprendre de nouvelles spécifications pour gérer votre infrastructure ; vous pouvez suivre le même modèle que celui utilisé sur vos scripts Kubernetes et [Kustomize](https://kustomize.io/), ce qui vous aide à rester cohérent. Ci-dessous, nous pouvons voir comment utiliser les paramètres sur vos workflows, et passer des paramètres est pratique lorsque votre configuration utilise des valeurs d'exécution. Par conséquent, vous ne connaîtrez certains composants qu'après les avoir créés, comme les [jetons d'accès](https://en.wikipedia.org/wiki/Access_token).

```sh
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: hello-world-parameters-
spec:
  entrypoint: whalesay
  arguments:
    parameters:
    - name: message
      value: Message string default value 
  templates:
  - name: whalesay
    inputs:
      parameters:
      - name: message
    container:
      image: docker/whalesay
      command: [cowsay]
      args: ["{{inputs.parameters.message}}"] 
```

Dans notre modèle, le paramètre `message` aura la valeur par défaut de `Message string default value`. Cependant, cette valeur peut être écrasée au moment de l'exécution, comme nous pouvons le voir en exécutant la commande ci-dessous :


```sh 
argo submit -n argo param.yml -p message="This ran from your container" --watch 
```

Nous pouvons valider la sortie à partir de l'interface utilisateur des logs des workflows Argo. (Vous pouvez accéder à l'IU par défaut à `https://localhost:2746/` si vous suivez rapidement les instructions de transfert de port lors de la création de votre cluster).

## <h2>2. Extraire des images de votre référentiel sécurisé</h2><br />
L'une des fonctionnalités que j'apprécie lors de l'automatisation d'un écosystème est l'utilisation de clés d'accès rotatives tout en gérant l'accès de mes services. Ceci est utile dans les cas où votre entreprise utilise des dépôts de conteneurs privés pour héberger vos images de conteneurs. Les workflows Argo vous aident à réaliser cela grâce au support natif des secrets Kubernetes. Dans notre exemple, nous pouvons voir que le secret `docker-registry-secret` va tirer l'image `docker/whalesay:latest`.

```sh
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: hello-world-
spec:
  entrypoint: whalesay
  imagePullSecrets:
  - name: docker-registry-secret
  templates:
  - name: whalesay
    container:
      image: docker/whalesay:latest
      command: [cowsay]
      args: ["hello world"] 
```

## <h2>3. Utilisation des conteneurs Sidecar </h2><br />
L'une de mes choses préférées à faire est d'utiliser des sidecars lors du démarrage de mes pods. Les sidecars Kubernetes sont des aides utiles qui peuvent gérer des tâches récurrentes, comme la synchronisation de vos dépôts Git avec `git-sync` comme indiqué [ici](https://github.com/kubernetes/git-sync). Les flux de travail d'Argo couvrent ce problème avec un support soigné des conteneurs sidecars.

```sh 
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: sidecar-nginx-
spec:
  entrypoint: sidecar-nginx-example
  templates:
  - name: sidecar-nginx-example
    container:
      image: appropriate/curl
      command: [sh, -c]
      args: ["until `curl -G 'http://127.0.0.1/' >& /tmp/out`; do echo sleep && sleep 1; done && cat /tmp/out"]
    sidecars:
    - name: nginx
      image: nginx:1.13 
```
Pour le déployer, enregistrez le code ci-dessus sous le nom de `sidecar-nginx.yml` et soumettez-le :

```sh
argo submit -n argo sidecar-nginx.yml --watch 
```

Et comme résultat, vous allez déployer une instance sidecar de proxy inverse de NGINX.

**_Pro-tip : Vous devrez peut-être prêter une attention particulière à vos workflows si vous utilisez [Istio](https://istio.io/) comme Service mesh. Consultez ce fil de discussion GitHub si vous envisagez de l'utiliser._** 

## <h2>4. Archiver l'état actuel de votre workflow sur un stockage persistant</h2><br /><br />
L'archivage des workflows est une fonctionnalité intéressante fournie par Argo Workflows afin que vous puissiez avoir des états de workflow précédents stockés sur une base de données relationnelle (Postgres ou MySQL pour l'instant). Cependant, l'archive d'Argo ne conservera pas les journaux d'exécution détaillés ; vous devrez configurer un stockage d'objets comme [artefact repository](https://argoproj.github.io/argo-workflows/configure-artifact-repository/), il peut s'agir d'une option opensource comme [MinIO](https://min.io/), ou [AWS S3](https://aws.amazon.com/s3/) ; pour mentionner une option de fournisseur de cloud d'AWS. 

Pour utiliser la fonctionnalité d'archivage, vous devez d'abord configurer l'option de stockage persistant de votre serveur Argo. Pour ce faire, vous aurez besoin de plus d'informations sur la façon de la configurer. En suivant ce lien, vous pourrez vous familiariser avec les éléments d'authentification requis pour l'archive Argo ; puis, basez votre configuration sur [Controller configmap](https://argoproj.github.io/argo-workflows/workflow-controller-configmap/). Vous devrez les avoir configurés de manière appropriée avec votre serveur Argo pour bénéficier de cette fonctionnalité. Une fois qu'il est configuré, vous pouvez stocker vos flux de travail avec le `spec.archiveLocation.archiveLogs` comme démontré ci-dessous.

```sh 
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: archive-location-
spec:
  entrypoint: whalesay
  templates:
  - name: whalesay
    container:
      image: docker/whalesay:latest
      command: [cowsay]
      args: ["hello world"]
    archiveLocation:
      archiveLogs: true 
```

## <h2>5. Passer un dépôt Git comme artefact d'entrée</h2><br />
Une autre fonctionnalité intéressante fournie par Argo Workflows est la possibilité de synchroniser votre dépôt Git sans avoir besoin de sidecars ou de conteneurs init supplémentaires. Le code ci-dessous se connecte au [dépôt Argo](https://github.com/argoproj/argo-workflows.git) sur Github. 
Vous pouvez choisir entre des demandes de pull HTTP ou SSH pour l'élément d'authentification. Dans le premier modèle, git-clone, vous devrez utiliser la combinaison des secrets Kubernetes usernameSecret et passwordSecret pour accéder à une URL dans son format HTTP. Vous pouvez voir un exemple de configuration HTTP de Git dans le code ci-dessous.

```sh
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: input-artifact-git-
spec:
  entrypoint: git-clone
  templates:
  - name: git-clone
    inputs:
      artifacts:
      - name: argo-source
        path: /src
        git:
          repo: https://github.com/argoproj/argo-workflows.git
          revision: "v2.1.1"
          usernameSecret:
            name: github-creds
            key: username
          passwordSecret:
            name: github-creds
            key: password
    container:
      image: golang:1.10
      command: [sh, -c]
      args: ["git status && ls && cat VERSION"]
      workingDir: /src 
```

Argo Workflows supporte également la connectivité SSH (e.g., `git@github.com:argoproj/argo-workflows.git`). Cependant, il a besoin du format d'URL suivant la connectivité SSH et du secret Kubernetes `sshPrivateKeySecret` au lieu des secrets `usernameSecret` et `passwordSecret`.
<br />

## <h2>6. création de flux de travail de graphes acycliques dirigés</h2><br /><br />
J'ai l'impression que le graphe acyclique dirigé (DAG) reçoit maintenant l'attention qu'il mérite dans les domaines de l'analyse en raison de la façon impressionnante dont il gère les étapes de la charge de travail de traitement des données sur [Apache Spark](https://spark.apache.org/docs/3.2.1/index.html) et de son utilisation comme modèle commun d'orchestration des données avec Apache Airflow. Avec Argo Workflows, vous disposerez d'une interface conviviale Kubernetes au lieu de devoir configurer un exécuteur Kubernetes pour Airflow qui est moins stable. 

Je vous suggère de consulter ce [lien](https://www.techopedia.com/definition/5739/directed-acyclic-graph-dag) pour en savoir plus sur le fonctionnement d'un DAG. Ci-dessous, vous pouvez voir comment Argo Workflows l'instancie.

```sh 
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: dag-target-
spec:
  entrypoint: dag-target
  arguments:
    parameters:
    - name: target
      value: E

  templates:
  - name: dag-target
    dag:
      target: "{{workflow.parameters.target}}"

      tasks:
      - name: A
        template: echo
        arguments:
          parameters: [{name: message, value: A}]
      - name: B
        depends: "A"
        template: echo
        arguments:
          parameters: [{name: message, value: B}]
      - name: C
        depends: "A"
        template: echo
        arguments:
          parameters: [{name: message, value: C}]
      - name: D
        depends: "B && C"
        template: echo
        arguments:
          parameters: [{name: message, value: D}]
      - name: E
        depends: "C"
        template: echo
        arguments:
          parameters: [{name: message, value: E}]

  - name: echo
    inputs:
      parameters:
      - name: message
    container:
      image: alpine:3.7
      command: [echo, "{{inputs.parameters.message}}"] 
```

Chaque tâche sera passée au serveur Argo en utilisant le nom du paramètre `target`, avec les noms des cibles séparés par des espaces. Les workflows Argo n'exécuteront que celles que vous avez spécifiées ; cependant, ils exécuteront chaque dépendance jusqu'à ce qu'elle atteigne les cibles renseignées. En clair, disons que nous enregistrons notre fichier sous le nom de `dag-targets.yml` et l'exécutons en utilisant la commande suivante : 

```sh
argo submit -n argo dag-targets.yml -p target="B E" --watch
``` 

Il ignorera uniquement la **cible D**, comme le montre la démonstration ci-dessous :

## <h2>7. exécuter des scripts Python</h2><br /><br />
Les conteneurs facilitent déjà la gestion des environnements d'exécution. Il est donc facile de construire un conteneur Python avec les bibliothèques et la version dont vous avez besoin pour vos étapes de workflow basées sur Python.

Avec les workflows Argo, vous pouvez appeler un script Python déjà installé sur le conteneur par son nom, ou passer du code via un champ `source` dans la description du workflow. Vous pouvez spécifier n'importe quel code valide dans le bloc source. 

Voici un exemple :

```sh
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  name: python-random-number-generator
  namespace: argo
spec:
  entrypoint: generator
  arguments:
    parameters:
      - name: min
        value: 0
      - name: max
        value: 10
  templates:
  - name: generator
    inputs:
      parameters:
        - name: min
        - name: max
    script:
      image: python:3.8
      command: [python]
      source: |
        from random import randrange
        range_min = {{inputs.parameters.min}}
        range_max = {{inputs.parameters.max}}
        random_number = randrange(range_min, range_max)
        print("Random number: {}".format(random_number)) 
```

## <h2>8. implémentation d'une stratégie de relance</h2><br />
Parfois, plusieurs cibles peuvent implémenter une certaine logique de relance, et Argo Workflows configure votre [stratégie de relance](https://argoproj.github.io/argo-workflows/retries/) au niveau du Workflow.

```sh
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: retry-container-
spec:
  entrypoint: retry-container
  templates:
  - name: retry-container
    retryStrategy:
      limit: "3"
      retryPolicy: "OnError"
    container:
      image: python:alpine3.6
      command: ["python", -c]
      # fail with a 66% probability
      args: ["import random; import sys; exit_code = random.choice([0, 1, 1]); sys.exit(exit_code)"] 
```

Dans notre exemple, la cible `retry-container` essaiera de redémarrer trois fois dans les cas où elle se termine avec un statut Error sur Kubernetes.

## <h2>9. Ajout de flux de travail conditionnels</h2><br />
Les flux de travail conditionnels font également partie de mes préférés et sont si simples à mettre en œuvre. Vous pouvez déployer votre architecture en fonction des états de retour des étapes précédentes, ce qui est très pratique lorsque vous orchestrez un ensemble de conteneurs. Argo Workflows vous offre la possibilité d'exécuter des cibles basées sur une condition booléenne. Sous le capot, il utilise [govaluate](https://github.com/Knetic/govaluate) pour vous permettre d'utiliser [les instructions expr de Golang](https://github.com/antonmedv/expr). 

Vous serez donc en mesure d'orchestrer vos conditions de la même manière que vous gérez vos aides Golang dans votre écosystème Kubernetes - un autre avantage supplémentaire de l'utilisation des CRD Kubernetes.

```sh 
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: conditional-parameter-
  labels:
    workflows.argoproj.io/test: "true"
  annotations:
    workflows.argoproj.io/version: '>= 3.1.0'
spec:
  entrypoint: main
  templates:
    - name: main
      steps:
        - - name: flip-coin
            template: flip-coin
        - - name: heads
            template: heads
            when: "{{steps.flip-coin.outputs.result}} == heads"
          - name: tails
            template: tails
            when: "{{steps.flip-coin.outputs.result}} == tails"
      outputs:
        parameters:
          - name: stepresult
            valueFrom:
              expression: "steps['flip-coin'].outputs.result == 'heads' ? steps.heads.outputs.result : steps.tails.outputs.result"

    - name: flip-coin
      script:
        image: python:alpine3.6
        command: [ python ]
        source: |
          import random
          print("heads" if random.randint(0,1) == 0 else "tails")
    - name: heads
      script:
        image: python:alpine3.6
        command: [ python ]
        source: |
          print("heads")
    - name: tails
      script:
        image: python:alpine3.6
        command: [ python ]
        source: |
          print("tails") 
```
En enregistrant le code ci-dessus sous le nom de `cond.yml` et en l'exécutant avec `argo submit`, vous obtiendrez le résultat suivant :

```sh
argo submit -n argo cond.yml --watch 
```
<br />

![Argo Workflows conditional execution results](https://user-images.githubusercontent.com/78096758/217082060-b565c064-74fb-457a-b7ae-6035e81019bb.png#center)

## <h2> 10. Gestion des ressources Kubernetes à partir de votre flux de travail </h2><br />
Les workflows Argo peuvent créer des composants Kubernetes ; ceci est très pratique lorsque vous avez besoin de développer des actions kubelet temporaires de manière déclarative. Cette fonctionnalité suit le même principe que les scripts en ligne pour déployer les composants Kubernetes responsables de l'application des correctifs à votre environnement. Cependant, Argo Workflows gère les fichiers YAML en ligne de ce code [Kubernetes CRD](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#customresourcedefinitions).

```sh 
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: k8s-jobs-
spec:
  entrypoint: pi-tmpl
  templates:
  - name: pi-tmpl
    resource:                  
      action: create           
      successCondition: status.succeeded > 0
      failureCondition: status.failed > 3
      manifest: |               
        apiVersion: batch/v1
        kind: Job
        metadata:
          generateName: pi-job-
        spec:
          template:
            metadata:
              name: pi
            spec:
              containers:
              - name: pi
                image: perl
                command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
              restartPolicy: Never
          backoffLimit: 4 
```

Cette fonctionnalité vous permet d'exécuter directement toutes les actions kubectl, ce qui vous permet de créer/mettre à jour/supprimer n'importe quelle ressource Kubernetes sur votre cluster en utilisant les définitions en ligne des [groupes API Kubernetes](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/).

## <h2>Conclusion</h2><br />
Les progrès réalisés dans la gestion et le développement des systèmes nous donnent de nombreuses raisons d'être optimistes. Par exemple, [infrastructure as code](https://en.wikipedia.org/wiki/Infrastructure_as_code) vous permet d'avoir la même infrastructure sur vos serveurs évolutifs et sur votre poste de travail local. Des outils tels que Argo Workflows nous aident à créer une infrastructure évolutive prête à être mise en production sur notre poste de travail local, ce qui est en soi une bonne chose. 

Avec les changements constants des exigences de l'infrastructure comme le [DNS dynamique](https://en.wikipedia.org/wiki/Dynamic_DNS#:~:text=Dynamic%20DNS%20(DDNS)%20is%20a,hostnames%2C%20adresses%20ou%20autres%20informations.), vous devez adapter vos déploiements à une approche plus modulaire. Ces flux de travail sont les incontournables de tout administrateur DevOps. Mais cette liste n'est qu'un début. Je vous conseille vivement de mettre en œuvre ces scripts dans vos pipelines de développement et de données. 

À+!