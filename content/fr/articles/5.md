---
title: "Examples des ETL utilisant des workflows Argo."
date: 2022-08-14
# weight: 1
# aliases: ["/first"]
tags: ["Kubernetes", "Argo", "Development", "Data Warehouse"]
categories: ["Kubernetes", "Development", "Data Warehouse"]
# series: ["Data Stack"]
author: "Daniel Paes"
ShowCodeCopyButtons: true
showToc: true
TocOpen: false
draft: false
hidemeta: false
comments: false
# canonicalURL: "https://pipekit.io/blog/argo-workflows-etl-examples"
disableHLJS: true # to disable highlightjs
# disableHLJS: false
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: false
ShowRssButtonInSectionTermList: true
UseHugoToc: true
cover:
    image: "https://user-images.githubusercontent.com/78096758/217079803-11a4c93a-7ae1-4273-b067-499a0d67023a.png"

---

Bonjour à tous, 

Ce post explore l'utilisation des [Argo Workflows](https://argoproj.github.io/argo-workflows/) pour orchestrer vos pipelines de données. Pour commencer, rappelons ce qu'est l'ETL tout en concevant l'architecture de haut niveau de notre travail. Je montrerai comment configurer vos pipelines de données pour qu'ils suivent plus naturellement la structure. Ensuite, nous verrons comment obtenir le même résultat en utilisant des [graphes acycliques dirigés](https://en.wikipedia.org/wiki/Directed_acyclic_graph) (DAGs). Enfin, je résumerai ce que nous avons vu et présenterai des raisons pour guider votre choix d'approche en fonction de la complexité de votre projet. Alors, commençons. 

### <h2>Que signifie ETL ? </h2><br />
Tout d'abord, rappelons ce qu'est l'ETL avant de commencer nos exemples. [L'extraction, la transformation et le chargement](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/etl-architecture-34-subsystems/) consistent en des tâches visant à nettoyer vos données et à regrouper les données de vos applications dans une base de données conforme. Imaginez cette base de données conforme comme la source unique de vérité de vos données. Ce référentiel centralisé vous aide à mieux connaître vos produits et vos clients. 

Cependant, chaque application a sa propre structure pour traiter les données. Chaque tâche ETL rend les données de l'application plus attrayantes pour l'analyse, en ayant des dépendances explicites à mesure que votre processus de [data wrangling](https://en.wikipedia.org/wiki/Data_wrangling) devient plus robuste. Notre code va déployer quatre tâches ETL et leur relation, comme le reflète l'image ci-dessous :

![exemples d'ETL](https://user-images.githubusercontent.com/78096758/217080436-2a286a21-2aff-40ab-9d83-1b874c8e5c6b.png#center)
_Architecture de haut niveau des tâches ETL_

### <h2>Comprendre chaque tâche ETL </h2><br />
Ce flux de travail prend en charge deux formats de données différents : colonne ou ligne. Ces différents formats nécessitent des analyseurs différents : parquet pour les colonnes et avro pour les lignes. Nous pouvons également nous y référer en tant que batch ou stream, respectivement.

Le flux de travail commence donc par un gestionnaire de requête qui identifie le type de données. En fonction de la valeur d'un indicateur de type de données, il transmet la tâche aux balises d'analyse batch ou stream.

Les analyseurs transmettent leurs résultats à la tâche Load Data, où ils sont chargés dans le stockage persistant, en fonction de la source et du type de données.

Bien qu'il semble s'agir d'un seul flux de travail avec deux chemins de code différents, il y a des avantages à traiter les deux types de données dans le même ensemble de tâches. Le partage d'une tâche de chargement entre les flux de données facilite la gestion des chevauchements et des relations. Par exemple, le magasin de données en colonnes peut avoir besoin d'être mis à jour avec des clés étrangères provenant de nouvelles données en lignes. Il est également plus facile de partager un seul flux ETL entre différentes équipes.

Maintenant que nous savons ce que nous allons construire, commençons par voir comment le mettre en œuvre en utilisant les étapes des workflows Argo.

![Souvenez-vous](https://user-images.githubusercontent.com/78096758/217080713-bbd06f86-f41d-45bf-87d6-c1981b97776c.png#center)
<br />

### <h2>Construire un pipeline ETL en utilisant les étapes des workflows Argo</h2> <br />
Dans cette approche, le pipeline de données va suivre une liste d'étapes pour nettoyer et traiter les données de vos sources de données. Votre code ETL devient encore plus robuste lorsque nous utilisons des conditionnels pour informer quel flux ETL vos données doivent prendre. Cela semble bien, non ? Alors, mettons nos mains dans le cambouis et voyons comment cela fonctionne en pratique. 

Le code ci-dessous va créer un workflow dans un espace de nom appelé `argo`. Cet espace de nom doit exister avant que le workflow ne soit exécuté avec `argo submit`. Cela permet d'éviter les problèmes de sécurité, comme le fait que votre utilisateur n'ait pas le droit de créer des espaces de noms. Cela évitera également les messages d'erreur vous avertissant de ne pas casser votre déploiement Kubernetes. Pour notre exemple, nous allons générer une valeur aléatoire sur une machine Linux et charger les données à venir en fonction de cette valeur. 

Bien que les deux étapes d'analyse syntaxique soient déclenchées simultanément, seule celle informée par l'étape de traitement des demandes s'exécutera. L'utilisation d'un code automatisé comme celui-ci réduira les chances d'avoir des problèmes avec notre flux de données ETL. L'automatisation de vos étapes de flux de travail gère les erreurs courantes telles que les types de mismatch dans votre base de données.

```sh 
apiVersion: argoproj.io/v1alpha1 
kind: Workflow 
metadata: 
  generateName: stream-batch-parser-  
  namespace: argo
spec: 
  entrypoint: sbp 
  templates: 
  - name: sbp 
    steps: 
    - - name: request-handler 
        template: edge-input 
    - - name: parser-stream 
        template: stream 
        when: "{{steps.handle-requests.outputs.result}} <= 163883" 
      - name: parser-batch 
        template: parquet 
        when: "{{steps.handle-requests.outputs.result}} > 163883" 
  - name: stream 
    steps: 
    - - name: avro-parser 
        template: avro 
    - - name: wrapper
        template: wrapper 
  - name: batch 
    steps: 
    - - name: parquet-parser 
        template: parquet 
    - - name: wrapper
        template: wrapper 
  - name: edge-input
    container: 
      image: alpine:3.6 
      command: [sh, -c]
      args: ["echo ${RANDOM}"] 
  - name: parquet 
    container: 
      image: alpine:3.6 
      command: [sh, -c]
      args: ["echo \"code to parse Parquet\""] 
  - name: avro 
    container: 
      image: alpine:3.6 
      command: [sh, -c]
      args: ["echo \"code to parse Avro\""]
  - name: wrapper
    container: 
      image: alpine:3.6 
      command: [sh, -c]
      args: ["echo \"code to load into Staging\""] 
```
<br />
Sauvegardez le fichier ci-dessus en tant que `etl_steps.yml` et démarrez votre flux de travail avec cette commande :

```sh 
argo submit -n argo etl_steps.yml
``` 

Nous pouvons maintenant obtenir le statut de notre workflow en exécutant la commande `argo get` suivante :

```sh 
argo get -n argo stream-batch-parser-xxvks
``` 

Les cinq derniers chiffres seront différents dans chaque environnement. Et en exécutant la commande précédente, votre journal de sortie devrait être similaire à l'image ci-dessous ; comme indiqué, notre workflow exécutera la tâche de flux d'analyseur en fonction de la valeur renvoyée par la tâche de traitement des demandes. 

Sortie des étapes ETL du workflow Argo
Etapes ETL retournées par l'exécution de la commande `argo get`.
Maintenant que nous avons vu comment construire un ETL avec des tâches, nous allons explorer comment utiliser les DAGs pour votre ETL.

### <h2>Construire un pipeline ETL avec des DAGs au lieu d'étapes </h2><br />
Maintenant, explorons comment réaliser le même travail en utilisant des modèles de DAGs au lieu d'étapes dans Argo Workflows. Même si le DSL semble similaire à première vue, les DAGs vous donnent plus de pouvoir pour spécifier les dépendances entre les étapes et exécuter les tâches en parallèle.

Dans un DAG, toute tâche peut être exécutée lorsque ses dépendances sont satisfaites. Si les dépendances de plus d'une tâche sont satisfaites, toutes les tâches seront exécutées en parallèle. Si une tâche n'a pas de dépendances, elle s'exécutera dès que le workflow sera lancé. Les DAGs sont excellents pour traiter l'ETL, et je vous suggère fortement de vous familiariser avec toutes les options qu'une tâche DAG peut fournir en consultant la [documentation officielle d'Argo](https://argoproj.github.io/argo-workflows/fields/#dagtask).

```sh 
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: dag-orchestrate-
  namespace: argo
spec:
  entrypoint: sbp
  archiveLogs: true
  templates:
    - name: sbp
      dag:
        tasks:
          - name: request-handler
            template: edge-input
          - name: stream-flow
            template: stream
            when: "{{tasks.handle-requests.outputs.result}} <= 163883" 
            depends: handle-requests
          - name: batch-flow
            template: batch
            when: "{{tasks.handle-requests.outputs.result}} > 163883" 
            depends: handle-requests
    - name: stream 
      steps: 
      - - name: avro-parser 
          template: avro 
      - - name: wrapper
          template: wrapper 
    - name: batch 
      steps: 
      - - name: parquet-parser 
          template: parquet 
      - - name: wrapper
          template: wrapper             
    - name: edge-input
      container: 
        image: alpine:3.6 
        command: [sh, -c]
        args: ["echo ${RANDOM}"] 
    - name: parquet 
      container: 
        image: alpine:3.6 
        command: [sh, -c]
        args: ["echo \"code to parse Parquet\""] 
    - name: avro 
      container: 
        image: alpine:3.6 
        command: [sh, -c]
        args: ["echo \"code to parse Avro\""]   
    - name: wrapper
      container: 
        image: alpine:3.6 
        command: [sh, -c]
        args: ["echo \"code to load into Staging\""] 
```
Enregistrez le fichier ci-dessus en tant que `etl_dag.yml` et soumettez votre flux de travail pour le lancer :  

```sh 
argo submit -n argo etl_dag.yml
``` 

Comme démontré ci-dessous, vous pouvez vérifier son évolution avec argo get :

```sh 
argo get -n argo dag-orchestrate-ctpkl
```

Dans ce scénario, notre workflow a exécuté la tâche de flux batch au lieu du flux stream en fonction de la valeur renvoyée par la tâche handle requests.
<br />

![etl examples](https://user-images.githubusercontent.com/78096758/217081047-efbc4d79-5f38-4e9f-8c2c-d00be60521ea.png#center)
<br />

Félicitations pour votre travail ! Vous pouvez maintenant concevoir vos flux de données ETL en utilisant un DAG ou une liste structurée d'étapes dans Argo Workflows. 

N'oubliez pas de nettoyer votre environnement avec `argo delete -n argo your-workflow`, où vous devez informer le workflow désiré comme `votre-workflow`.

```sh 
argo delete -n argo your-workflow
``` 

### <h2>Conclusion </h2><br />
Bien qu'il soit généralement utilisé pour la gestion de l'infrastructure, Argo Workflows peut également orchestrer vos tâches ETL. En l'utilisant ainsi, vous n'avez plus besoin de recourir à différents outils pour atteindre le même objectif, c'est-à-dire Argo pour le CI/CD et Airflow pour les tâches ETL. 

L'approche DAG est souvent meilleure que l'approche par étapes pour l'exécution des pipelines ETL. Pour commencer, le traitement des tâches DAG est optimisé au moment de l'exécution. Vous aurez moins de points de décision pour certains de vos pipelines simplement en informant le flux de données souhaité. 

Pour les tâches simples, les flux séquentiels (comme vous obtenez avec l'approche par étapes dans les workflows Argo) fonctionnent bien. Cependant, ils deviennent plus difficiles à maintenir dans les cas où vous devez cibler un sous-ensemble de votre flux de données et gérer des dépendances complexes dans le temps. 

Un autre avantage de l'utilisation des DAGs est de pouvoir spécifier l'étape exacte au moment de l'exécution. L'exécution vous donne plus de liberté pour créer un code conditionnel avec moins de boucles indentées tout en optimisant le code et les ressources de l'infrastructure.

Je vous invite à approfondir la documentation d'Argo Workflows sur les DAGs. Maîtriser le fonctionnement des DAGs peut augmenter la qualité de vos pipelines ETL, en vous permettant de gérer vos tâches ETL de manière plus dynamique par rapport à la méthode des étapes. 

Pour des moyens plus optimisés de gérer vos ressources Kubernetes, explorez comment Pipekit peut vous aider à orchestrer l'ensemble de votre déploiement Argo Workflows.

À+!