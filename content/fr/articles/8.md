---
title: "Chargements des données incrementels: Un moyen nickel de charger vos données."
date: 2022-12-24
# weight: 1
# aliases: ["/first"]
tags: ["Data Warehouse", "Best Practices"]
categories: ["Data Modeling"]
# series: ["Best Practices"]
author: "Daniel Paes"
# author: ["Me", "You"] # multiple authors
showToc: true
TocOpen: false
draft: false
hidemeta: false
comments: false
# canonicalURL: "https://blog.panoply.io/incremental-loading"
disableHLJS: true # to disable highlightjs
disableShare: false
disableHLJS: false
hideSummary: false
searchHidden: false
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: false
ShowRssButtonInSectionTermList: true
UseHugoToc: true
cover:
    image: "https://user-images.githubusercontent.com/78096758/217088971-06b427ad-92dc-48d3-8c87-ebafc8e2ee1b.png" 
---

Salut à tous, j'espère que tout va bien. 

Un entrepôt de données vise à donner du sens à un sujet spécifique dans le temps en analysant les données historiques. Ce système, également appelé **système d'aide à la décision**, peut s'attaquer à des tendances aussi diverses que le pourcentage de désabonnement des clients ou les tendances de la consommation de bière dans une zone géographique.

Le chargement incrémentiel est l'une de ces questions cruciales que vous devez prendre en compte lorsque vous définissez vos pipelines de chargement.

Dans ce billet, je vais expliquer ce qu'est le chargement incrémental et pourquoi il est si important. Je poursuivrai en définissant certains composants clés et terminerai par une brève conclusion.

Qu'est-ce que le chargement incrémental, et pourquoi en avons-nous besoin ?
Également connu sous les noms de ***change data capture*** et ***delta load***, le chargement incrémental est chargé de prendre un instantané des données les plus récentes. Il est essentiel de s'assurer que votre entrepôt de données est cohérent et fiable.

Cela dit, pour comprendre les avantages d'un chargement incrémentiel, vous devez savoir ce qu'est un entrepôt de données.

Je vais donc commencer par vous expliquer les étapes du pipeline de données que vous devez suivre pour charger les bases de données de votre entrepôt de données. Cela devrait vous permettre de comprendre de façon claire et nette pourquoi c'est essentiel.

En termes simples, les ***entrepôts de données*** sont des bases de données qui soutiennent le ***processus décisionnel***. Il est plus sûr de dire qu'ils vous aident à donner un sens à vos données historiques, car l'un des objectifs des entrepôts de données est de permettre à leurs utilisateurs d'analyser les données dans le temps.

Vous trouverez ci-dessous un bref rappel des phases les plus utilisées des entrepôts de données selon l'approche **Kimball** :
<br />

## <h3>Phases de l'entrepôt de données en utilisant Kimball</h3><br />

- [Online Transactional Processing](https://docs.microsoft.com/en-us/azure/architecture/data-guide/relational-data/online-transaction-processing) (OLTP / Staging area / Landing Zone) : Les données sont extraites de leurs systèmes sources vers un stockage transitoire. Il s'agit de données à l'état brut, aussi proches que possible de leur source.
- [Operational Data Stores](https://philosophy-question.com/library/lecture/read/51614-what-is-operational-data-store-in-data-warehouse) (également connu sous le nom de ODS) : Une base de données avec tous les processus de qualité nécessaires et les corrélations entre les différentes sources. Les données sont ici dans un état préparé pour les utilisateurs.
- [Data Warehouse](https://www.techopedia.com/definition/1184/data-warehouse-dw) (également connu sous le nom de DW / EDW) : Ici, les données sont stockées dans des tableaux appelés dimensions ou faits. La redondance des données n'est pas un problème car elles montrent leur évolution dans le temps.
- cubes [Online Analytical Processing](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/star-schema-olap-cube/) (également connu sous le nom de OLAP / Presentation Layer) : Ce sont les données finales auxquelles les utilisateurs accèdent pour leurs rapports, avec des métriques spécifiques si nécessaire.
Vous ne pouvez pas (ou du moins vous ne devriez pas) avoir des informations détaillées telles que des adresses e-mail dans votre entrepôt de données.

Comme la zone de transit n'est pas prête pour les transactions de base de données, vous devez interroger vos données au début de la phase transactionnelle pour ce type de demande. L'ODS fonctionne mieux pour les requêtes opérationnelles telles que "qui a fait quoi où ?".

Vous êtes toujours avec moi ? Super, ça en vaut la peine !

Maintenant que nous avons couvert nos bases, nous pouvons passer à ce que vous devez considérer lors de la conception de votre structure de chargement.

Comment votre entrepôt de données est chargé
Chaque application contenant des données significatives pour votre entrepôt de données est appelée une ***source de données***.


Chaque pompe de source de données a ses propres contraintes lors du chargement des données dans un entrepôt de données, subdivisées en deux groupes principaux : les chargements [streaming](https://databricks.com/blog/2017/01/19/real-time-streaming-etl-structured-streaming-apache-spark-2-1.html) et [batch](https://askinglot.com/what-is-bulk-load-in-data-warehouse).

- Nous utilisons l'approche ***streaming*** lorsque les données sont disponibles en continu. Imaginez les données de votre tracker de santé provenant de votre smartwatch.
- Nous utilisons les techniques de ***batch*** lorsque votre source de données fournit des données en masse, comme les données des points de vente d'une application de détaillant.


La seconde est plus courante lorsque votre système source a un temps de chargement spécifique pour éviter la concurrence des processus internes.

Ce que l'on appelle ***micro-batch*** est un mélange des deux car il combine l'approche de chargement continu avec la périodicité que l'on trouve dans l'approche par lots. Les micro-lots sont pratiques lorsque vos données arrivent en flux constant, mais pas plus de dix minutes en petits lots.

Maintenant que vous avez une compréhension de haut niveau de la façon dont votre entrepôt de données est rempli, nous pouvons passer à l'étape suivante : _identifier les clés métier_.
<br /><br />

## <h3>Qu'est-ce que les clés métiers ("Business keys" de l'Anglais) ?</h3><br />
Chaque source de données possède ce que l'on appelle des **_clés métier_**. Ces clés nous aident à identifier ce qui donne à chaque enregistrement son caractère unique d'un point de vue commercial pour cette application spécifique.

Il est important de disposer d'un référentiel central solide où vous pouvez trouver toutes les informations sur vos sources de données. Un catalogue de données peut être utile à cet égard. Il s'agit d'un référentiel central contenant toutes vos sources de données que vous pouvez explorer.

Il vous sera utile si vous disposez de clés métier pour identifier les enregistrements uniques. En raison du haut niveau de détails disponibles, le tableau ci-dessous a une cardinalité plus grande ou **_haute granularité des données_**.
<br />

***Cardinality example:***

| ID  | 	Name	    | Profession	| Address |
| :-: | --------------- | ------------- | ------------------------ | 
| 1	  | Adam Smith	    | Fireman	    | 4389 Boul. Faraway App 6 |
| 2	  | John Smith	    | Croupier	    | 4389 Boul. Faraway App 1 |
| 3	  | Juliette Bull   | Saleswoman	| 64 Holest Nawy App 6 |
| 4	  | Lucille	        | Croupier	    | 1234 Runaway Train |
| 5	  | Samantha	    | Policewoman	| 4389 Galway Road |
<br/> 

Nous pouvons utiliser une combinaison de **Nom** et **Profession** dans cet exemple. Ce sont les **clés commerciales** ou les champs qui donnent une unicité à des fins commerciales.

Par exemple, nous pouvons utiliser l'identifiant ou *ID* comme _clé primaire unique_, mais cela ne servira pas à grand-chose s'il fait référence à un enregistrement différent de ceux de l'exemple ci-dessus.

Une fois que les clés métier sont claires, nous pouvons définir nos attributs et nos métriques : sur la base desquels nous pouvons comprendre les tendances des données dans nos sources de données, comme les métriques et attributs pertinents qui donnent des informations précises.
<br />

## <h3>Quel est le rapport entre les clés commerciales et les clés incrémentielles ?</h3><br />
Une fois que vous savez comment identifier les clés métier de votre source de données, vous devez déterminer quel champ correspondra à la clé incrémentielle de votre chargement. En plus des clés métier, la clé incrémentielle est responsable du chargement de la **nouvelle version de vos données**.

Sur la base des clés métier et incrémentielles, vous serez correctement en mesure de charger uniquement la dernière version des données. Cette version mettra à jour sa version existante dans la phase de staging/ODS.

Ce faisant, votre entrepôt de données sera mis à jour avec une nouvelle version de votre enregistrement, laissant la version précédente avec un drapeau déprécié sur elle.

Dans ce scénario, je ne parle pas des dimensions qui changent lentement. Les clés métier et d'intégration sont celles qui nous donnent ce que nous appelons la ***unicité de l'enregistrement***, ou ce qui le rend unique, créant ce que l'on appelle **le versionnage des données**.

Le versionnage des données nous permet de faire la différence entre l'état actuel de l'enregistrement et ses états précédents. Il permet ce que l'on appelle l'"analyse temporelle" du document. Le versionnement des données sur les systèmes sources est censé exister sur la phase ODS de vos processus de pipelines de données.

## <h3>Conclusion</h3><br />
Dans cet article, nous avons abordé le **chargement incrémental** et son importance pour l'**intégrité des données**.

Nous avons commencé par présenter d'autres noms pour ce type de chargement - le chargement delta et la capture des données de changement - et avons passé en revue ce qui constitue un entrepôt de données basé sur la méthodologie Kimball, puis les types d'approches pour les chargements de données en continu ou en masse.

Nous avons terminé en expliquant les **clés métier et incrémentales** et en les distinguant des clés système.

Il est toujours bon de vous conseiller d'utiliser l'option de chargement incrémentiel lorsque vous interrogez vos données. En procédant ainsi, vous supprimez les vues dupliquées de votre instantané de données, ce qui réduit la charge lors de la gestion de vos pipelines d'ingestion de données.

Le chargement incrémentiel est **fortement recommandé** (voire obligatoire) lors de la définition et du développement de vos pipelines de données, en particulier dans la phase ODS. Il peut vous aider à charger correctement les données de vos sources de données en **chargeant correctement votre table** et même en échelonnant vos pics d'ingestion de données. En divisant les données qui donnent ces goulots d'étranglement sur vos processus de chargement dans différents pipelines.


À+!