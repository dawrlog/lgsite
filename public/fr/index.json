[{"content":"Salut tout le monde, j\u0026rsquo;espère que tout va bien.\nMême la base de données la plus puissante est toujours sujette à l\u0026rsquo;erreur terrifiante que tout système de base de données peut rencontrer : les goulots d\u0026rsquo;étranglement des requêtes.\nParfois, les systèmes qui gèrent nos requêtes ne peuvent pas traiter de la même manière que nous. Ainsi, nos questions renvoient des résultats précis grâce à une puissance de traitement supplémentaire.\nDans cet article, je vais présenter le fonctionnement des files d\u0026rsquo;attente de requêtes Redshift. Nous couvrirons également le fonctionnement de la concurrence à un haut niveau sur Redshift.\nCes informations nous seront utiles lorsque nous descendrons plus loin dans le terrier et que nous superviserons le déploiement d\u0026rsquo;un bon ami, bien que parfois menaçant, la gestion de la charge de travail (ou WLM) sur Redshift.\nNous terminerons par une conclusion rapide de ce que nous avons vu et par quelques techniques d\u0026rsquo;optimisation des requêtes.\nLes optimisations de requêtes sont très pratiques car elles aident à nettoyer le mauvais code. Puis d\u0026rsquo;augmenter la vitesse de vos transformations, ce qui permet d\u0026rsquo;explorer plus de données avec moins. En bref, la bonne vieille frugalité des données.\nQu\u0026rsquo;est-ce que Redshift, et comment fonctionne le traitement des requêtes?\nRedshift est la version d\u0026rsquo;Amazon d\u0026rsquo;un système de base de données à traitement massivement parallèle (ou MPP).\nCe type de base de données est le premier choix pour le stockage d\u0026rsquo;entrepôts de données grâce à ses capacités de traitement de quantités massives de données en parallèle. Quant aux deux autres grands fournisseurs de services en nuage, il y a BigQuery sur Google Cloud Platform et Synapse Analytics sur Microsoft Azure.\nExaminons maintenant plus en détail le processus d\u0026rsquo;interrogation.\nTraitement des requêtes Une fois que la requête est lancée (soit par la console, soit par un accès pragmatique - les commandes API et CLI suivent la même procédure), elle génère un plan de requête, qui est la traduction de la requête effectuée par l\u0026rsquo;analyseur syntaxique pendant l\u0026rsquo;extraction des données des nœuds.\nAvec ce plan d\u0026rsquo;exécution, nous pouvons commencer à inspecter les goulots d\u0026rsquo;étranglement de votre code.\nVoici un exemple d\u0026rsquo;une commande EXPLAIN Redshift :\nexplain select lastname, catname, venuename, venuecity, venuestate, eventname, month, sum(pricepaid) as buyercost, max(totalprice) as maxtotalprice from category join event on category.catid = event.catid join venue on venue.venueid = event.venueid join sales on sales.eventid = event.eventid join listing on sales.listid = listing.listid join date on sales.dateid = date.dateid join users on users.userid = sales.buyerid group by lastname, catname, venuename, venuecity, venuestate, eventname, month having sum(pricepaid)\u0026gt;9999 order by catname, buyercost desc; A partir des résultats EXPLAIN ci-dessus, je veux identifier les tables : category, venue, sales, listing, date, et users. Chacune d\u0026rsquo;entre elles utilise la clause INNER JOIN.\nLa puissance de Redshift repose sur un traitement lourd, donc plus ces tables sont grandes, mieux c\u0026rsquo;est pour vous, sur le plan informatique.\nMais comment pouvez-vous le savoir ? Plus profondément, comment pouvez-vous identifier les plus petites tables pour que vous puissiez prendre les métriques et les mesures nécessaires si importantes pour vos KPI ?\nC\u0026rsquo;est ici que le plan de requête est utile ; ci-dessous, vous pouvez voir toutes les étapes que Redshift exécute en fonction du SQL que vous avez écrit. C\u0026rsquo;est donc ici que Redshift vous dit si ce que vous avez écrit est ce que Redshift a compris.\nVoici le résultat de la commande EXPLAIN ci-dessus :\nRésultats du plan de requête._\nMaintenant que nous savons comment créer le plan de requête, nous pouvons approfondir l\u0026rsquo;optimisation des requêtes, qui n\u0026rsquo;est rien d\u0026rsquo;autre que le remaniement de vos requêtes pour réduire les coûts de traitement décrits par les étapes du plan de requête.\nQu\u0026rsquo;est-ce que la gestion de la charge de travail, ou WLM ?\nL\u0026rsquo;un des moyens les plus rapides de gérer vos flux de requêtes est la gestion de la charge de travail. Avec cette fonction, vous ne sacrifiez pas votre capacité à répondre à des questions rapides en raison de processus longs, car elle permet une certaine flexibilité dans la gestion de vos charges de travail.\nImaginons le scénario suivant :\nVotre data scientist principal déploie certains modèles d\u0026rsquo;apprentissage automatique pour détecter d\u0026rsquo;éventuelles activités frauduleuses. Ces activités doivent être croisées avec l\u0026rsquo;emplacement géographique des dernières transactions. Ensuite, ces microservices indépendants chaotiques commencent à s\u0026rsquo;exécuter sur vos clusters Redshift au moment précis où vos indicateurs de performance clés déclenchent de nouveaux processus sur le même cluster Redshift. C\u0026rsquo;est drôle, non ?\nWLM vient à la rescousse, car il crée ce que l\u0026rsquo;on appelle des queues de requêtes au moment de l\u0026rsquo;exécution. WLM regroupe ces files d\u0026rsquo;attente par un label de groupe de requêtes défini par l\u0026rsquo;utilisateur avant l\u0026rsquo;exécution de la requête.\nCes files d\u0026rsquo;attente ont des niveaux de concurrence, c\u0026rsquo;est-à-dire le nombre de charges de travail lancées en même temps. Pour nous habituer à Workload Manager, ou WLM\nWLM est proposé avec deux types d\u0026rsquo;implémentations : automatique, où Redshift se charge de gérer la mémoire de vos requêtes et l\u0026rsquo;allocation de la concurrence, et manuel, où vous fournissez ces valeurs à la place.\nCi-dessous, je souhaite partager les tableaux et vues système utiles qui devraient être utilisés comme points de départ lorsque vous avez besoin d\u0026rsquo;améliorer, ou simplement de vérifier, vos charges de travail WLM.\nSTL_WLM_ERROR STL_WLM_QUERY STV_WLM_CLASSIFICATION_CONFIG STV_WLM_QUERY_QUEUE_STATE STV_WLM_QUERY_STATE STV_WLM_QUERY_TASK_STATE STV_WLM_SERVICE_CLASS_CONFIG STV_WLM_SERVICE_CLASS_STATE Les choses à faire et à ne pas faire en matière d\u0026rsquo;échelonnement de la concurrence.\nUne fois que vous avez activé votre WLM, vous activez une fonction pratique utilisée sur le traitement massif : mise à l\u0026rsquo;échelle concomitante ! Vous percevrez les gains du processus d\u0026rsquo;écriture dans la cible pour la cohérence sur un débit plus élevé avec des demandes de sessions multiples.\nToujours est-il que lorsque la fonctionnalité est active, la mise à l\u0026rsquo;échelle concurrentielle est appliquée pour les opérations read et write. Il prend également en charge les instructions SQL Data Manipulation Language, les bons vieux INSERT, DELETE et UPDATE. Il prend également en charge l\u0026rsquo;instruction CREATE et Redshift COPY, ce qui vous permet de couvrir la plupart de vos charges de données quotidiennes.\nLes limitations les plus critiques de la mise à l\u0026rsquo;échelle de la concurrence sont la limitation de ANALYZE pour les commandes COPY et l\u0026rsquo;impossibilité d\u0026rsquo;utiliser COPY à partir de Redshift Spectrum ou lorsque vous interrogez des données à partir de votre stockage HDFS dans vos clusters EMR. Étant donné que la commande COPY est l\u0026rsquo;option suggérée pour importer des données massives dans des tables Redshift natives, ne pas l\u0026rsquo;exécuter en parallèle peut limiter l\u0026rsquo;utilisation de la commande COPY pour votre cas d\u0026rsquo;utilisation.\nIl est également important de se rappeler que vous devez confirmer si la région où se trouve votre cluster Redshift dispose de la fonction de mise à l\u0026rsquo;échelle de la concurrence ; pour une référence rapide, regardez ici.\nChangez l\u0026rsquo;option Concurrency Scaling Mode en auto sur votre file d\u0026rsquo;attente WLM, et ce faisant, vous activerez le routage de vos requêtes dans les clusters de mise à l\u0026rsquo;échelle de la concurrence. Obtenir des statistiques pour votre WLM\nAvec votre WLM activé et vos files d\u0026rsquo;attente avec le \u0026ldquo;Concurrency Scaling Mode\u0026rdquo; activé sur auto, vous pouvez suivre ce qui se passe dans votre cluster.\nPour ce faire, vous pouvez aller dans l\u0026rsquo;éditeur de requêtes de votre choix ou dans la console Redshift et exécuter la requête suivante : Requête SQL pour le suivi des files d\u0026rsquo;attente concurrentes\nSELECT w.service_class AS queue , q.concurrency_scaling_status , COUNT( * ) AS queries , SUM( q.aborted ) AS aborted , SUM( ROUND( total_queue_time::NUMERIC / 1000000,2 ) ) AS queue_secs , SUM( ROUND( total_exec_time::NUMERIC / 1000000,2 ) ) AS exec_secs FROM stl_query q JOIN stl_wlm_query w USING (userid,query) WHERE q.userid \u0026gt; 1 AND q.starttime \u0026gt; # Time on format: \u0026#39;YYYY-MM-DD 24HH:MI:SS\u0026#39; AND q.endtime \u0026lt; # Time on format: \u0026#39;YYYY-MM-DD 24HH:MI:SS\u0026#39; GROUP BY 1,2 ORDER BY 1,2; Les résultats de la requête vous fourniront un spectre complet de ce qui se passe sur votre cluster, en accordant toutes les informations nécessaires du point de vue de la gestion du cluster Redshift. Toutes ces informations sont utiles lorsque vous cherchez à savoir quelles sessions sont actives ou non sur vos ensembles (vous pouvez vérifier à quoi ressemblerait le résultat ci-dessous).\nRésultats des requêtes\nEn plus des mesures collectées par AWS CloudWatch et AWS CloudTrail, vous disposerez d\u0026rsquo;un environnement entièrement conforme. Tout cela utilise les services AWS natifs, ce qui vous évite quelques maux de tête supplémentaires.\nCette configuration fonctionnera comme une couche de cohérence en plus de vos pipelines de données bien gérés et matures.\nConclusion\nDans cet article, nous avons supervisé le fonctionnement de la concurrence sur Redshift. Nous avons également vu comment analyser et améliorer les plans de charge des requêtes : les rendre plus rapides tout en consommant moins de puissance de traitement (vous pouvez interpréter cela comme réduire votre facture de cloud).\nRappelez-vous simplement les tables de contrôle et les vues mentionnées, et vous serez prêt. Leur maîtrise vous aidera également à vérifier les éventuels conflits avec les tables de la requête.\nIl est également conseillé d\u0026rsquo;exécuter périodiquement les commandes ANALYZE et VACUUM.\nSi vous avez besoin d\u0026rsquo;aide pour transformer vos données, vous pouvez nous contacter pour toute question et même découvrir de nouvelles informations dont vous n\u0026rsquo;étiez pas conscient grâce à une meilleure compréhension de vos données.\nÀ+!\n","permalink":"https://www.dawrlog.com/fr/articles/9/","summary":"Salut tout le monde, j\u0026rsquo;espère que tout va bien.\nMême la base de données la plus puissante est toujours sujette à l\u0026rsquo;erreur terrifiante que tout système de base de données peut rencontrer : les goulots d\u0026rsquo;étranglement des requêtes.\nParfois, les systèmes qui gèrent nos requêtes ne peuvent pas traiter de la même manière que nous. Ainsi, nos questions renvoient des résultats précis grâce à une puissance de traitement supplémentaire.\nDans cet article, je vais présenter le fonctionnement des files d\u0026rsquo;attente de requêtes Redshift.","title":"Files d'attente de requêtes SQL sur Redshift : Le guide complet"},{"content":" De nos jours, disposer de bonnes données est un besoin que toute entreprise peut se permettre, et nous vous aidons à y parvenir. Chez Dataware Logistics (alias Dawrlog), nous sommes passionnés par ce que les bonnes informations peuvent nous apporter. Et nourris par cette passion, nous nous concentrons sur l\u0026rsquo;aide à nos clients avec un objectif clair : leur fournir le meilleur en matière de stockage et de consommation de données, de pipelines d\u0026rsquo;architecture de calcul, de migration d\u0026rsquo;applications et de la façon d\u0026rsquo;en tirer le meilleur parti.\nNous comptons sur des professionnels ayant plus d\u0026rsquo;une décennie de projets de données. Ils sont responsables de l\u0026rsquo;architecture de nouveaux flux de données à partir de zéro, de l\u0026rsquo;intégration de nouveaux flux dans les flux existants et de la migration des bases de données. Avec plus d\u0026rsquo;une décennie de valeur ajoutée pour nos clients, Dataware Logistics peut vous guider dans l\u0026rsquo;optimisation des coûts de votre pile existante en la migrant vers le cloud. Il peut s\u0026rsquo;agir des premières étapes de votre validation de concept ou de la migration de vos applications matures vers le cloud.\nNous nous attachons à fournir les bases de la pratique des données pour votre entreprise. Nous vous aidons à obtenir plus pour moins en vous fournissant une expertise technique sur l\u0026rsquo;ingestion, la consommation et la distribution des données à l\u0026rsquo;aide de technologies natives du cloud. Notre mission est d\u0026rsquo;aider votre entreprise à répondre à ses besoins en matière de données et d\u0026rsquo;infrastructure avec la meilleure pile technique dont vous avez besoin.\nNos professionnels ont plus d\u0026rsquo;une décennie d\u0026rsquo;expérience dans l\u0026rsquo;analyse de données, les migrations d\u0026rsquo;infrastructure et l\u0026rsquo;intégration de données, dans différents domaines d\u0026rsquo;activité. Nous nous concentrons sur ce que vous pouvez réaliser avec vos données en fournissant des architectures résilientes et fiables basées sur vos exigences commerciales. Nous fournissons les meilleures pratiques et un travail de qualité basé sur vos choix technologiques, au cas où vous les auriez déjà choisis. Vous faites votre travail, et nous vous aidons à donner un sens à tous ces aspects techniques.\nNous vous aidons également à acquérir une nouvelle expertise sur les tendances technologiques en offrant une formation ponctuelle et mesurée à vos équipes internes, créant ainsi un véritable champion de la technologie dans vos rangs. Chez Dawrlog, nous comprenons que chaque individu a des approches d\u0026rsquo;apprentissage différentes. Dans cette optique, nous pouvons fournir des formations complémentaires sur d\u0026rsquo;autres supports. Nous proposons des formations d\u0026rsquo;entreprise en direct, des capsules vidéo et des livres blancs techniques. Nous tenons à fournir toutes les informations nécessaires pour que votre équipe puisse faire le bon choix technologique en toute confiance.\nChez Dataware Logistics, nous soutenons votre entreprise dans la logistique de vos données et de l\u0026rsquo;infrastructure de vos plateformes analytiques. Cela va au-delà de la migration de vos systèmes vers le cloud et des conseils sur la façon dont ces nouvelles données doivent être ingérées en fonction de leur nature et de votre capacité d\u0026rsquo;adaptation pour les intégrer. Ou Nos consultants sont là pour vous proposer de nouveaux modes de traitement. Notre équipe vous aidera en vous indiquant comment les absorber en douceur sur votre infrastructure existante.\nNotre Expertise. Création de contenu Posts techniques Livres blancs Développement technique Formation en direct Capsules vidéo Services professionnels Optimisation des plateformes existantes Solutions d\u0026rsquo;entreprise pour le cloud Migration des données dans le nuage Présentation des données Ingestion de données Nos consultants ont des années d\u0026rsquo;expérience dans les domaines suivants.\nRetail Banques Assurance Telecom Les agences en marketing digitale\nNos clients nous font confiance sur la pile technologique suivante.\nModern Data Stack ","permalink":"https://www.dawrlog.com/fr/apropos/","summary":"De nos jours, disposer de bonnes données est un besoin que toute entreprise peut se permettre, et nous vous aidons à y parvenir. Chez Dataware Logistics (alias Dawrlog), nous sommes passionnés par ce que les bonnes informations peuvent nous apporter. Et nourris par cette passion, nous nous concentrons sur l\u0026rsquo;aide à nos clients avec un objectif clair : leur fournir le meilleur en matière de stockage et de consommation de données, de pipelines d\u0026rsquo;architecture de calcul, de migration d\u0026rsquo;applications et de la façon d\u0026rsquo;en tirer le meilleur parti.","title":"Allo, voici Dataware Logistics! Sinon Dawrlog simplement."},{"content":"Lorem ipsum dolor sit amet, et essent mediocritatem quo, choro volumus oporteat an mei. Ipsum dolor sit amet, et essent mediocritatem quo.\n","permalink":"https://www.dawrlog.com/fr/expertises/","summary":"Lorem ipsum dolor sit amet, et essent mediocritatem quo, choro volumus oporteat an mei. Ipsum dolor sit amet, et essent mediocritatem quo.","title":"Voici nos services"},{"content":"Salut à tous, j\u0026rsquo;espère que tout va bien.\nUn entrepôt de données vise à donner du sens à un sujet spécifique dans le temps en analysant les données historiques. Ce système, également appelé système d\u0026rsquo;aide à la décision, peut s\u0026rsquo;attaquer à des tendances aussi diverses que le pourcentage de désabonnement des clients ou les tendances de la consommation de bière dans une zone géographique.\nLe chargement incrémentiel est l\u0026rsquo;une de ces questions cruciales que vous devez prendre en compte lorsque vous définissez vos pipelines de chargement.\nDans ce billet, je vais expliquer ce qu\u0026rsquo;est le chargement incrémental et pourquoi il est si important. Je poursuivrai en définissant certains composants clés et terminerai par une brève conclusion.\nQu\u0026rsquo;est-ce que le chargement incrémental, et pourquoi en avons-nous besoin ? Également connu sous les noms de change data capture et delta load, le chargement incrémental est chargé de prendre un instantané des données les plus récentes. Il est essentiel de s\u0026rsquo;assurer que votre entrepôt de données est cohérent et fiable.\nCela dit, pour comprendre les avantages d\u0026rsquo;un chargement incrémentiel, vous devez savoir ce qu\u0026rsquo;est un entrepôt de données.\nJe vais donc commencer par vous expliquer les étapes du pipeline de données que vous devez suivre pour charger les bases de données de votre entrepôt de données. Cela devrait vous permettre de comprendre de façon claire et nette pourquoi c\u0026rsquo;est essentiel.\nEn termes simples, les entrepôts de données sont des bases de données qui soutiennent le processus décisionnel. Il est plus sûr de dire qu\u0026rsquo;ils vous aident à donner un sens à vos données historiques, car l\u0026rsquo;un des objectifs des entrepôts de données est de permettre à leurs utilisateurs d\u0026rsquo;analyser les données dans le temps.\nVous trouverez ci-dessous un bref rappel des phases les plus utilisées des entrepôts de données selon l\u0026rsquo;approche Kimball : Phases de l\u0026rsquo;entrepôt de données en utilisant Kimball\nOnline Transactional Processing (OLTP / Staging area / Landing Zone) : Les données sont extraites de leurs systèmes sources vers un stockage transitoire. Il s\u0026rsquo;agit de données à l\u0026rsquo;état brut, aussi proches que possible de leur source. Operational Data Stores (également connu sous le nom de ODS) : Une base de données avec tous les processus de qualité nécessaires et les corrélations entre les différentes sources. Les données sont ici dans un état préparé pour les utilisateurs. Data Warehouse (également connu sous le nom de DW / EDW) : Ici, les données sont stockées dans des tableaux appelés dimensions ou faits. La redondance des données n\u0026rsquo;est pas un problème car elles montrent leur évolution dans le temps. cubes Online Analytical Processing (également connu sous le nom de OLAP / Presentation Layer) : Ce sont les données finales auxquelles les utilisateurs accèdent pour leurs rapports, avec des métriques spécifiques si nécessaire. Vous ne pouvez pas (ou du moins vous ne devriez pas) avoir des informations détaillées telles que des adresses e-mail dans votre entrepôt de données. Comme la zone de transit n\u0026rsquo;est pas prête pour les transactions de base de données, vous devez interroger vos données au début de la phase transactionnelle pour ce type de demande. L\u0026rsquo;ODS fonctionne mieux pour les requêtes opérationnelles telles que \u0026ldquo;qui a fait quoi où ?\u0026rdquo;.\nVous êtes toujours avec moi ? Super, ça en vaut la peine !\nMaintenant que nous avons couvert nos bases, nous pouvons passer à ce que vous devez considérer lors de la conception de votre structure de chargement.\nComment votre entrepôt de données est chargé Chaque application contenant des données significatives pour votre entrepôt de données est appelée une source de données.\nChaque pompe de source de données a ses propres contraintes lors du chargement des données dans un entrepôt de données, subdivisées en deux groupes principaux : les chargements streaming et batch.\nNous utilisons l\u0026rsquo;approche streaming lorsque les données sont disponibles en continu. Imaginez les données de votre tracker de santé provenant de votre smartwatch. Nous utilisons les techniques de batch lorsque votre source de données fournit des données en masse, comme les données des points de vente d\u0026rsquo;une application de détaillant. La seconde est plus courante lorsque votre système source a un temps de chargement spécifique pour éviter la concurrence des processus internes.\nCe que l\u0026rsquo;on appelle micro-batch est un mélange des deux car il combine l\u0026rsquo;approche de chargement continu avec la périodicité que l\u0026rsquo;on trouve dans l\u0026rsquo;approche par lots. Les micro-lots sont pratiques lorsque vos données arrivent en flux constant, mais pas plus de dix minutes en petits lots.\nMaintenant que vous avez une compréhension de haut niveau de la façon dont votre entrepôt de données est rempli, nous pouvons passer à l\u0026rsquo;étape suivante : identifier les clés métier. Qu\u0026rsquo;est-ce que les clés métiers (\u0026ldquo;Business keys\u0026rdquo; de l\u0026rsquo;Anglais) ?\nChaque source de données possède ce que l\u0026rsquo;on appelle des clés métier. Ces clés nous aident à identifier ce qui donne à chaque enregistrement son caractère unique d\u0026rsquo;un point de vue commercial pour cette application spécifique.\nIl est important de disposer d\u0026rsquo;un référentiel central solide où vous pouvez trouver toutes les informations sur vos sources de données. Un catalogue de données peut être utile à cet égard. Il s\u0026rsquo;agit d\u0026rsquo;un référentiel central contenant toutes vos sources de données que vous pouvez explorer.\nIl vous sera utile si vous disposez de clés métier pour identifier les enregistrements uniques. En raison du haut niveau de détails disponibles, le tableau ci-dessous a une cardinalité plus grande ou haute granularité des données. Cardinality example:\nID Name Profession Address 1 Adam Smith Fireman 4389 Boul. Faraway App 6 2 John Smith Croupier 4389 Boul. Faraway App 1 3 Juliette Bull Saleswoman 64 Holest Nawy App 6 4 Lucille Croupier 1234 Runaway Train 5 Samantha Policewoman 4389 Galway Road Nous pouvons utiliser une combinaison de Nom et Profession dans cet exemple. Ce sont les clés commerciales ou les champs qui donnent une unicité à des fins commerciales.\nPar exemple, nous pouvons utiliser l\u0026rsquo;identifiant ou ID comme clé primaire unique, mais cela ne servira pas à grand-chose s\u0026rsquo;il fait référence à un enregistrement différent de ceux de l\u0026rsquo;exemple ci-dessus.\nUne fois que les clés métier sont claires, nous pouvons définir nos attributs et nos métriques : sur la base desquels nous pouvons comprendre les tendances des données dans nos sources de données, comme les métriques et attributs pertinents qui donnent des informations précises. Quel est le rapport entre les clés commerciales et les clés incrémentielles ?\nUne fois que vous savez comment identifier les clés métier de votre source de données, vous devez déterminer quel champ correspondra à la clé incrémentielle de votre chargement. En plus des clés métier, la clé incrémentielle est responsable du chargement de la nouvelle version de vos données.\nSur la base des clés métier et incrémentielles, vous serez correctement en mesure de charger uniquement la dernière version des données. Cette version mettra à jour sa version existante dans la phase de staging/ODS.\nCe faisant, votre entrepôt de données sera mis à jour avec une nouvelle version de votre enregistrement, laissant la version précédente avec un drapeau déprécié sur elle.\nDans ce scénario, je ne parle pas des dimensions qui changent lentement. Les clés métier et d\u0026rsquo;intégration sont celles qui nous donnent ce que nous appelons la unicité de l\u0026rsquo;enregistrement, ou ce qui le rend unique, créant ce que l\u0026rsquo;on appelle le versionnage des données.\nLe versionnage des données nous permet de faire la différence entre l\u0026rsquo;état actuel de l\u0026rsquo;enregistrement et ses états précédents. Il permet ce que l\u0026rsquo;on appelle l\u0026rsquo;\u0026ldquo;analyse temporelle\u0026rdquo; du document. Le versionnement des données sur les systèmes sources est censé exister sur la phase ODS de vos processus de pipelines de données.\nConclusion\nDans cet article, nous avons abordé le chargement incrémental et son importance pour l\u0026rsquo;intégrité des données.\nNous avons commencé par présenter d\u0026rsquo;autres noms pour ce type de chargement - le chargement delta et la capture des données de changement - et avons passé en revue ce qui constitue un entrepôt de données basé sur la méthodologie Kimball, puis les types d\u0026rsquo;approches pour les chargements de données en continu ou en masse.\nNous avons terminé en expliquant les clés métier et incrémentales et en les distinguant des clés système.\nIl est toujours bon de vous conseiller d\u0026rsquo;utiliser l\u0026rsquo;option de chargement incrémentiel lorsque vous interrogez vos données. En procédant ainsi, vous supprimez les vues dupliquées de votre instantané de données, ce qui réduit la charge lors de la gestion de vos pipelines d\u0026rsquo;ingestion de données.\nLe chargement incrémentiel est fortement recommandé (voire obligatoire) lors de la définition et du développement de vos pipelines de données, en particulier dans la phase ODS. Il peut vous aider à charger correctement les données de vos sources de données en chargeant correctement votre table et même en échelonnant vos pics d\u0026rsquo;ingestion de données. En divisant les données qui donnent ces goulots d\u0026rsquo;étranglement sur vos processus de chargement dans différents pipelines.\nÀ+!\n","permalink":"https://www.dawrlog.com/fr/articles/8/","summary":"Salut à tous, j\u0026rsquo;espère que tout va bien.\nUn entrepôt de données vise à donner du sens à un sujet spécifique dans le temps en analysant les données historiques. Ce système, également appelé système d\u0026rsquo;aide à la décision, peut s\u0026rsquo;attaquer à des tendances aussi diverses que le pourcentage de désabonnement des clients ou les tendances de la consommation de bière dans une zone géographique.\nLe chargement incrémentiel est l\u0026rsquo;une de ces questions cruciales que vous devez prendre en compte lorsque vous définissez vos pipelines de chargement.","title":"Chargements des données incrementels: Un moyen nickel de charger vos données."},{"content":"Bonjour à tous, aujourd\u0026rsquo;hui nous revisitons l\u0026rsquo;architecture REST, tout en la comparant aux appels gRPC.\nUn framework microservices ressemble parfois à un rêve, mais il peut devenir un cauchemar lors du traitement des données. De nos jours, il ne suffit pas de faire transiter les données avec moins de latence. Il faut aussi qu\u0026rsquo;elles se connectent facilement aux systèmes. La vitesse n\u0026rsquo;est pas le seul critère important, il faut aussi que l\u0026rsquo;application puisse s\u0026rsquo;adapter à vos besoins informatiques sans faire exploser vos coûts d\u0026rsquo;infrastructure. Et plus une application est utilisée, plus il est difficile de la maintenir en utilisant une approche modulaire. Aujourd\u0026rsquo;hui, je veux vous aider à choisir entre les API gRPC et REST comme pipelines de messages entre vos services. Je commencerai par présenter REST et les problèmes qu\u0026rsquo;elle peut résoudre. Ensuite, je m\u0026rsquo;intéresserai à gRPC et à certaines de ses fonctionnalités prêtes à l\u0026rsquo;emploi, avant d\u0026rsquo;évoquer les avantages de l\u0026rsquo;architecture gRPC. Je souhaite terminer cet article par un guide permettant de choisir l\u0026rsquo;une ou l\u0026rsquo;autre solution pour votre application. Et je vous montrerai lequel des deux est meilleur que l\u0026rsquo;autre sur chaque point. Je terminerai en résumant ce qu\u0026rsquo;il faut garder à l\u0026rsquo;esprit lors de l\u0026rsquo;architecture de vos microservices. En quoi consiste l\u0026rsquo;architecture REST ? Commençons par l\u0026rsquo;architecture REST bien connue. L\u0026rsquo;architecture REST suit un modèle sans état, ce qui signifie que chaque service porte en lui toutes ses métadonnées. Ses messages, également appelés charges utiles, sont stockés au format JSON.\nEn outre, il est plus facile d\u0026rsquo;intercepter les demandes non désirées de vos clients en utilisant REST. L\u0026rsquo;architecture utilise des méthodes http pour ses interactions avec les services. Les méthodes sont des composants qui suivent les mêmes CRUD operations trouvées sur les applications de base de données relationnelle pour conserver l\u0026rsquo;indépendance de vos transactions et la cohérence de vos données.\nLorsque vous utilisez ces méthodes, vos données restent cohérentes dans toutes les couches de votre application. Et grâce à ces caractéristiques, les appels REST peuvent gérer votre maintenance des ressources de manière plus fluide, même lorsque la connectivité réseau peut poser problème.\nLes applications RESTful sont également capables de stocker localement les données les plus utilisées côté client ; technique appelée mise en cache. Ce faisant, les services REST accèdent aux services les plus récurrents, ce qui permet de réduire la latence et la bande passante. Il devient plus rapide puisque les données demandées sont stockées localement pendant que la session est active. Dans ce cas, votre service web renvoie la version en cache de vos informations recueillies par un appel précédent. Il existe certaines procédures de contrôle de la sécurité pour atténuer certains problèmes. Ces procédures ne sont pas seulement liées à la politique de mise en cache persistante pour éviter l\u0026rsquo;utilisation de données non pertinentes et gérer l\u0026rsquo;infrastructure et la protection des données pour vos données en transit et vos données au repos. Jetez un coup d\u0026rsquo;œil à l\u0026rsquo;architecture RESTful de haut niveau ci-dessous.\nArchitecture de service web RESTFul.\nJ\u0026rsquo;espère que cela vous permettra de vous familiariser ou de vous rafraîchir la mémoire sur le fonctionnement de l\u0026rsquo;architecture RESTful. Ensuite, nous allons explorer les problèmes qu\u0026rsquo;elle peut résoudre. Défis que nous pouvons résoudre avec REST\nNous pouvons commencer par dire que l\u0026rsquo;API REST existe depuis un certain temps et qu\u0026rsquo;elle est plus mature que gRPC. Naturellement, elle bénéficie d\u0026rsquo;un meilleur support et d\u0026rsquo;une documentation de meilleure qualité puisqu\u0026rsquo;elle a subi de nombreux tests de résistance au fil des ans. En vous permettant d\u0026rsquo;intégrer plus facilement de nouvelles sources de données dans vos pipelines de données ; en lui permettant de rester plus proche de vos sources de données. Réduire les problèmes liés au réseau au sein de votre écosystème et au transit des données dans et hors de vos flux de données.\nEn utilisant le framework REST, vous devez développer chaque code d\u0026rsquo;éditeur et d\u0026rsquo;abonné, ce qui permet une logique de transformation de données supplémentaire nécessaire pour transmettre correctement vos données entre les services. Un autre avantage provient d\u0026rsquo;un cadre plus robuste, rendant l\u0026rsquo;intégration de votre équipe plus facile à gérer en utilisant des technologies éprouvées pour gérer votre charge de travail. Lorsque vos messages sont dans un format convivial comme JSON, l\u0026rsquo;audit de vos pipelines devient moins pénible lorsqu\u0026rsquo;une défaillance se produit.\nChaque point de terminaison intègre sa documentation dans le code, ce qui rend l\u0026rsquo;API REST plus facile à utiliser pour les humains. Ses actions suivent une structure simple, les activités étant décrites par des verbes. Il est plus logique, par exemple, que la méthode GET soit utilisée pour accéder à des enregistrements. De plus, les API REST sont remarquablement bien structurées, ce qui aide leurs responsables et leurs utilisateurs à améliorer les applications existantes. Maintenant que nous avons vu ce que REST peut faire pour nous, examinons gRPC. Qu\u0026rsquo;est-ce que gRPC ? The gRPC framework is an extension of remote procedure calls. In addition to using simple messages, this framework allows multiple message requests on the same connection. And it gets better with the support of bidirectional data streaming. This feature turns the gRPC into a powerful ally. You will be able to handle multiple independent, nonrelated threads.\nThe gRPC framework uses protocol buffers (or protobuf) under the hood, allowing evolutive schema support for your streaming data. You can avoid some of the trouble of having different JSON payloads from your sources as there is no need for client libraries to handle data cleansing jobs like auditing missing JSON fundamental values.\nIn addition to removing some of your business logic while extracting data from your API calls, protocol buffers are more optimal. Your data pipelines become more performant, and as a result, you can feel the network latency of your data ingestion channels lowered even with the smaller chunks of data. It becomes easier to correlate different data sources with adaptive client-server libraries generation based on your original protobuf file. gRPC connectivity overview.\nConsultez le Guide de langage des tampons de protocole pour une analyse plus approfondie de la configuration de votre message. Voyons maintenant ce que nous pouvons résoudre à l\u0026rsquo;aide de gRPC. Des défis que nous pouvons résoudre avec gRPC.\ngRPC a un avantage significatif : il peut créer un client pour deux clients sur différents langages de programmation dès le départ. En soi, cela rend l\u0026rsquo;adoption de vos sources plus accessible du point de vue du développement, et vous n\u0026rsquo;aurez pas besoin de tout un pipeline de développement pour les changements dans vos bibliothèques clientes. En modifiant simplement votre fichier proto, le langage de programmation ou la plate-forme de votre choix peut facilement reproduire ses modifications. De plus, votre fichier proto peut même définir l\u0026rsquo;acheminement des messages, ce qui évite de devoir le configurer sur chacun des codes producteurs ou consommateurs.\nUne autre caractéristique intéressante de gRPC est qu\u0026rsquo;il est nettement plus rapide que HTTPS et que la taille des messages est plus légère que celle de JSON (https://auth0.com/blog/beating-json-performance-with-protobuf/). Cela le rend essentiel lorsque différents langages de programmation consomment votre service. En utilisant gRPC, vous n\u0026rsquo;aurez pas à vous battre autant pour gérer votre communication. Bien que cela puisse être très tentant, un déploiement réussi dépendra de la maturité de votre écosystème pour le permettre. Conclusion\nJ\u0026rsquo;ai couvert les protocoles de communication utilisés sur deux architectures de microservices, les frameworks gRPC et REST. Il peut sembler intéressant d\u0026rsquo;adopter gRPC car il est plus rapide et plus vite adopté, mais vous risquez de vous créer beaucoup d\u0026rsquo;ennuis en ajustant vos pipelines de traitement. La raison principale vient de la façon dont gRPC traite les messages. Vous pouvez y remédier avec le protobuf mappage JSON prêt à l\u0026rsquo;emploi, mais vous devrez tout de même modifier vos services REST existants pour qu\u0026rsquo;ils interagissent avec les nouveaux services gRPC. Gardez à l\u0026rsquo;esprit que tous les navigateurs ne prennent pas en charge toutes les fonctionnalités de gRPC, et qu\u0026rsquo;il ne s\u0026rsquo;agit donc pas d\u0026rsquo;un excellent cadre de travail pour l\u0026rsquo;utilisateur.\nVous aurez besoin d\u0026rsquo;un plan technique bien conçu pour faire passer votre architecture de REST à gRPC. Mais vous obtiendrez un meilleur plan d\u0026rsquo;observabilité. Le fait d\u0026rsquo;avoir vos données dans un format non humain peut renforcer votre sécurité car vos données ne seront pas en texte clair.\nIl est également bon de renforcer que Aucune approche ne fonctionnera bien sans une analyse de sécurité robuste, c\u0026rsquo;est une bonne idée de revoir vos règles en fonction des métriques de votre trafic de données.\nSi vous avez besoin d\u0026rsquo;aide pour transformer vos données, vous pouvez nous contacter pour toute question et même découvrir de nouvelles informations dont vous n\u0026rsquo;étiez pas conscient grâce à une meilleure compréhension de vos données.\nÀ+!\n","permalink":"https://www.dawrlog.com/fr/articles/13/","summary":"Bonjour à tous, aujourd\u0026rsquo;hui nous revisitons l\u0026rsquo;architecture REST, tout en la comparant aux appels gRPC.\nUn framework microservices ressemble parfois à un rêve, mais il peut devenir un cauchemar lors du traitement des données. De nos jours, il ne suffit pas de faire transiter les données avec moins de latence. Il faut aussi qu\u0026rsquo;elles se connectent facilement aux systèmes. La vitesse n\u0026rsquo;est pas le seul critère important, il faut aussi que l\u0026rsquo;application puisse s\u0026rsquo;adapter à vos besoins informatiques sans faire exploser vos coûts d\u0026rsquo;infrastructure.","title":"gRPC vs REST: Lequel dois-je choisir pour mon application et pourquoi?"},{"content":"Bonjour à tous,\nCe post explore l\u0026rsquo;utilisation des Argo Workflows pour orchestrer vos pipelines de données. Pour commencer, rappelons ce qu\u0026rsquo;est l\u0026rsquo;ETL tout en concevant l\u0026rsquo;architecture de haut niveau de notre travail. Je montrerai comment configurer vos pipelines de données pour qu\u0026rsquo;ils suivent plus naturellement la structure. Ensuite, nous verrons comment obtenir le même résultat en utilisant des graphes acycliques dirigés (DAGs). Enfin, je résumerai ce que nous avons vu et présenterai des raisons pour guider votre choix d\u0026rsquo;approche en fonction de la complexité de votre projet. Alors, commençons.\nQue signifie ETL ? Tout d\u0026rsquo;abord, rappelons ce qu\u0026rsquo;est l\u0026rsquo;ETL avant de commencer nos exemples. L\u0026rsquo;extraction, la transformation et le chargement consistent en des tâches visant à nettoyer vos données et à regrouper les données de vos applications dans une base de données conforme. Imaginez cette base de données conforme comme la source unique de vérité de vos données. Ce référentiel centralisé vous aide à mieux connaître vos produits et vos clients.\nCependant, chaque application a sa propre structure pour traiter les données. Chaque tâche ETL rend les données de l\u0026rsquo;application plus attrayantes pour l\u0026rsquo;analyse, en ayant des dépendances explicites à mesure que votre processus de data wrangling devient plus robuste. Notre code va déployer quatre tâches ETL et leur relation, comme le reflète l\u0026rsquo;image ci-dessous :\nArchitecture de haut niveau des tâches ETL\nComprendre chaque tâche ETL Ce flux de travail prend en charge deux formats de données différents : colonne ou ligne. Ces différents formats nécessitent des analyseurs différents : parquet pour les colonnes et avro pour les lignes. Nous pouvons également nous y référer en tant que batch ou stream, respectivement.\nLe flux de travail commence donc par un gestionnaire de requête qui identifie le type de données. En fonction de la valeur d\u0026rsquo;un indicateur de type de données, il transmet la tâche aux balises d\u0026rsquo;analyse batch ou stream.\nLes analyseurs transmettent leurs résultats à la tâche Load Data, où ils sont chargés dans le stockage persistant, en fonction de la source et du type de données.\nBien qu\u0026rsquo;il semble s\u0026rsquo;agir d\u0026rsquo;un seul flux de travail avec deux chemins de code différents, il y a des avantages à traiter les deux types de données dans le même ensemble de tâches. Le partage d\u0026rsquo;une tâche de chargement entre les flux de données facilite la gestion des chevauchements et des relations. Par exemple, le magasin de données en colonnes peut avoir besoin d\u0026rsquo;être mis à jour avec des clés étrangères provenant de nouvelles données en lignes. Il est également plus facile de partager un seul flux ETL entre différentes équipes.\nMaintenant que nous savons ce que nous allons construire, commençons par voir comment le mettre en œuvre en utilisant les étapes des workflows Argo.\nConstruire un pipeline ETL en utilisant les étapes des workflows Argo Dans cette approche, le pipeline de données va suivre une liste d\u0026rsquo;étapes pour nettoyer et traiter les données de vos sources de données. Votre code ETL devient encore plus robuste lorsque nous utilisons des conditionnels pour informer quel flux ETL vos données doivent prendre. Cela semble bien, non ? Alors, mettons nos mains dans le cambouis et voyons comment cela fonctionne en pratique.\nLe code ci-dessous va créer un workflow dans un espace de nom appelé argo. Cet espace de nom doit exister avant que le workflow ne soit exécuté avec argo submit. Cela permet d\u0026rsquo;éviter les problèmes de sécurité, comme le fait que votre utilisateur n\u0026rsquo;ait pas le droit de créer des espaces de noms. Cela évitera également les messages d\u0026rsquo;erreur vous avertissant de ne pas casser votre déploiement Kubernetes. Pour notre exemple, nous allons générer une valeur aléatoire sur une machine Linux et charger les données à venir en fonction de cette valeur.\nBien que les deux étapes d\u0026rsquo;analyse syntaxique soient déclenchées simultanément, seule celle informée par l\u0026rsquo;étape de traitement des demandes s\u0026rsquo;exécutera. L\u0026rsquo;utilisation d\u0026rsquo;un code automatisé comme celui-ci réduira les chances d\u0026rsquo;avoir des problèmes avec notre flux de données ETL. L\u0026rsquo;automatisation de vos étapes de flux de travail gère les erreurs courantes telles que les types de mismatch dans votre base de données.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: stream-batch-parser- namespace: argo spec: entrypoint: sbp templates: - name: sbp steps: - - name: request-handler template: edge-input - - name: parser-stream template: stream when: \u0026#34;{{steps.handle-requests.outputs.result}} \u0026lt;= 163883\u0026#34; - name: parser-batch template: parquet when: \u0026#34;{{steps.handle-requests.outputs.result}} \u0026gt; 163883\u0026#34; - name: stream steps: - - name: avro-parser template: avro - - name: wrapper template: wrapper - name: batch steps: - - name: parquet-parser template: parquet - - name: wrapper template: wrapper - name: edge-input container: image: alpine:3.6 command: [sh, -c] args: [\u0026#34;echo ${RANDOM}\u0026#34;] - name: parquet container: image: alpine:3.6 command: [sh, -c] args: [\u0026#34;echo \\\u0026#34;code to parse Parquet\\\u0026#34;\u0026#34;] - name: avro container: image: alpine:3.6 command: [sh, -c] args: [\u0026#34;echo \\\u0026#34;code to parse Avro\\\u0026#34;\u0026#34;] - name: wrapper container: image: alpine:3.6 command: [sh, -c] args: [\u0026#34;echo \\\u0026#34;code to load into Staging\\\u0026#34;\u0026#34;] Sauvegardez le fichier ci-dessus en tant que `etl_steps.yml` et démarrez votre flux de travail avec cette commande : argo submit -n argo etl_steps.yml Nous pouvons maintenant obtenir le statut de notre workflow en exécutant la commande argo get suivante :\nargo get -n argo stream-batch-parser-xxvks Les cinq derniers chiffres seront différents dans chaque environnement. Et en exécutant la commande précédente, votre journal de sortie devrait être similaire à l\u0026rsquo;image ci-dessous ; comme indiqué, notre workflow exécutera la tâche de flux d\u0026rsquo;analyseur en fonction de la valeur renvoyée par la tâche de traitement des demandes.\nSortie des étapes ETL du workflow Argo Etapes ETL retournées par l\u0026rsquo;exécution de la commande argo get. Maintenant que nous avons vu comment construire un ETL avec des tâches, nous allons explorer comment utiliser les DAGs pour votre ETL.\nConstruire un pipeline ETL avec des DAGs au lieu d\u0026rsquo;étapes Maintenant, explorons comment réaliser le même travail en utilisant des modèles de DAGs au lieu d\u0026rsquo;étapes dans Argo Workflows. Même si le DSL semble similaire à première vue, les DAGs vous donnent plus de pouvoir pour spécifier les dépendances entre les étapes et exécuter les tâches en parallèle.\nDans un DAG, toute tâche peut être exécutée lorsque ses dépendances sont satisfaites. Si les dépendances de plus d\u0026rsquo;une tâche sont satisfaites, toutes les tâches seront exécutées en parallèle. Si une tâche n\u0026rsquo;a pas de dépendances, elle s\u0026rsquo;exécutera dès que le workflow sera lancé. Les DAGs sont excellents pour traiter l\u0026rsquo;ETL, et je vous suggère fortement de vous familiariser avec toutes les options qu\u0026rsquo;une tâche DAG peut fournir en consultant la documentation officielle d\u0026rsquo;Argo.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: dag-orchestrate- namespace: argo spec: entrypoint: sbp archiveLogs: true templates: - name: sbp dag: tasks: - name: request-handler template: edge-input - name: stream-flow template: stream when: \u0026#34;{{tasks.handle-requests.outputs.result}} \u0026lt;= 163883\u0026#34; depends: handle-requests - name: batch-flow template: batch when: \u0026#34;{{tasks.handle-requests.outputs.result}} \u0026gt; 163883\u0026#34; depends: handle-requests - name: stream steps: - - name: avro-parser template: avro - - name: wrapper template: wrapper - name: batch steps: - - name: parquet-parser template: parquet - - name: wrapper template: wrapper - name: edge-input container: image: alpine:3.6 command: [sh, -c] args: [\u0026#34;echo ${RANDOM}\u0026#34;] - name: parquet container: image: alpine:3.6 command: [sh, -c] args: [\u0026#34;echo \\\u0026#34;code to parse Parquet\\\u0026#34;\u0026#34;] - name: avro container: image: alpine:3.6 command: [sh, -c] args: [\u0026#34;echo \\\u0026#34;code to parse Avro\\\u0026#34;\u0026#34;] - name: wrapper container: image: alpine:3.6 command: [sh, -c] args: [\u0026#34;echo \\\u0026#34;code to load into Staging\\\u0026#34;\u0026#34;] Enregistrez le fichier ci-dessus en tant que etl_dag.yml et soumettez votre flux de travail pour le lancer :\nargo submit -n argo etl_dag.yml Comme démontré ci-dessous, vous pouvez vérifier son évolution avec argo get :\nargo get -n argo dag-orchestrate-ctpkl Dans ce scénario, notre workflow a exécuté la tâche de flux batch au lieu du flux stream en fonction de la valeur renvoyée par la tâche handle requests. Félicitations pour votre travail ! Vous pouvez maintenant concevoir vos flux de données ETL en utilisant un DAG ou une liste structurée d\u0026rsquo;étapes dans Argo Workflows.\nN\u0026rsquo;oubliez pas de nettoyer votre environnement avec argo delete -n argo your-workflow, où vous devez informer le workflow désiré comme votre-workflow.\nargo delete -n argo your-workflow Conclusion Bien qu\u0026rsquo;il soit généralement utilisé pour la gestion de l\u0026rsquo;infrastructure, Argo Workflows peut également orchestrer vos tâches ETL. En l\u0026rsquo;utilisant ainsi, vous n\u0026rsquo;avez plus besoin de recourir à différents outils pour atteindre le même objectif, c\u0026rsquo;est-à-dire Argo pour le CI/CD et Airflow pour les tâches ETL.\nL\u0026rsquo;approche DAG est souvent meilleure que l\u0026rsquo;approche par étapes pour l\u0026rsquo;exécution des pipelines ETL. Pour commencer, le traitement des tâches DAG est optimisé au moment de l\u0026rsquo;exécution. Vous aurez moins de points de décision pour certains de vos pipelines simplement en informant le flux de données souhaité.\nPour les tâches simples, les flux séquentiels (comme vous obtenez avec l\u0026rsquo;approche par étapes dans les workflows Argo) fonctionnent bien. Cependant, ils deviennent plus difficiles à maintenir dans les cas où vous devez cibler un sous-ensemble de votre flux de données et gérer des dépendances complexes dans le temps.\nUn autre avantage de l\u0026rsquo;utilisation des DAGs est de pouvoir spécifier l\u0026rsquo;étape exacte au moment de l\u0026rsquo;exécution. L\u0026rsquo;exécution vous donne plus de liberté pour créer un code conditionnel avec moins de boucles indentées tout en optimisant le code et les ressources de l\u0026rsquo;infrastructure.\nJe vous invite à approfondir la documentation d\u0026rsquo;Argo Workflows sur les DAGs. Maîtriser le fonctionnement des DAGs peut augmenter la qualité de vos pipelines ETL, en vous permettant de gérer vos tâches ETL de manière plus dynamique par rapport à la méthode des étapes.\nPour des moyens plus optimisés de gérer vos ressources Kubernetes, explorez comment Pipekit peut vous aider à orchestrer l\u0026rsquo;ensemble de votre déploiement Argo Workflows.\nÀ+!\n","permalink":"https://www.dawrlog.com/fr/articles/5/","summary":"Bonjour à tous,\nCe post explore l\u0026rsquo;utilisation des Argo Workflows pour orchestrer vos pipelines de données. Pour commencer, rappelons ce qu\u0026rsquo;est l\u0026rsquo;ETL tout en concevant l\u0026rsquo;architecture de haut niveau de notre travail. Je montrerai comment configurer vos pipelines de données pour qu\u0026rsquo;ils suivent plus naturellement la structure. Ensuite, nous verrons comment obtenir le même résultat en utilisant des graphes acycliques dirigés (DAGs). Enfin, je résumerai ce que nous avons vu et présenterai des raisons pour guider votre choix d\u0026rsquo;approche en fonction de la complexité de votre projet.","title":"Examples des ETL utilisant des workflows Argo."},{"content":"Bonjour à tous,\nNous savons tous comment Argo Workflows permet d\u0026rsquo;orchestrer facilement des travaux parallèles sur Kubernetes. Bien qu\u0026rsquo;il soit le plus souvent associé aux projets de traitement de données et d\u0026rsquo;ETL, il est utile pour bien d\u0026rsquo;autres choses ! Ces 10 workflows vont changer la façon dont vous voyez cet orchestrateur Kubernetes.\nAllons-y !\nConfiguration des Workflows Argo\nSi vous n\u0026rsquo;avez pas actuellement de workflow en cours d\u0026rsquo;exécution, je vous suggère de créer votre premier Argo Workflow pour comprendre ce dont nous allons parler dans ce post. Pour ce faire, suivez les instructions ici pour créer un déploiement local de Workflows Argo sur votre cluster. Je vous suggère également d\u0026rsquo;utiliser k3d pour votre plan de contrôle Kubernetes local ; ce tutoriel utilise un cluster k3d nommé argo. N\u0026rsquo;hésitez pas à reproduire la commande ci-dessous pour le créer dans votre environnement :\nk3d cluster create argo kubectl create -n argo -f https://raw.githubusercontent.com/argoproj/argo-workflows/master/manifests/quick-start-postgres.yaml kubectl -n argo port-forward deployment/argo-server 2746:2746 \u0026amp; Maintenant, regardons notre premier exemple !\n1. Améliorer votre flux de travail à l\u0026rsquo;aide de paramètres\nArgo utilise des custom resource definitions stockées dans des fichiers YAML pour gérer ses déploiements. Il n\u0026rsquo;est donc pas nécessaire d\u0026rsquo;apprendre de nouvelles spécifications pour gérer votre infrastructure ; vous pouvez suivre le même modèle que celui utilisé sur vos scripts Kubernetes et Kustomize, ce qui vous aide à rester cohérent. Ci-dessous, nous pouvons voir comment utiliser les paramètres sur vos workflows, et passer des paramètres est pratique lorsque votre configuration utilise des valeurs d\u0026rsquo;exécution. Par conséquent, vous ne connaîtrez certains composants qu\u0026rsquo;après les avoir créés, comme les jetons d\u0026rsquo;accès.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: hello-world-parameters- spec: entrypoint: whalesay arguments: parameters: - name: message value: Message string default value templates: - name: whalesay inputs: parameters: - name: message container: image: docker/whalesay command: [cowsay] args: [\u0026#34;{{inputs.parameters.message}}\u0026#34;] Dans notre modèle, le paramètre message aura la valeur par défaut de Message string default value. Cependant, cette valeur peut être écrasée au moment de l\u0026rsquo;exécution, comme nous pouvons le voir en exécutant la commande ci-dessous :\nargo submit -n argo param.yml -p message=\u0026#34;This ran from your container\u0026#34; --watch Nous pouvons valider la sortie à partir de l\u0026rsquo;interface utilisateur des logs des workflows Argo. (Vous pouvez accéder à l\u0026rsquo;IU par défaut à https://localhost:2746/ si vous suivez rapidement les instructions de transfert de port lors de la création de votre cluster).\n2. Extraire des images de votre référentiel sécurisé\nL\u0026rsquo;une des fonctionnalités que j\u0026rsquo;apprécie lors de l\u0026rsquo;automatisation d\u0026rsquo;un écosystème est l\u0026rsquo;utilisation de clés d\u0026rsquo;accès rotatives tout en gérant l\u0026rsquo;accès de mes services. Ceci est utile dans les cas où votre entreprise utilise des dépôts de conteneurs privés pour héberger vos images de conteneurs. Les workflows Argo vous aident à réaliser cela grâce au support natif des secrets Kubernetes. Dans notre exemple, nous pouvons voir que le secret docker-registry-secret va tirer l\u0026rsquo;image docker/whalesay:latest.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: hello-world- spec: entrypoint: whalesay imagePullSecrets: - name: docker-registry-secret templates: - name: whalesay container: image: docker/whalesay:latest command: [cowsay] args: [\u0026#34;hello world\u0026#34;] 3. Utilisation des conteneurs Sidecar L\u0026rsquo;une de mes choses préférées à faire est d\u0026rsquo;utiliser des sidecars lors du démarrage de mes pods. Les sidecars Kubernetes sont des aides utiles qui peuvent gérer des tâches récurrentes, comme la synchronisation de vos dépôts Git avec git-sync comme indiqué ici. Les flux de travail d\u0026rsquo;Argo couvrent ce problème avec un support soigné des conteneurs sidecars.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: sidecar-nginx- spec: entrypoint: sidecar-nginx-example templates: - name: sidecar-nginx-example container: image: appropriate/curl command: [sh, -c] args: [\u0026#34;until `curl -G \u0026#39;http://127.0.0.1/\u0026#39; \u0026gt;\u0026amp; /tmp/out`; do echo sleep \u0026amp;\u0026amp; sleep 1; done \u0026amp;\u0026amp; cat /tmp/out\u0026#34;] sidecars: - name: nginx image: nginx:1.13 Pour le déployer, enregistrez le code ci-dessus sous le nom de sidecar-nginx.yml et soumettez-le :\nargo submit -n argo sidecar-nginx.yml --watch Et comme résultat, vous allez déployer une instance sidecar de proxy inverse de NGINX.\nPro-tip : Vous devrez peut-être prêter une attention particulière à vos workflows si vous utilisez Istio comme Service mesh. Consultez ce fil de discussion GitHub si vous envisagez de l\u0026rsquo;utiliser.\n4. Archiver l\u0026rsquo;état actuel de votre workflow sur un stockage persistant\nL\u0026rsquo;archivage des workflows est une fonctionnalité intéressante fournie par Argo Workflows afin que vous puissiez avoir des états de workflow précédents stockés sur une base de données relationnelle (Postgres ou MySQL pour l\u0026rsquo;instant). Cependant, l\u0026rsquo;archive d\u0026rsquo;Argo ne conservera pas les journaux d\u0026rsquo;exécution détaillés ; vous devrez configurer un stockage d\u0026rsquo;objets comme artefact repository, il peut s\u0026rsquo;agir d\u0026rsquo;une option opensource comme MinIO, ou AWS S3 ; pour mentionner une option de fournisseur de cloud d\u0026rsquo;AWS.\nPour utiliser la fonctionnalité d\u0026rsquo;archivage, vous devez d\u0026rsquo;abord configurer l\u0026rsquo;option de stockage persistant de votre serveur Argo. Pour ce faire, vous aurez besoin de plus d\u0026rsquo;informations sur la façon de la configurer. En suivant ce lien, vous pourrez vous familiariser avec les éléments d\u0026rsquo;authentification requis pour l\u0026rsquo;archive Argo ; puis, basez votre configuration sur Controller configmap. Vous devrez les avoir configurés de manière appropriée avec votre serveur Argo pour bénéficier de cette fonctionnalité. Une fois qu\u0026rsquo;il est configuré, vous pouvez stocker vos flux de travail avec le spec.archiveLocation.archiveLogs comme démontré ci-dessous.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: archive-location- spec: entrypoint: whalesay templates: - name: whalesay container: image: docker/whalesay:latest command: [cowsay] args: [\u0026#34;hello world\u0026#34;] archiveLocation: archiveLogs: true 5. Passer un dépôt Git comme artefact d\u0026rsquo;entrée\nUne autre fonctionnalité intéressante fournie par Argo Workflows est la possibilité de synchroniser votre dépôt Git sans avoir besoin de sidecars ou de conteneurs init supplémentaires. Le code ci-dessous se connecte au dépôt Argo sur Github. Vous pouvez choisir entre des demandes de pull HTTP ou SSH pour l\u0026rsquo;élément d\u0026rsquo;authentification. Dans le premier modèle, git-clone, vous devrez utiliser la combinaison des secrets Kubernetes usernameSecret et passwordSecret pour accéder à une URL dans son format HTTP. Vous pouvez voir un exemple de configuration HTTP de Git dans le code ci-dessous.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: input-artifact-git- spec: entrypoint: git-clone templates: - name: git-clone inputs: artifacts: - name: argo-source path: /src git: repo: https://github.com/argoproj/argo-workflows.git revision: \u0026#34;v2.1.1\u0026#34; usernameSecret: name: github-creds key: username passwordSecret: name: github-creds key: password container: image: golang:1.10 command: [sh, -c] args: [\u0026#34;git status \u0026amp;\u0026amp; ls \u0026amp;\u0026amp; cat VERSION\u0026#34;] workingDir: /src Argo Workflows supporte également la connectivité SSH (e.g., git@github.com:argoproj/argo-workflows.git). Cependant, il a besoin du format d\u0026rsquo;URL suivant la connectivité SSH et du secret Kubernetes sshPrivateKeySecret au lieu des secrets usernameSecret et passwordSecret. 6. création de flux de travail de graphes acycliques dirigés\nJ\u0026rsquo;ai l\u0026rsquo;impression que le graphe acyclique dirigé (DAG) reçoit maintenant l\u0026rsquo;attention qu\u0026rsquo;il mérite dans les domaines de l\u0026rsquo;analyse en raison de la façon impressionnante dont il gère les étapes de la charge de travail de traitement des données sur Apache Spark et de son utilisation comme modèle commun d\u0026rsquo;orchestration des données avec Apache Airflow. Avec Argo Workflows, vous disposerez d\u0026rsquo;une interface conviviale Kubernetes au lieu de devoir configurer un exécuteur Kubernetes pour Airflow qui est moins stable.\nJe vous suggère de consulter ce lien pour en savoir plus sur le fonctionnement d\u0026rsquo;un DAG. Ci-dessous, vous pouvez voir comment Argo Workflows l\u0026rsquo;instancie.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: dag-target- spec: entrypoint: dag-target arguments: parameters: - name: target value: E templates: - name: dag-target dag: target: \u0026#34;{{workflow.parameters.target}}\u0026#34; tasks: - name: A template: echo arguments: parameters: [{name: message, value: A}] - name: B depends: \u0026#34;A\u0026#34; template: echo arguments: parameters: [{name: message, value: B}] - name: C depends: \u0026#34;A\u0026#34; template: echo arguments: parameters: [{name: message, value: C}] - name: D depends: \u0026#34;B \u0026amp;\u0026amp; C\u0026#34; template: echo arguments: parameters: [{name: message, value: D}] - name: E depends: \u0026#34;C\u0026#34; template: echo arguments: parameters: [{name: message, value: E}] - name: echo inputs: parameters: - name: message container: image: alpine:3.7 command: [echo, \u0026#34;{{inputs.parameters.message}}\u0026#34;] Chaque tâche sera passée au serveur Argo en utilisant le nom du paramètre target, avec les noms des cibles séparés par des espaces. Les workflows Argo n\u0026rsquo;exécuteront que celles que vous avez spécifiées ; cependant, ils exécuteront chaque dépendance jusqu\u0026rsquo;à ce qu\u0026rsquo;elle atteigne les cibles renseignées. En clair, disons que nous enregistrons notre fichier sous le nom de dag-targets.yml et l\u0026rsquo;exécutons en utilisant la commande suivante :\nargo submit -n argo dag-targets.yml -p target=\u0026#34;B E\u0026#34; --watch Il ignorera uniquement la cible D, comme le montre la démonstration ci-dessous :\n7. exécuter des scripts Python\nLes conteneurs facilitent déjà la gestion des environnements d\u0026rsquo;exécution. Il est donc facile de construire un conteneur Python avec les bibliothèques et la version dont vous avez besoin pour vos étapes de workflow basées sur Python.\nAvec les workflows Argo, vous pouvez appeler un script Python déjà installé sur le conteneur par son nom, ou passer du code via un champ source dans la description du workflow. Vous pouvez spécifier n\u0026rsquo;importe quel code valide dans le bloc source.\nVoici un exemple :\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: name: python-random-number-generator namespace: argo spec: entrypoint: generator arguments: parameters: - name: min value: 0 - name: max value: 10 templates: - name: generator inputs: parameters: - name: min - name: max script: image: python:3.8 command: [python] source: | from random import randrange range_min = {{inputs.parameters.min}} range_max = {{inputs.parameters.max}} random_number = randrange(range_min, range_max) print(\u0026#34;Random number: {}\u0026#34;.format(random_number)) 8. implémentation d\u0026rsquo;une stratégie de relance\nParfois, plusieurs cibles peuvent implémenter une certaine logique de relance, et Argo Workflows configure votre stratégie de relance au niveau du Workflow.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: retry-container- spec: entrypoint: retry-container templates: - name: retry-container retryStrategy: limit: \u0026#34;3\u0026#34; retryPolicy: \u0026#34;OnError\u0026#34; container: image: python:alpine3.6 command: [\u0026#34;python\u0026#34;, -c] # fail with a 66% probability args: [\u0026#34;import random; import sys; exit_code = random.choice([0, 1, 1]); sys.exit(exit_code)\u0026#34;] Dans notre exemple, la cible retry-container essaiera de redémarrer trois fois dans les cas où elle se termine avec un statut Error sur Kubernetes.\n9. Ajout de flux de travail conditionnels\nLes flux de travail conditionnels font également partie de mes préférés et sont si simples à mettre en œuvre. Vous pouvez déployer votre architecture en fonction des états de retour des étapes précédentes, ce qui est très pratique lorsque vous orchestrez un ensemble de conteneurs. Argo Workflows vous offre la possibilité d\u0026rsquo;exécuter des cibles basées sur une condition booléenne. Sous le capot, il utilise govaluate pour vous permettre d\u0026rsquo;utiliser les instructions expr de Golang.\nVous serez donc en mesure d\u0026rsquo;orchestrer vos conditions de la même manière que vous gérez vos aides Golang dans votre écosystème Kubernetes - un autre avantage supplémentaire de l\u0026rsquo;utilisation des CRD Kubernetes.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: conditional-parameter- labels: workflows.argoproj.io/test: \u0026#34;true\u0026#34; annotations: workflows.argoproj.io/version: \u0026#39;\u0026gt;= 3.1.0\u0026#39; spec: entrypoint: main templates: - name: main steps: - - name: flip-coin template: flip-coin - - name: heads template: heads when: \u0026#34;{{steps.flip-coin.outputs.result}} == heads\u0026#34; - name: tails template: tails when: \u0026#34;{{steps.flip-coin.outputs.result}} == tails\u0026#34; outputs: parameters: - name: stepresult valueFrom: expression: \u0026#34;steps[\u0026#39;flip-coin\u0026#39;].outputs.result == \u0026#39;heads\u0026#39; ? steps.heads.outputs.result : steps.tails.outputs.result\u0026#34; - name: flip-coin script: image: python:alpine3.6 command: [ python ] source: | import random print(\u0026#34;heads\u0026#34; if random.randint(0,1) == 0 else \u0026#34;tails\u0026#34;) - name: heads script: image: python:alpine3.6 command: [ python ] source: | print(\u0026#34;heads\u0026#34;) - name: tails script: image: python:alpine3.6 command: [ python ] source: | print(\u0026#34;tails\u0026#34;) En enregistrant le code ci-dessus sous le nom de cond.yml et en l\u0026rsquo;exécutant avec argo submit, vous obtiendrez le résultat suivant :\nargo submit -n argo cond.yml --watch 10. Gestion des ressources Kubernetes à partir de votre flux de travail Les workflows Argo peuvent créer des composants Kubernetes ; ceci est très pratique lorsque vous avez besoin de développer des actions kubelet temporaires de manière déclarative. Cette fonctionnalité suit le même principe que les scripts en ligne pour déployer les composants Kubernetes responsables de l\u0026rsquo;application des correctifs à votre environnement. Cependant, Argo Workflows gère les fichiers YAML en ligne de ce code Kubernetes CRD.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: k8s-jobs- spec: entrypoint: pi-tmpl templates: - name: pi-tmpl resource: action: create successCondition: status.succeeded \u0026gt; 0 failureCondition: status.failed \u0026gt; 3 manifest: | apiVersion: batch/v1 kind: Job metadata: generateName: pi-job- spec: template: metadata: name: pi spec: containers: - name: pi image: perl command: [\u0026#34;perl\u0026#34;, \u0026#34;-Mbignum=bpi\u0026#34;, \u0026#34;-wle\u0026#34;, \u0026#34;print bpi(2000)\u0026#34;] restartPolicy: Never backoffLimit: 4 Cette fonctionnalité vous permet d\u0026rsquo;exécuter directement toutes les actions kubectl, ce qui vous permet de créer/mettre à jour/supprimer n\u0026rsquo;importe quelle ressource Kubernetes sur votre cluster en utilisant les définitions en ligne des groupes API Kubernetes.\nConclusion\nLes progrès réalisés dans la gestion et le développement des systèmes nous donnent de nombreuses raisons d\u0026rsquo;être optimistes. Par exemple, infrastructure as code vous permet d\u0026rsquo;avoir la même infrastructure sur vos serveurs évolutifs et sur votre poste de travail local. Des outils tels que Argo Workflows nous aident à créer une infrastructure évolutive prête à être mise en production sur notre poste de travail local, ce qui est en soi une bonne chose.\nAvec les changements constants des exigences de l\u0026rsquo;infrastructure comme le DNS dynamique, vous devez adapter vos déploiements à une approche plus modulaire. Ces flux de travail sont les incontournables de tout administrateur DevOps. Mais cette liste n\u0026rsquo;est qu\u0026rsquo;un début. Je vous conseille vivement de mettre en œuvre ces scripts dans vos pipelines de développement et de données.\nÀ+!\n","permalink":"https://www.dawrlog.com/fr/articles/6/","summary":"Bonjour à tous,\nNous savons tous comment Argo Workflows permet d\u0026rsquo;orchestrer facilement des travaux parallèles sur Kubernetes. Bien qu\u0026rsquo;il soit le plus souvent associé aux projets de traitement de données et d\u0026rsquo;ETL, il est utile pour bien d\u0026rsquo;autres choses ! Ces 10 workflows vont changer la façon dont vous voyez cet orchestrateur Kubernetes.\nAllons-y !\nConfiguration des Workflows Argo\nSi vous n\u0026rsquo;avez pas actuellement de workflow en cours d\u0026rsquo;exécution, je vous suggère de créer votre premier Argo Workflow pour comprendre ce dont nous allons parler dans ce post.","title":"Les 10 optimales workflows Argo; avec examples."},{"content":"Bonjour à tous,\nAujourd\u0026rsquo;hui, je voudrais vous expliquer comment déboguer vos flux de travail Argo, ce qui est pratique lorsque vous avez besoin de plus de détails sur votre environnement. Voyons les deux approches différentes pour déboguer vos déploiements de workflows Argo.\nTout d\u0026rsquo;abord, nous utiliserons l\u0026rsquo;Interface de ligne de commande Argo, ou CLI pour faire court. Les commandes CLI d\u0026rsquo;Argo couvrent la plupart des erreurs les plus courantes rencontrées avec les workflows, telles que les mauvaises configurations. Ensuite, nous verrons comment déboguer les workflows en utilisant l\u0026rsquo;interface utilisateur d\u0026rsquo;Argo Workflows. Une fois que nous en aurons terminé avec les outils natifs d\u0026rsquo;Argo, nous terminerons par la façon de déboguer votre environnement à l\u0026rsquo;aide de kubectl.\nL\u0026rsquo;interface de ligne de commande d\u0026rsquo;Argo Workflows\nL\u0026rsquo;interface en ligne de commande (CLI) d\u0026rsquo;Argo Workflows rend les interactions avec votre cluster simples. Je vais présenter ci-dessous comment exécuter des commandes Argo directement depuis votre cluster Kubernetes sur un espace de nom appelé argo. N\u0026rsquo;oubliez pas que vous pouvez rapidement adapter le code présenté ci-dessous à votre environnement. Pour le reproduire dans votre environnement, changez l\u0026rsquo;option en -n namespace où namespace est l\u0026rsquo;espace de nom Kubernetes où vous avez déployé Argo.\nÉtat du flux de travail avec argo list Je veux couvrir deux commandes : argo list et argo get. Avec argo list, vous pouvez identifier rapidement le statut de tous les workflows déployés sur l\u0026rsquo;espace de nom Kubernetes fourni. Et vous pouvez examiner de près chaque workflow avec la commande argo get. Bien que les deux commandes ne soient pas dépendantes l\u0026rsquo;une de l\u0026rsquo;autre, leur combinaison devrait répondre à la plupart de vos questions sur l\u0026rsquo;exécution des pods. Je suggère d\u0026rsquo;utiliser le drapeau -A comme indiqué ci-dessous. Cela élargira la recherche de tous vos espaces de noms.\nargo list -A Comme vous pouvez le voir, la commande renvoie sous forme de tableau un état mis à jour de tous les workflows de votre environnement par espace de noms. Cette présentation est utile pour les contrôles de santé automatisés de vos services.\nDans les cas où vous souhaitez concentrer votre recherche sur un espace de noms spécifique, la commande argo list peut également rechercher un seul espace de noms grâce à la balise -n. Ceci est utile lorsque vous avez des flux de travail similaires déployés sur différents espaces de noms, un scénario très répandu dans la gestion des environnements multi-tenant. Ainsi, dans des cas comme celui-ci, il est préférable d\u0026rsquo;exécuter argo list -n , où est l\u0026rsquo;espace de noms audité.\nVérifier vos flux de travail avec kubectl\nIl existe des scénarios où la sortie de l\u0026rsquo;Argo CLI ne fournit pas assez d\u0026rsquo;informations pour une analyse complète d\u0026rsquo;un pod.\nPar exemple, dans certains cas, votre serveur Argo ne sera pas capable d\u0026rsquo;accéder à votre clé de jeton, et il serait utile de voir comment votre plan de contrôle a résolu cette valeur. Pour ce faire, vous pouvez utiliser les commandes kubectl pour explorer la configuration et la santé de votre instance Argo Workflows. Si les services dépendants ne sont pas accessibles, cette méthode vous sera utile.\nArgo Workflows déploie trois conteneurs dans chaque pod, et tous sont accessibles à l\u0026rsquo;aide des commandes kubectl, comme mentionné précédemment.\nGetting your Argo deployment details with Kubernetes native commands Here we’re looking at the k8s cluster as a whole. Using the command below, you can retrieve high-level info about the health of your deployed services.\nkubectl get svc,po -n argo -o wide This presents you with information about the k8s control plane for your argo namespace. From here, you can use kubectl describe for more detailed information.\nArgo and Kubernetes use the same pod name for their deployed components. Bear that in mind if you plan to automate your pipelines with a mix of kubectl and Argo Workflows CLI combinations for your observability strategy.\nSo, use kubectl describe to view how Kubernetes sees your pod deployment. This should resemble what you see on the Argo console UI.\nKubectl generates a lot of output, so pipe it through more or less.\nkubectl describe -n argo pod/dag-sum-pm8rp-2964331963 | less It is good to note that the pod name will be the same in both kubectl and Argo CLI commands. Vous n\u0026rsquo;aurez aucune surprise en choisissant celle que vous préférez utiliser dans votre analyse tant que vous utilisez le même nom de pod. Ainsi, l\u0026rsquo;exécution de kubectl logs -n argo pod/dag-swtgb-320908401 -c main ou argo logs -n argo dag-swtgb dag-swtgb-320908401 -c main ; imprimera alors la trace d\u0026rsquo;audit de votre conteneur principal à l\u0026rsquo;intérieur du pod Kubernetes dag-swtgb-320908401 mais en utilisant des interfaces de ligne de commande différentes.\nVous pouvez également explorer les conteneurs init et wait de la même manière qu\u0026rsquo;en utilisant les commandes CLI du workflow Argo. Bien que légèrement différent, il renverra les données détaillées des déploiements Kubernetes. C\u0026rsquo;est un choix personnel de savoir si vous voulez utiliser kubectl ou les commandes natives d\u0026rsquo;Argo.\nPlus de détails avec argo get\nUne fois que vous avez le service dont vous avez besoin et son espace de nom, vous pouvez voir plus de détails avec la commande argo get. L\u0026rsquo;exécution de la commande suivante vous donnera un message consolidé par étape du workflow Argo. Dans notre exemple, le workflow dag-swtgb existe sur l\u0026rsquo;espace de nom argo.\nargo get -n argo dag-swtgb L\u0026rsquo;exécution de la même commande sans l\u0026rsquo;option -o yaml renverra une sortie comme celle ci-dessous avec une vue plus consolidée. Il est utile de voir le message produit par vos pods problématiques.\nSortie de la commande get d\u0026rsquo;Argo avec des erreurs\nGo Deeper with argo logs Les déploiements Argo partagent les déploiements de flux de travail de chaque pod sur les conteneurs principal, init et wait. Je couvrirai comment y accéder en utilisant kubectl dans un peu, mais vous pouvez également y accéder en utilisant les journaux Argo sur le pod désiré.\nJe dois également préciser que si votre pod ne démarre pas, ces conteneurs ne démarreront pas non plus.\nVous pouvez suivre les logs d\u0026rsquo;un workflow avec les logs d\u0026rsquo;Argo.\nConsidérez cette session de ligne de commande :\n``sh argo -n argo submit sum.yaml\net sa sortie : Nom : dag-sum-gm5sv Espace de nom : argo ServiceAccount : unset (sera exécuté avec le ServiceAccount par défaut) Statut : En attente Créé : Tue Apr 05 12:53:30 -0400 (now) Progression :\nCe flux de travail n\u0026rsquo;a pas de contexte de sécurité défini. Vous pouvez exécuter vos pods de workflow de manière plus sécurisée en le définissant. Pour en savoir plus, rendez-vous sur https://argoproj.github.io/argo-workflows/workflow-pod-security-context/\nLorsque vous soumettez un nouveau workflow, Argo vous donne son nom. Passez ce nom aux logs d\u0026#39;argo avec le namesapce et l\u0026#39;option `--follow` : ```sh argo -n argo logs dag-sum-gm5sv --follow Sortie :\ndag-sum-gm5sv-899595302 : time=\u0026#34;2022-04-05T16:53:32.791Z\u0026#34; level=info msg=\u0026#34;capturing logs\u0026#34; argo=true dag-sum-gm5sv-899595302 : 2 dag-sum-gm5sv-508066804 : time=\u0026#34;2022-04-05T16:53:33.072Z\u0026#34; level=info msg=\u0026#34;capturing logs\u0026#34; argo=true dag-sum-gm5sv-508066804 : 2 dag-sum-gm5sv-844819 : time=\u0026#34;2022-04-05T16:53:42.984Z\u0026#34; level=info msg=\u0026#34;capturing logs\u0026#34; argo=true dag-sum-gm5sv-844819 : 4 Argo affichera les logs à l\u0026rsquo;écran au fur et à mesure de la progression du workflow.\nSi vous ne souhaitez pas utiliser la ligne de commande, vous pouvez également le faire via l\u0026rsquo;interface utilisateur des workflows Argo.\nVoir les événements de vos workflows Argo dans la console UI\nEnfin, vous pouvez également déboguer votre environnement Argo en utilisant l\u0026rsquo;interface console qu\u0026rsquo;il fournit. Ce service est accessible en suivant l\u0026rsquo;une des étapes mentionnées dans leur docs, mais dans ce cas, nous ferons un simple port forward entre le déploiement Kubernetes et l\u0026rsquo;hôte. Le code présenté ici peut fonctionner sans problème sur des machines Linux et macOS. Ces environnements vous permettent de lier le port entre votre poste de travail et votre cluster Kubernetes en tant que processus d\u0026rsquo;arrière-plan avec cette commande: kubectl -n argo port-forward svc/argo-server 2746:2746 \u0026amp;. Le service étant accessible depuis l\u0026rsquo;hôte, vous pouvez faire pointer n\u0026rsquo;importe quel navigateur web sur l\u0026rsquo;adresse https://localhost:2746.\nConclusion\nVous avez vu comment déboguer les composants des workflows Argo à l\u0026rsquo;aide de la CLI et de l\u0026rsquo;interface utilisateur Argo, ou des commandes kubectl. Les étapes que j\u0026rsquo;ai décrites ici peuvent vous aider à comprendre ce qui se passe dans votre environnement.\nÀ+!\n","permalink":"https://www.dawrlog.com/fr/articles/4/","summary":"Bonjour à tous,\nAujourd\u0026rsquo;hui, je voudrais vous expliquer comment déboguer vos flux de travail Argo, ce qui est pratique lorsque vous avez besoin de plus de détails sur votre environnement. Voyons les deux approches différentes pour déboguer vos déploiements de workflows Argo.\nTout d\u0026rsquo;abord, nous utiliserons l\u0026rsquo;Interface de ligne de commande Argo, ou CLI pour faire court. Les commandes CLI d\u0026rsquo;Argo couvrent la plupart des erreurs les plus courantes rencontrées avec les workflows, telles que les mauvaises configurations.","title":"Moyens de debug un workflow Argo."},{"content":"Bonjour à tous, j\u0026rsquo;espère que vous allez bien !\nSi vous connaissez Argo Workflows, vous savez déjà qu\u0026rsquo;il peut piloter vos pipelines CI/CD, gérer vos processus ETL et orchestrer tout ensemble de tâches que vous pouvez imaginer pour un cluster Kubernetes. Mais saviez-vous qu\u0026rsquo;Argo sait aussi archiver les résultats de ses workflows dans une base de données SQL ?\nDans ce billet, je vais montrer comment Argo Workflows archive l\u0026rsquo;état des workflows dans un stockage persistant en utilisant une base de données Postgres. Pour ce faire, je présenterai un rapide résumé des composants d\u0026rsquo;Argo tout en montrant ce que signifie l\u0026rsquo;archivage de votre workflow. Nous déploierons les workflows Argo avec une base de données Postgres sur une instance locale de Kubernetes en utilisant k3d. Enfin, nous discuterons de quelques considérations de sécurité importantes pour votre déploiement d\u0026rsquo;Argo Workflows.\nAlors, c\u0026rsquo;est parti.\nQu\u0026rsquo;est-ce que l\u0026rsquo;option d\u0026rsquo;archivage dans les workflows Argo ? La possibilité de stocker les exécutions de flux de travail antérieures vous fournit un enregistrement précis de vos états de flux de travail antérieurs. Cet état des lieux change la donne, car il permet de provisionner vos ensembles de tâches en fonction de métriques en temps réel, comme les pics de besoins de traitement des déploiements passés.\nL\u0026rsquo;option d\u0026rsquo;archivage des flux de travail stocke vos états de flux de travail dans MySQL ou Postgres. Une fois les archives configurées, vous pouvez les utiliser pour mieux comprendre le fonctionnement de vos tâches et les points à améliorer.\nPar exemple, elles peuvent vous aider à savoir quand c\u0026rsquo;est une bonne idée de faire évoluer votre trafic à l\u0026rsquo;aide d\u0026rsquo;instances temporaires, dont les états seront également stockés dans la même base de données. Avec tous vos états conservés au fil du temps, vous pouvez appliquer des règles pour ajuster la taille de votre cluster en fonction de l\u0026rsquo;utilisation précédente ; une bonne analyse des séries chronologiques pourrait même vous faire économiser de l\u0026rsquo;argent au final.\nL\u0026rsquo;archive ne stocke que les états précédents du flux de travail, et non les journaux d\u0026rsquo;instance détaillés. Les journaux d\u0026rsquo;audit détaillés sont un autre élément à prendre en compte. L\u0026rsquo;option de stockage des artefacts gère l\u0026rsquo;option de persistance des journaux détaillés, en les stockant localement par MinIO. Mais vous pouvez également configurer toute autre option de stockage d\u0026rsquo;objets. Ceci est couvert dans la documentation officielle d\u0026rsquo;Argo, où vous pouvez voir comment utiliser des options telles que les buckets Google Cloud Storage ou AWS S3.\nMais avant de commencer l\u0026rsquo;implémentation technique, faisons un rapide rappel sur les composants des workflows Argo. Il est nécessaire de savoir comment ils sont corrélés avec le stockage persistant de vos workflows archivés ; cette image tirée de la documentation des workflows Argo présente une vue d\u0026rsquo;ensemble de l\u0026rsquo;environnement où réside un workflow :\n_Source : Dépôt Github du flux de travail Argo. How to Deploy Argo Workflows with Persistent Storage\nNow that we know what\u0026rsquo;s in store for us let\u0026rsquo;s get started. We\u0026rsquo;ll be using k3d to manage our local Kubernetes environment (instead of minikube and VirtualBox). In addition to k3d, you\u0026rsquo;ll need to install Docker as an additional dependency. Using kubectl to interact with your Kubernetes cluster works fine, too. As for our tutorial, we\u0026rsquo;ll be using local Kubernetes deployment scripts.\nFirst, we\u0026rsquo;ll start our local control plane with the following command:\nk3d cluster create cluster-demo The successful creation will provide a log similar to this one:\nCreating the cluster\nOnce we have our `cluster-demo`, we'll deploy our Argo Workflows instance. To install Argo Workflows, you'll need to execute the following commands: kubectl create ns argo kubectl apply -n argo -f https://raw.githubusercontent.com/argoproj/argo-workflows/master/manifests/quick-start-postgres.yaml La première crée un espace de nom appelé argo dans votre cluster, et la ligne suivante va déployer les composants des workflows Argo sur votre cluster, comme vous pouvez le voir ci-dessous :\nDéploiements effectués sur le cluster\nCréer le contrôleur de workflow Pour exécuter le workflow avec l\u0026rsquo;option d\u0026rsquo;archivage, vous devez d\u0026rsquo;abord changer la configuration de persistance à archive:true sur votre déploiement de serveur Argo. En la modifiant, vous indiquerez à votre serveur Argo de stocker les états d\u0026rsquo;exécution de votre workflow dans la base de données signalée par la clé postgresql.\nNous appliquerons un nouveau ConfigMap dans notre espace de noms Kubernetes argo actuel avec l\u0026rsquo;instance Postgres pour stocker vos workflows archivés. Vous pouvez ensuite archiver vos workflows en utilisant l\u0026rsquo;option archiveLogs.\nNous avons déployé une instance Postgres avec le YAML de démarrage rapide que nous avons utilisé précédemment. Vous en aurez besoin uniquement pour appliquer la configuration suivante à votre déploiement. Changer cette configuration permet à votre déploiement de serveur Argo d\u0026rsquo;accepter la notation archiveLocation.archiveLogs lors de la création de vos workflows. Nous allons commencer par créer un nouveau workflow-controller-configmap.yml avec le contenu suivant et le sauvegarder localement :\napiVersion : v1 kind : ConfigMap métadonnées : nom : workflow-controller-configmap données : persistance : | connectionPool : maxIdleConns : 100 maxOpenConns : 0 connMaxLifetime : 0s nodeStatusOffLoad : true archive : true archiveTTL : 7d postgresql : hôte : postgres port : 5432 base de données : postgres tableName : argo_workflows nom d\u0026#39;utilisateurSecret : nom : argo-postgres-config clé : username passwordSecret : nom : argo-postgres-config clé : mot de passe retentionPolicy : | terminé : 10 échoué : 3 erroné : 3 Déployer votre environnement avec kubectl Nous allons exposer l\u0026rsquo;interface web des workflows Argo en utilisant un équilibreur de charge sur notre espace de nom argo. L\u0026rsquo;équilibreur de charge exposera le pod exécutant le composant web aux connexions provenant de l\u0026rsquo;extérieur de Kubernetes.\nkubectl apply -n argo -f workflow-controller-configmap.yml Votre serveur Argo va redémarrer avec la nouvelle configuration dans quelques minutes. N\u0026rsquo;hésitez pas à vérifier son état en exécutant kubectl get -n argo svc,pod sur votre cluster Kubernetes.\nkubectl get -n argo svc,pod Vous pouvez ensuite lier votre cluster Kubernetes et votre hôte au port 2746 en exécutant la commande suivante sur votre cluster :\nkubectl -n argo port-forward deployment/argo-server 2746:2746 \u0026amp; Félicitations, vous venez de déployer les workflows Argo sur un cluster k3d. Pour confirmer que votre instance locale est opérationnelle, rendez-vous sur https://localhost:2746.\nPage de l\u0026rsquo;interface utilisateur des flux de travail Argo. Testing Your Deployment\nFélicitations pour avoir installé votre instance Argo Workflows sur votre cluster Kubernetes local avec l\u0026rsquo;option d\u0026rsquo;archivage. Et maintenant que nous avons coché cela sur notre liste, archivons nos workflows. L\u0026rsquo;ajout de l\u0026rsquo;annotation archiveLogs vous permet de spécifier ceux que vous souhaitez archiver, comme le montre le modèle suivant, que nous appellerons workflow-archive.yml.\napiVersion : argoproj.io/v1alpha1 kind : Flux de travail métadonnées : generateName : archive-location- spec : archiveLogs : true # active les journaux pour ce flux de travail point d\u0026#39;entrée : whalesay modèles : - nom : whalesay conteneur : image : docker/whalesay:latest commande : [cowsay] args : [\u0026#34;hello world\u0026#34;](Bonjour monde) Nous devons exécuter argo submit -n argo --watch -f workflow-archive.yml sur un terminal pour le déployer.\nargo submit -n argo --watch -f workflow-archive.yml En faisant cela, vous lancerez le workflow archive-location sous l\u0026rsquo;espace de noms argo ; la sortie suivante confirme que notre exemple s\u0026rsquo;est exécuté avec succès :\nExemple d\u0026rsquo;archivage des flux de travail Argo\nCela ne change pas sur la ligne de commande ; cependant, comme nous avons un stockage persistant pour nos workflows, vous pouvez voir leurs états précédents sur l\u0026rsquo;interface utilisateur de la console. Cela vous donnera les états précédents du workflow qui ont été exécutés avec les options d\u0026rsquo;archivage activées - et en allant sur l\u0026rsquo;interface utilisateur de la console Argo Workflows à https://localhost:2746, comme nous l\u0026rsquo;avons vu précédemment, vous pouvez accéder à l\u0026rsquo;option d\u0026rsquo;interface utilisateur du workflow archivé depuis les icônes de la barre de menu de gauche. Une fois que vous êtes là, vous pouvez voir toutes les exécutions passées d\u0026rsquo;un workflow. L\u0026rsquo;historique de votre workflow se trouve dans l\u0026rsquo;interface utilisateur sous \u0026ldquo;Workflows archivés\u0026rdquo; (voir ci-dessous).\nEcran d\u0026rsquo;archivage des flux de travail Argo Bonnes pratiques de sécurité pour l\u0026rsquo;archivage des flux de travail Argo dans Postgres.\nDans notre travail, nous avons déployé une instance Argo avec l\u0026rsquo;option d\u0026rsquo;archivage configurée avec une base de données Postgres. Comme mentionné précédemment, ce code n\u0026rsquo;est pas prêt pour la production. Comme prochaine étape, je suggère de gérer vos jetons d\u0026rsquo;accès pour sécuriser votre instance Argo.\nUne bonne pratique est d\u0026rsquo;éviter les valeurs codées en dur pour les informations d\u0026rsquo;exécution du serveur lorsque cela est possible. Votre infrastructure devrait générer des données comme votre nom d\u0026rsquo;hôte Postgres au moment de l\u0026rsquo;exécution au lieu de les coder en dur. Votre infrastructure devrait utiliser des secrets pour stocker des informations sensibles comme les clés d\u0026rsquo;accès au référentiel.\nArchivage Postgres\nJetez un œil à cette introduction sur les secrets et les configmaps dans Kubernetes, pour plus de détails sur les informations Kubernetes qui doivent être discrètes. L\u0026rsquo;adoption de bonnes pratiques de sécurité comme celle-ci dès le début est plus facile pour vos utilisateurs et vos développeurs lorsque vous commencez à évoluer. En outre, l\u0026rsquo;automatisation de la configuration permet de réduire la surface d\u0026rsquo;attaque de votre environnement tout en réduisant les tâches de gestion de l\u0026rsquo;infrastructure.\nVoici un article de blog utile contenant d\u0026rsquo;autres bonnes pratiques de sécurité Argo par le responsable des flux de travail Argo, Alex Collins.\nConclusion\nNous avons déployé Argo Workflows localement et archivé un workflow en utilisant une base de données Postgres dans ce post. Les scripts présentés ici sont de bons points de départ pour comprendre et expérimenter l\u0026rsquo;option d\u0026rsquo;archivage d\u0026rsquo;Argo Workflows, mais gardez à l\u0026rsquo;esprit que certains facteurs critiques sont manquants pour un environnement entièrement cloud native.\nA+ !\n","permalink":"https://www.dawrlog.com/fr/articles/2/","summary":"Bonjour à tous, j\u0026rsquo;espère que vous allez bien !\nSi vous connaissez Argo Workflows, vous savez déjà qu\u0026rsquo;il peut piloter vos pipelines CI/CD, gérer vos processus ETL et orchestrer tout ensemble de tâches que vous pouvez imaginer pour un cluster Kubernetes. Mais saviez-vous qu\u0026rsquo;Argo sait aussi archiver les résultats de ses workflows dans une base de données SQL ?\nDans ce billet, je vais montrer comment Argo Workflows archive l\u0026rsquo;état des workflows dans un stockage persistant en utilisant une base de données Postgres.","title":"Archivage des Argo Workflows dans une instance Postgres: Quoi a faire pour configurer votre entrepôt de données"},{"content":"Salut à tous, j\u0026rsquo;espère que vous allez tous bien.\nIl est frustrant de constater que les délais de livraison d\u0026rsquo;un projet sont parfois si serrés que vous négligez la qualité de votre application de développement. En raison de la faiblesse des politiques de sécurité, c\u0026rsquo;est encore pire lorsque votre équipe de sécurité ne parvient pas à détecter les dégâts avant qu\u0026rsquo;il ne soit trop tard. Pour vous aider, je vais vous parler des attaques DDoS ciblées et de leurs effets sur votre interface de programmation d\u0026rsquo;applications, également connue sous le nom de points de terminaison API.\nJe vais vous expliquer ce qu\u0026rsquo;est un DDoS et les problèmes qu\u0026rsquo;il peut causer. Ensuite, je vous expliquerai comment vérifier si vous avez été attaqué à l\u0026rsquo;aide de Wireshark. Il s\u0026rsquo;agit de l\u0026rsquo;un des analyseurs de réseau les plus connus du marché. Je montrerai ensuite comment vous pouvez réduire la surface d\u0026rsquo;attaque de votre environnement. Enfin, je terminerai par un récapitulatif de ce que nous avons analysé. Commençons par comprendre ce qu\u0026rsquo;est un DDoS et ce qu\u0026rsquo;il peut faire à vos points d\u0026rsquo;extrémité d\u0026rsquo;API.\nQu\u0026rsquo;est-ce qu\u0026rsquo;un DDoS, et comment cela affecte-t-il vos requêtes d\u0026rsquo;API ? Passons en revue quelques concepts liés aux API et explorons ce que signifie une attaque DDoS. DDoS est l\u0026rsquo;abréviation de distributed denial of service (déni de service distribué). Elle consiste à bourrer la connexion de votre réseau à vos services. Ces requêtes, effectuées sur la couche 7 du modèle OSI, sont dites non valides. La couche 7 est également connue sous le nom de couche applicative, qui inonde votre serveur de requêtes fantômes, ce qui crée à son tour ce que l\u0026rsquo;on appelle un réseau zombie.\nIl existe donc des cas où les hôtes de votre machine attaquent un serveur ciblé sans votre consentement. C\u0026rsquo;est une raison supplémentaire de s\u0026rsquo;assurer que même la sécurité de votre ordinateur personnel est plus robuste. Maintenant que nous avons passé en revue une attaque DDoS, voyons ce qu\u0026rsquo;est une demande d\u0026rsquo;API. Il existe d\u0026rsquo;autres types d\u0026rsquo;attaques ciblant les demandes d\u0026rsquo;API, comme le montre ce repo Github.\nLes attaques DDoS se concentrent non seulement sur le serveur où votre API est exécutée, mais aussi sur chaque point de terminaison de votre service d\u0026rsquo;API. Dans les attaques plus avancées, votre service d\u0026rsquo;API est attaqué à la fois sur le serveur et sur le service d\u0026rsquo;API lui-même. En cas d\u0026rsquo;attaque réussie, les résultats sont dramatiques pour la santé de votre serveur d\u0026rsquo;API. Ceci étant dit, voyons comment identifier un réseau compromis avec Wireshark, un analyseur de réseau. Comment trouvé la compromise d\u0026rsquo;un trafic réseau avec Wireshark Wireshark est un outil pratique pour l\u0026rsquo;analyse judiciaire de votre réseau. Il s\u0026rsquo;agit également d\u0026rsquo;un outil polyvalent que vous devriez avoir à votre disposition si vous voulez vraiment entrer dans les détails de votre trafic. Prenons l\u0026rsquo;exemple d\u0026rsquo;un réseau compromis. Dans notre exemple, nous allons nous rendre sur ce lien pour accéder à un journal Wireshark nommé sec-sickclient.pcapng, comme illustré ci-dessous.\nsec-sickclient.pcapng wireshark log\nLe journal confirme que les requêtes effectuées par l\u0026rsquo;IP 10.129.211.13 sur le port 1047 ne peuvent pas atteindre le serveur 216.234.235.165 sur le port 18067. La première chose à remarquer est le numéro de port inhabituel. Les attaques DDoS ciblent généralement des ports non réguliers. L\u0026rsquo;objectif de l\u0026rsquo;attaquant est d\u0026rsquo;inonder le serveur de requêtes non valides, qui seront concurrentes des requêtes valides. Un autre conseil lors du contrôle de la validité des appels d\u0026rsquo;API est de vérifier si la somme de contrôle est correcte. Sur l\u0026rsquo;extrait, vous pouvez voir que la somme de contrôle d\u0026rsquo;une demande non valide est incorrecte - les demandes non valides comme celle-ci inondent le serveur, qui ne répond plus. Maintenant que vous savez ce qu\u0026rsquo;est une attaque DDoS et comment la traquer, voyons quelques approches qui permettent de limiter la surface d\u0026rsquo;attaque de vos services. Nous commencerons par atténuer votre surface d\u0026rsquo;attaque en filtrant les demandes de trafic en amont. L\u0026rsquo;approche des demandes filtrées en amont Il existe des moyens de filtrer vos demandes. Je préfère le réseau de diffusion de contenu, ou CDN pour faire court. Le CDN masque le code source de votre application tout en servant les données de la couche d\u0026rsquo;application avec son contenu en cache. Il fonctionne comme une option de défense en amont de la sécurité en filtrant les requêtes sur vos applications et en aidant vos utilisateurs avec des données à faible latence mises en cache. Vous pouvez disposer d\u0026rsquo;outils tiers offrant des solutions CDN, tels que AWS CloudFront. Néanmoins, il est bon d\u0026rsquo;avoir un plan de réponse minimal avant de contacter vos fournisseurs d\u0026rsquo;accès Internet. Il peut également être utile que vos services destinés aux utilisateurs accèdent à votre contenu web, comme les vidéos et la musique, sur un stockage en cache sécurisé. Cette approche permet de filtrer le trafic avant qu\u0026rsquo;il n\u0026rsquo;atteigne votre réseau, ce qui facilite la gestion de vos serveurs. Mais cette approche nécessite encore quelque chose de plus pour vous protéger si votre environnement est découvert et compromis. C\u0026rsquo;est là qu\u0026rsquo;un pot de miel peut vous aider.\nL\u0026rsquo;approche Honeypot (pot de miel). Je trouve que votre environnement est la meilleure source de données pour votre plan d\u0026rsquo;atténuation. Vous disposerez de données précises sur vos attaques grâce à un pot de miel de logiciels malveillants qui pourra simuler vos environnements frontaux et dorsaux. Votre pot de miel peut fonctionner comme un piège à rat si vous laissez délibérément certaines vulnérabilités ouvertes aux attaquants. C\u0026rsquo;est un jeu risqué, car votre pot de miel doit être identique à votre environnement de production. Sinon, vous avez invité vos attaquants à explorer votre environnement. Mais lorsqu\u0026rsquo;il est déployé correctement, il devient un outil puissant pour sécuriser votre domaine. Un bon pot de miel peut également montrer dans quelle mesure vos systèmes de défense bloquent les attaques. Un autre avantage est qu\u0026rsquo;il montre quelles données doivent faire l\u0026rsquo;objet de plus de mesures de sécurité. Même avec un pot de miel exposé, votre réseau peut souffrir sans une excellente gestion de vos demandes d\u0026rsquo;API. Pour vous assurer que vous êtes couvert à cet égard, vous pouvez limiter les ressources de votre réseau. Limiter les ressources de votre réseau Vous pouvez configurer votre contrôleur d\u0026rsquo;interface réseau pour qu\u0026rsquo;il gère un trafic maximal par session. Ce que l\u0026rsquo;on appelle la limitation du débit peut se faire par logiciel ou par matériel. Si le premier gère le nombre d\u0026rsquo;appels simultanés, le second s\u0026rsquo;occupe de la configuration de vos commutateurs et routeurs. En limitant le débit de vos ressources réseau, vous avez la certitude que votre application est en bon état, même si certains utilisateurs subissent une latence plus élevée de vos services attaqués. Un bon plan de réponse s\u0026rsquo;accompagne de plusieurs couches de sécurité. Nous allons maintenant voir comment vous pouvez bénéficier d\u0026rsquo;un réseau de diffusion de contenu avec un pot de miel. Comment un pot de miel pour logiciels malveillants et un réseau de diffusion de contenu peuvent améliorer vos défenses Comme nous l\u0026rsquo;avons déjà mentionné, le CDN servira le contenu de votre couche applicative, ce qui ne couvre qu\u0026rsquo;une partie de votre plan de sécurité. Vous pouvez bénéficier d\u0026rsquo;un pot de miel comme première surface d\u0026rsquo;attaque, et il doit se trouver dans un environnement contrôlé où réside votre application. Votre plan de sécurité doit utiliser une combinaison de services axés sur différents domaines d\u0026rsquo;application, et le principe directeur de sécurité renforce la sécurité des parties interconnectées. Ainsi, la combinaison de votre CDN et d\u0026rsquo;un pot de miel de logiciels malveillants peut aider votre équipe à appliquer le plan d\u0026rsquo;intervention en place, en atténuant la lenteur et la non-disponibilité de vos services. Cela vous donnera ensuite suffisamment de temps pour réitérer vos prestations dégradées de manière plus sûre, sans ouvrir de nouvelles menaces. Faisons le point en vérifiant les sujets que nous avons abordés aujourd\u0026rsquo;hui. Conclusion Les attaques DDoS rendent votre environnement instable en lançant des appels de service à un service ciblé avec des requêtes non valides. Bien qu\u0026rsquo;il existe de nombreux types d\u0026rsquo;attaques DDoS, nous négligeons souvent celles qui portent sur la santé de vos services API. Je vous suggère de revoir les conseils de l\u0026rsquo;OWASP API security. En fonction du flux de données et de l\u0026rsquo;accessibilité de vos services, vous pouvez adopter des mesures supplémentaires. L\u0026rsquo;idée est de réduire votre surface d\u0026rsquo;attaque. Cependant, vous ne voulez pas construire une boîte noire. La sécurité et la convivialité de vos composants doivent être équilibrées pour que votre service soit adopté par vos développeurs et vos utilisateurs.\nÀ+!\n","permalink":"https://www.dawrlog.com/fr/articles/3/","summary":"Salut à tous, j\u0026rsquo;espère que vous allez tous bien.\nIl est frustrant de constater que les délais de livraison d\u0026rsquo;un projet sont parfois si serrés que vous négligez la qualité de votre application de développement. En raison de la faiblesse des politiques de sécurité, c\u0026rsquo;est encore pire lorsque votre équipe de sécurité ne parvient pas à détecter les dégâts avant qu\u0026rsquo;il ne soit trop tard. Pour vous aider, je vais vous parler des attaques DDoS ciblées et de leurs effets sur votre interface de programmation d\u0026rsquo;applications, également connue sous le nom de points de terminaison API.","title":"Comment diminuer les attaques DDOS parmis vos APIs."},{"content":"Bonjour à tous, aujourd\u0026rsquo;hui je voudrais vous parler de Bigquery, le service géré par Google Cloud pour les entrepôts de données. Et j\u0026rsquo;aimerais explorer ses types de données.\nGoogle Cloud Platform est depuis longtemps le fournisseur de clouds de prédilection pour l\u0026rsquo;analyse Web, l\u0026rsquo;impressionnant BigQuery étant l\u0026rsquo;option de traitement massivement parallèle (alias MPP) de Google. Les bases de données MPP sont très courantes dans les écosystèmes Hadoop.\nBigQuery est très convivial pour ses utilisateurs (convenons que BigQuery est aussi agréable que sa connaissance de SQL), ce qui favorise l\u0026rsquo;arrivée de nouveaux utilisateurs. En partie, grâce à une exploration plus détaillée des données Google Analytics, comme décrit sur cet exemple.\nCet article a pour but de présenter BigQuery et ses types de données existants par catégories SQL. Je donnerai également un cas d\u0026rsquo;utilisation pratique où la connaissance de ces types de données peut être utile : la vérification des données lors de l\u0026rsquo;ingestion des données.\nEn sachant quel type de données BigQuery utiliser, vous pourrez créer plus facilement vos pipelines de données. Une fois que vous aurez mieux compris quels types de données sont disponibles, la qualité de vos rapports augmentera considérablement en un rien de temps. Types SQL dans BigQuery\nBigQuery fonctionne à l\u0026rsquo;aide d\u0026rsquo;un langage d\u0026rsquo;interrogation structuré (SQL), subdivisé en catégories anciennes et standard.\nIl est bon de préciser que chaque sous-groupe a des types de données et des façons d\u0026rsquo;interroger la base de données différents. Lorsque l\u0026rsquo;indice n\u0026rsquo;est pas renseigné, BigQuery utilise les fonctions et méthodes SQL standard. Jetez un coup d\u0026rsquo;œil à la rubrique Indices SQL si vous êtes novice et souhaitez en savoir un peu plus à leur sujet.\nPour présenter ce à quoi ressemble une requête patrimoniale, je vais montrer un exemple d\u0026rsquo;exécution d\u0026rsquo;une commande SQL utilisant le type de catégorie patrimoniale.\nExemple d\u0026rsquo;astuce SQL\n#legacySQL -- This hint informs that the query will use Legacy SQL SELECT weight_pounds, state, year, gestation_weeks FROM [bigquery-public-data:samples.natality] ORDER BY weight_pounds DESC; Le SQL standard est le type que vous devriez chercher à utiliser lorsque vous n\u0026rsquo;avez pas de contraintes, comme le code hérité sur votre déploiement lift-and-shift.\nLe SQL standard permet également des fonctionnalités très intéressantes comme les fonctions de fenêtre et une structure de fonctions définies par l\u0026rsquo;utilisateur plus robuste, et il est plus souple de créer sur le SQL standard plutôt que sur le SQL hérité.\nIl est également recommandé, en tant que meilleure pratique, de migrer votre SQL patrimonial existant vers le SQL standard.\nGoogle propose même une page utile contenant des conseils sur cette conversion. Vous pouvez vérifier comment la lancer correctement sur cette page.\nTypes de données SQL standard\nLe tableau ci-dessous présente les types de données possibles en utilisant la catégorie SQL standard. Je tiens à souligner que la catégorie SQL standard est la catégorie préférée de Google, ce qui signifie que vous pouvez supposer à juste titre qu\u0026rsquo;elle offre de meilleures performances et davantage d\u0026rsquo;options par rapport à la catégorie SQL traditionnelle.\nBigquery Standard SQL Datatypes (en anglais) Types de données SQL standardTypes de données SQL standard\nLa catégorie SQL standard accepte des types de données plus complexes tels que les types de données ARRAY, STRUCT et GEOGRAPHY. Tous les types de données mentionnés peuvent ordonner (ou grouper) les résultats de n\u0026rsquo;importe quel ensemble de données ; l\u0026rsquo;une de ses graves limitations est que les types de données STRUCT et ARRAY sont fortement utilisés pour les [entrées de données] en continu (https://whatis.techtarget.com/definition/data-ingestion).\nUn autre exemple vient du fait que le type de données ARRAY ne compare pas ses valeurs avec un champ quelconque, ce qui se produit dans STRUCT et GEOGRAPHY.\nNous pouvons utiliser ST_EQUALS comme \u0026ldquo;solution de rechange\u0026rdquo; pour comparer directement les valeurs géographiques.\nTous les autres types de données peuvent filtrer les clauses SQL JOIN et être utilisés pour ordonner les résultats ; n\u0026rsquo;oubliez jamais de caster les colonnes utilisées pour éviter explicitement les effets indésirables.\nLe langage SQL standard permet également l\u0026rsquo;utilisation de ce que l\u0026rsquo;on appelle les procédures stockées.\nLa procédure stockée permet l\u0026rsquo;exécution de fonctions reproductibles - très utile pour les transformations de logique métier partageables entre différents départements.\nPar exemple, la façon dont le département des ressources humaines calcule les bénéfices pourrait être utile au département du marketing pour le calcul des campagnes.\nL\u0026rsquo;avantage des formats de données bien définis commence avec vos procédures stockées - car les options sur l\u0026rsquo;étape précédente de vos pipelines d\u0026rsquo;analyse donnent à vos analystes un temps de réaction plus court pour analyser vos données.\nTypes de données SQL hérité (Legacy SQL)\nLegacy SQL utilise les mêmes types de données que ceux utilisés avec SQL standard, à l\u0026rsquo;exception de ARRAY, STRUCT et GEOGRAPHY.\nLes champs de type NUMERIC offrent un support limité, ce qui rend nécessaire une conversion explicite à l\u0026rsquo;aide de la fonction cast lors de l\u0026rsquo;interaction avec ces champs.\nLe tableau ci-dessous énumère tous les types de données possibles disponibles lors de l\u0026rsquo;utilisation de la catégorie de requête SQL héritée.\nVous pouvez toujours accéder aux données imbriquées à l\u0026rsquo;aide de la notation point, mais elle ne permet pas de réaliser de belles astuces comme la génération de votre tableau au moment de l\u0026rsquo;exécution.\nLegacy SQL vous permettra de créer des fonctions réutilisables et partageables entre différentes requêtes. Cette possibilité est offerte par les fonctions définies par l\u0026rsquo;utilisateur (ou UDF) ; vous trouverez ci-dessous un exemple d\u0026rsquo;une fonction simple.\nUDF sur legacy SQL\n// UDF definition function urlDecode(row, emit) { emit({title: decodeHelper(row.title), requests: row.num_requests}); } // Helper function with error handling function decodeHelper(s) { try { return decodeURI(s); } catch (ex) { return s; } } // UDF registration bigquery.defineFunction(\u0026#39;urlDecode\u0026#39;, // Name used to call the function from SQL [\u0026#39;title\u0026#39;, \u0026#39;num_requests\u0026#39;], // Input column names // JSON representation of the output schema [{name: \u0026#39;title\u0026#39;, type: \u0026#39;string\u0026#39;},{name: \u0026#39;requests\u0026#39;, type: \u0026#39;integer\u0026#39;}], urlDecode // The function reference ); Ainsi, étant donné que les types de données disponibles sont moins nombreux et qu\u0026rsquo;il existe certaines limitations, comme le fait de ne pas pouvoir créer une logique d\u0026rsquo;entreprise partageable comme le fait la catégorie SQL standard, la catégorie SQL patrimoniale n\u0026rsquo;est pas une option viable.\nValidation du type de données cible lors de l\u0026rsquo;insertion.\nPour avoir une meilleure compréhension des types de données, regardons un peu de code.\nNous allons nous attaquer à l\u0026rsquo;instruction insert dans la catégorie SQL standard puisque c\u0026rsquo;est la catégorie suggérée par la documentation, en nous concentrant sur le type de données STRUCT. Ce type de données peut être un défi et est très courant lors de l\u0026rsquo;ingestion de données provenant de charges utiles d\u0026rsquo;API REST.\nDe plus, je pense que vous pourriez vous lasser des manipulations avec les seuls Integers et Strings. La commande suivante insère des données dans la table DetailedInventory sous le schéma dataset.\nL\u0026rsquo;instruction SQL suivante, écrite en utilisant le langage SQL standard, va insérer des valeurs dans la table mentionnée avec certains types STRUCT.\nInstruction d\u0026rsquo;insertion\nINSERT dataset.DetailedInventory VALUES (\u0026#39;top load washer\u0026#39;, 10, FALSE, [(CURRENT_DATE, \u0026#34;comment1\u0026#34;)], (\u0026#34;white\u0026#34;,\u0026#34;1 year\u0026#34;,(30,40,28))), (\u0026#39;front load washer\u0026#39;, 20, FALSE, [(CURRENT_DATE, \u0026#34;comment1\u0026#34;)], (\u0026#34;beige\u0026#34;,\u0026#34;1 year\u0026#34;,(35,45,30))); Aussi simple que démontré, il insère sans aucune complexité (vous pouvez voir à quoi ressemblent les enregistrements insérés ci-dessous).\nRésultats de l\u0026rsquo;insertion Lorsque vous interagissez avec vos données, vous devez être conscient de manipuler correctement chaque type de données.\nUne erreur courante consiste à comparer les formats de données de l\u0026rsquo;heure et de l\u0026rsquo;horodatage sans prendre les précautions nécessaires. Bien que les deux types de données puissent se ressembler beaucoup, cette erreur peut entraîner des ensembles de données inexacts.\nVérifiez également que la fonction que vous utilisez appartient à la bonne catégorie SQL Bigquery***.\nUn bon exemple est la fonction cast sous legacy SQL et sa référence sous standard SQL, donc connaissez votre terrain avant de faire des changements à votre code.\nConclusion\nComme nous l\u0026rsquo;avons vu, le type de SQL utilisé peut bloquer l\u0026rsquo;utilisation de certains types de données.\nEn raison des différentes manières de traiter les données, il existe une différence significative entre legacy SQL et standard SQL, ce qui rend les choses bien plus complexes qu\u0026rsquo;un simple \u0026ldquo;indice\u0026rdquo; au début du SQL.\nL\u0026rsquo;utilisation du bon type de données peut vous aider à contrôler vos données à un stade précoce. Cela pourrait signifier l\u0026rsquo;ajout d\u0026rsquo;une réaction automatisée aux données insuffisamment formatées, ce qui épargnerait à votre équipe de support de production quelques investigations. En effet, la même réaction pourrait prendre des heures pour identifier le problème de fond, sans compter le temps nécessaire pour le résoudre une fois identifié.\nParfois, vous pouvez même avoir besoin d\u0026rsquo;appliquer certaines règles prédéfinies pour traiter vos données en fonction des problèmes de traitement appris.\nLe SQL standard doit être préféré à l\u0026rsquo;utilisation du SQL hérité (Legacy SQL).\nCe dernier ne dispose pas d\u0026rsquo;astuces sympas telles que les fonctions de fenêtrage ou un meilleur support lexical lors de la création de vos instructions SQL (bien meilleures que la simple notation par points du SQL hérité).\nCes informations sont précieuses lorsqu\u0026rsquo;il s\u0026rsquo;agit d\u0026rsquo;analyser les raisons pour lesquelles vos données présentent toujours des goulots d\u0026rsquo;étranglement lors de leur ingestion.\nSi vous avez besoin d\u0026rsquo;aide pour transformer vos données, vous pouvez nous contacter pour toute question et même découvrir de nouvelles informations dont vous n\u0026rsquo;étiez pas conscient grâce à une meilleure compréhension de vos données.\nÀ+!\n","permalink":"https://www.dawrlog.com/fr/articles/10/","summary":"Bonjour à tous, aujourd\u0026rsquo;hui je voudrais vous parler de Bigquery, le service géré par Google Cloud pour les entrepôts de données. Et j\u0026rsquo;aimerais explorer ses types de données.\nGoogle Cloud Platform est depuis longtemps le fournisseur de clouds de prédilection pour l\u0026rsquo;analyse Web, l\u0026rsquo;impressionnant BigQuery étant l\u0026rsquo;option de traitement massivement parallèle (alias MPP) de Google. Les bases de données MPP sont très courantes dans les écosystèmes Hadoop.\nBigQuery est très convivial pour ses utilisateurs (convenons que BigQuery est aussi agréable que sa connaissance de SQL), ce qui favorise l\u0026rsquo;arrivée de nouveaux utilisateurs.","title":"Les types de données BigQuery examinés et expliqués"},{"content":"Bonjour à tous, aujourd\u0026rsquo;hui nous allons vérifier comment valoriser correctement votre initiative DataOps.\nLes améliorations apportées à l\u0026rsquo;ingestion des données ont rendu évidente la quantité de données perdues lors de la génération d\u0026rsquo;insights. Cependant, en l\u0026rsquo;absence de méthodologies telles que The DataOps Manifesto, certaines entreprises s\u0026rsquo;efforcent encore de combiner les pipelines de données des anciennes bases de données, telles qu\u0026rsquo;une base de données ADABAS, avec les nouvelles bases de données, telles que MongoDB ou Neo4j.\nEt il est toujours bon de rappeler que ces anciens systèmes ne sont pas faciles à abandonner. La plupart d\u0026rsquo;entre eux sont responsables du traitement de base (comme les transactions bancaires en Cobol, par exemple). En tant que tels, il n\u0026rsquo;est pas intéressant de les changer, car c\u0026rsquo;est trop problématique.\nCela dit, je pense que l\u0026rsquo;intégration des nouvelles applications et des bases de données existantes est l\u0026rsquo;un des plus grands défis pour les entreprises axées sur les données. Cependant, les méthodologies et les cadres d\u0026rsquo;ingestion de données ont connu des avancées significatives au cours des dernières années. Pourquoi est-il si essentiel de disposer de données précises ? Avec des pipelines d\u0026rsquo;ingestion de données optimisés, vous pouvez améliorer votre prise de décision. Dans le même temps, la mise à jour de vos anciens systèmes centrés sur le sujet en applications modulaires semble prometteuse, non ?\nLa bonne nouvelle, c\u0026rsquo;est qu\u0026rsquo;il ne s\u0026rsquo;agit pas d\u0026rsquo;un rêve irréalisable ! Grâce aux progrès de la puissance informatique, il est désormais possible de faire beaucoup plus avec très peu. Nous pouvons maintenant déconstruire les applications gonflées en versions plus fines.\nEn utilisant des concepts qui n\u0026rsquo;étaient possibles qu\u0026rsquo;en théorie, nous pouvons maintenant ingérer des données provenant d\u0026rsquo;endroits inhabituels. Par exemple, nous pouvons avoir un pipeline automatisé pour ingérer des données à partir de documents numérisés grâce à la reconnaissance optique de caractères, ou OCR. C\u0026rsquo;est génial, non ?\nDans ce billet, je veux vous aider à comprendre comment votre organisation pourra bénéficier d\u0026rsquo;un environnement DataOps mature. Tout d\u0026rsquo;abord, je présenterai ce qu\u0026rsquo;est DataOps et pourquoi il ne s\u0026rsquo;agit pas seulement de DevOps pour les données. Ensuite, j\u0026rsquo;expliquerai certains avantages importants qui découlent de la cartographie des flux de valeur pour votre propre pratique DataOps. Enfin, je présenterai quelques considérations relatives à l\u0026rsquo;application du cadre DataOps à votre équipe. Qu\u0026rsquo;est-ce que DataOps ? Il est difficile de fournir à vos utilisateurs les mesures les plus précises pour les tableaux de bord existants avec plusieurs fournisseurs de données. La situation s\u0026rsquo;aggrave lorsque votre base de données est si lourdement grevée de dettes techniques que la réalisation d\u0026rsquo;une tâche aussi simple que la création d\u0026rsquo;un indicateur clé de processus, ou KPI, est un véritable cauchemar.\nEt c\u0026rsquo;est là que DataOps vient à la rescousse ! Il permet à vos utilisateurs de consommer vos données plus rapidement et dans un environnement automatisé intégré à la version la plus récente de vos fournisseurs de données. Pas mal, non ?\nJ\u0026rsquo;aime à rappeler que ce n\u0026rsquo;est pas quelque chose de nouveau, car le système d\u0026rsquo;aide à la décision (SAD) consiste en un environnement automatisé pour vos pipelines analytiques. Et je crois que les DSS ont toujours joué un rôle essentiel dans toute entreprise. Les avantages pour les parties prenantes comprennent une compréhension complète du processus de votre chaîne d\u0026rsquo;approvisionnement ou la possibilité de savoir plus rapidement quel département ou produit est le plus rentable, pour n\u0026rsquo;en citer que quelques-uns.\nLes fournisseurs de données chargent ces systèmes, qui peuvent être n\u0026rsquo;importe quelle source d\u0026rsquo;information pour vos besoins. Voici quelques exemples d\u0026rsquo;informations fournies par les fournisseurs de données\nDonnées sur les ressources humaines Données de campagnes d\u0026rsquo;agences Dispositifs IoT de la chaîne d\u0026rsquo;approvisionnement Systèmes de facturation Journaux de serveur et de trafic DataOps vs systèmes d\u0026rsquo;aide à la décision. Vous voyez souvent des pipelines de données orchestrés provenant de différents fournisseurs de données dans les systèmes de gestion des données existants. Ainsi, chaque fournisseur de données charge ses données dans cette base de données centralisée. Mais ces flux de données indépendants entraînent des données incohérentes, ce qui rend difficile la confiance dans les résultats présentés par le DSS.\nCe que DataOps facilite, c\u0026rsquo;est de meilleurs résultats avec un DSS optimisé ! Grâce à l\u0026rsquo;approche agile, DataOps renforce une approche plus centrée sur le client par rapport au DSS en raison de son architecture modulaire. En outre, DataOps allège les contraintes d\u0026rsquo;évolutivité, d\u0026rsquo;automatisation et de sécurité grâce à la réutilisation de composants utilisés par d\u0026rsquo;autres pipelines de données.\nCes composants peuvent aller d\u0026rsquo;une simple connectivité de base de données à une règle de gestion utilisée par le département des finances et dont les ressources humaines pourraient bénéficier. Tout cela grâce à des référentiels centralisés, standardisés et réutilisables. Les points d\u0026rsquo;intersection et de divergence entre DataOps et DevOps\nAlors que DataOps ressemble à DevOps pour les données, j\u0026rsquo;aime à rappeler que les objectifs de DevOps et DataOps sont différents, bien que les méthodologies partagent les mêmes principes.\nDevOps se concentre sur l\u0026rsquo;assouplissement du développement pour inclure les opérations. En revanche, l\u0026rsquo;objectif de DataOps est de rendre le développement analytique plus fonctionnel. DataOps utilise le contrôle statistique des processus, l\u0026rsquo;approche mathématique utilisée dans la production allégée, les disciplines DevOps et les meilleures pratiques du Manifeste Agile.\nAvec ces stratégies en place, vous pouvez découpler votre environnement ; DataOps facilite le détachement de votre logique métier et des contraintes techniques de vos pipelines de données. Ainsi, vous êtes plus confiant dans vos données tout en permettant à vos équipes de mieux réagir aux tendances des données.\nVous bénéficierez de l\u0026rsquo;efficacité et des résultats plus rapidement lors de la conception des composants de votre pipeline de données. Et vos équipes se concentreront davantage sur la création de valeur plutôt que d\u0026rsquo;être liées par des décisions techniques prises dans le passé.\nPar ailleurs, j\u0026rsquo;aime toujours évoquer les gains de sécurité résultant d\u0026rsquo;un déploiement correct des DataOps : des systèmes robustes et sécurisés, moins sujets aux violations et aux fuites ! Avantages à considérer lors de la cartographie de vos flux de valeur\nMaintenant que nous savons ce qu\u0026rsquo;est DataOps, je veux présenter les avantages d\u0026rsquo;une cartographie correcte de vos flux de valeurs.\nJe me concentre ici sur les avantages pour vos pipelines de données, bien qu\u0026rsquo;ils puissent également s\u0026rsquo;appliquer à vos déploiements d\u0026rsquo;applications. Du point de vue de l\u0026rsquo;entreprise, la principale valeur ajoutée par l\u0026rsquo;adoption de DataOps est la possibilité d\u0026rsquo;explorer rapidement de nouvelles sources de données.\nVoici les avantages de la valeur ajoutée pour le client lors de la cartographie de vos flux de valeur. Un meilleur contrôle grâce au découplage technique.\nComme je l\u0026rsquo;ai déjà mentionné, les fournisseurs de données sont toutes les applications sources contenant des données pertinentes pour votre analyse. En d\u0026rsquo;autres termes, ce sont les points d\u0026rsquo;entrée des données qui alimentent votre pipeline de données.\nCes applications produisent des données à l\u0026rsquo;état brut. Et comme leur nom l\u0026rsquo;indique, ces données doivent être laissées aussi intactes que possible. C\u0026rsquo;est utile en cas de retraitement ou d\u0026rsquo;analyse de la lignée des données.\nCes données nécessitent des étapes de traitement supplémentaires afin de les débarrasser des bruits inutiles, car leur forme originale peut ne pas correspondre à vos besoins. Cependant, à partir de cette sortie, vous pouvez extraire les métriques nécessaires pour couvrir les besoins de vos utilisateurs.\nJe souhaite également évoquer l\u0026rsquo;une des valeurs du découplage conscient : son contrôle automatisé robuste du pipeline de données de chaque composant. Cette orchestration apporte des mesures de qualité supplémentaires, permettant d\u0026rsquo;augmenter la productivité puisque vos utilisateurs n\u0026rsquo;auront pas à effectuer des tâches répétitives. Exploration rapide des fournisseurs de données existants et nouveaux.\nSur les systèmes patrimoniaux, le développement de nouveaux pipelines est chaotique, comme mentionné précédemment. L\u0026rsquo;approche DataOps permet également une exploration rapide de vos fournisseurs de données.\nDataOps facilite la création de composants conformes grâce à son déploiement modulaire. Ce que je veux dire par là, c\u0026rsquo;est que vous pouvez réutiliser des composants déjà déployés et testés.\nEn d\u0026rsquo;autres termes, DataOps permet un état d\u0026rsquo;esprit d\u0026rsquo;amélioration continue. Par conséquent, il réduira considérablement vos coûts de développement et augmentera en même temps votre retour sur investissement.\nVotre équipe s\u0026rsquo;occupera de tâches plus difficiles pour apporter de la valeur à l\u0026rsquo;entreprise, et non plus des activités quotidiennes de traitement des données. En conséquence, vous bénéficiez d\u0026rsquo;un gain de productivité instantané grâce au déploiement automatisé de votre application. Gouvernance fiable des données.\nGrâce aux pipelines de données automatisés déployés par DataOps, il devient plus facile de retracer comment vos utilisateurs consomment vos données. Ces informations peuvent vous aider à répondre rapidement à des questions récurrentes.\nVos utilisateurs peuvent voir où se trouve la logique métier en un clin d\u0026rsquo;œil. De plus, la référence entre son nom canonique et son nom d\u0026rsquo;implémentation technique devient facile à assimiler puisque les nouveaux analystes qui se joignent à vos projets en font une source attrayante pour le profilage de vos données.\nPar conséquent, un solide datalog de vos fournisseurs de données est une étape obligatoire à laquelle il faut penser lors de la cartographie de vos flux de valeur, à mon avis. Il devient facile de gérer vos pipelines de données lorsque vous créez un catalogue métier conforme au niveau de l\u0026rsquo;entreprise. Toutes ces informations structurées sur vos fournisseurs de données créent un catalogue de données intuitif.\nCes informations, également appelées métadonnées, fournissent le contexte commercial dans lequel ces données donnent leur valeur. En d\u0026rsquo;autres termes, vos informations deviennent plus précises. Considérations importantes concernant votre propre déploiement de DataOps.\nCe que nous avons vu jusqu\u0026rsquo;à présent montre comment des modules détachés conformes permettent de créer un catalogue de données plus robuste pour votre entreprise. En outre, la cohérence entre vos composants analytiques permet de clarifier les points sur lesquels vous pouvez améliorer votre ingestion de données.\nJ\u0026rsquo;aime toujours rappeler que ces améliorations ne sont pas gratuites. Comme vous réagirez plus rapidement et plus judicieusement, soyez prêt à remodeler certains des processus internes de votre entreprise. N\u0026rsquo;oubliez pas qu\u0026rsquo;il est difficile d\u0026rsquo;apprendre de nouveaux tours à un vieux chien. Attendez-vous donc à une certaine résistance de la part de vos coéquipiers plus expérimentés.\nUn pipeline de données auto-réparateur évolue horizontalement ou verticalement selon les besoins. Ainsi, vous pouvez ajouter des unités supplémentaires pour la puissance de traitement (ce que l\u0026rsquo;on appelle la mise à l\u0026rsquo;échelle horizontale) ou améliorer vos clusters (ce que l\u0026rsquo;on appelle la mise à l\u0026rsquo;échelle verticale) lorsque vos ensembles commencent à avoir des problèmes de goulot d\u0026rsquo;étranglement lors du traitement de vos données ; Remember the rule of thumb :\nIl est plus facile d\u0026rsquo;enrichir vos sorties lorsque vous êtes pleinement conscient de l\u0026rsquo;étendue de vos données. Ainsi, en plus de ses composants modulaires, DataOps permet d\u0026rsquo;effectuer des actions telles que le masquage et l\u0026rsquo;obscurcissement de vos données de manière plus fluide dans les premières étapes de leur traitement.\nAvec DataOps, vous construisez un pipe de données fiable capable de réagir aux nouveaux concepts et technologies à la même vitesse que l\u0026rsquo;évolution de votre entreprise. Réflexions finales.\nDans ce billet, j\u0026rsquo;ai donné un aperçu de ce que vous gagnerez en déployant correctement DataOps. Le résultat est un mélange de valeur ajoutée commerciale et technique, car vous disposerez de pipelines de données orchestrés minces et robustes.\nJ\u0026rsquo;ai commencé par présenter ce qu\u0026rsquo;est DataOps et la valeur commerciale qu\u0026rsquo;elle apporte. Ensuite, j\u0026rsquo;ai expliqué où elle se recoupe et où ses objectifs diffèrent des méthodologies agiles et DevOps.\nNous avons également jeté un rapide coup d\u0026rsquo;œil à ce que je crois être les avantages à court terme du déploiement correct d\u0026rsquo;une mise en œuvre DataOps mature et comment les déploiements automatisés peuvent ajouter de la valeur technique.\nEnfin, nous avons vu certains défis que votre équipe peut rencontrer lorsque vous adoptez DataOps. Par exemple, votre unité peut être résistante à adopter de nouvelles technologies et méthodologies. Cependant, nous avons également vu les avantages d\u0026rsquo;un déploiement correct en tant que catalogue de données concis.\nRappelez-vous simplement que vous devez mettre en œuvre l\u0026rsquo;intégralité des exigences de DataOps. Ne vous attendez donc pas à une mise en œuvre fiable de DataOps avec des déploiements partiels des disciplines agiles ou DevOps.\nSi vous avez besoin d\u0026rsquo;aide pour transformer vos données, vous pouvez nous contacter pour toute question et même découvrir de nouvelles informations dont vous n\u0026rsquo;étiez pas conscient grâce à une meilleure compréhension de vos données.\nÀ+!\n","permalink":"https://www.dawrlog.com/fr/articles/12/","summary":"Bonjour à tous, aujourd\u0026rsquo;hui nous allons vérifier comment valoriser correctement votre initiative DataOps.\nLes améliorations apportées à l\u0026rsquo;ingestion des données ont rendu évidente la quantité de données perdues lors de la génération d\u0026rsquo;insights. Cependant, en l\u0026rsquo;absence de méthodologies telles que The DataOps Manifesto, certaines entreprises s\u0026rsquo;efforcent encore de combiner les pipelines de données des anciennes bases de données, telles qu\u0026rsquo;une base de données ADABAS, avec les nouvelles bases de données, telles que MongoDB ou Neo4j.","title":"Comment bien evaluer vos Value Stream pour votre pratique DataOps?"},{"content":"Bonjour à tous, aujourd\u0026rsquo;hui je veux revenir sur ce que sont les phases du cycle de vie du provisionnement des identités et des accès.\nAvec autant de données générées chaque seconde, il est logique que les fuites de données soient l\u0026rsquo;un des plus grands cauchemars de toute entreprise axée sur les données. De nos jours, nous apprenons rapidement que des politiques de sécurité faibles peuvent coûter cher. Cet article présente l\u0026rsquo;une des techniques permettant de maintenir la sécurité de vos données : le cycle de vie du provisionnement des identités et des accès.\nPour vous aider à comprendre ce cycle de vie, je présenterai les étapes d\u0026rsquo;action de ses deux catégories d\u0026rsquo;états : le provisionnement et le dé-provisionnement. Je commencerai par identifier les tâches et expliquer ce que signifie \u0026ldquo;provisionnement des identités et des accès\u0026rdquo;. Puis j\u0026rsquo;expliquerai les actions à entreprendre pour atteindre correctement ces états, depuis la compréhension des besoins et la demande d\u0026rsquo;accès pour vos identités (l\u0026rsquo;état provisioning) jusqu\u0026rsquo;à la suppression des identités inutiles (l\u0026rsquo;état de-provisioning).\nJe vais ensuite vous présenter comment mettre en œuvre le cycle de vie du provisionnement d\u0026rsquo;accès. Vous aurez un aperçu des modèles de provisionnement d\u0026rsquo;accès et de la manière dont les différents modèles de provisionnement déploient les concepts que j\u0026rsquo;ai présentés dans le paragraphe précédent. Je décrirai le fonctionnement des différents types de modèles de provisionnement d\u0026rsquo;accès. Commençons par les bases. Qu\u0026rsquo;est-ce que le cycle de vie du provisionnement des identités et des accès ?\nDans chaque entreprise, il existe de multiples systèmes contenant différents types de données. Certaines données peuvent être dans le domaine public. D\u0026rsquo;autres données nécessitent des règles de confidentialité différentes. En effet, leur exposition pourrait causer des problèmes, comme le fait que vos concurrents soient informés de vos prochaines étapes.\nIl est essentiel d\u0026rsquo;intégrer correctement tout nouveau membre de l\u0026rsquo;équipe en lui accordant les privilèges nécessaires à l\u0026rsquo;exécution de ses tâches. Vous pouvez faciliter ce processus d\u0026rsquo;intégration en fournissant un identifiant unique lié à ce nouveau membre.\nIl peut s\u0026rsquo;agir d\u0026rsquo;un identifiant physique, tel qu\u0026rsquo;un badge d\u0026rsquo;identification ou un dispositif d\u0026rsquo;authentification à double facteur. Il peut aussi s\u0026rsquo;agir d\u0026rsquo;un identifiant logique. Ces identifiants, tels que l\u0026rsquo;UID ou l\u0026rsquo;identité unique, sont utilisés pour l\u0026rsquo;accès physique aux installations de l\u0026rsquo;entreprise.\nLorsqu\u0026rsquo;une alerte est déclenchée, les confirmations de suppression des accès qui ne sont plus nécessaires sont révoquées. L\u0026rsquo;équipe administrative ne doit jamais confirmer les demandes d\u0026rsquo;accès. Au contraire, d\u0026rsquo;autres pairs doivent valider ces demandes. Les demandes d\u0026rsquo;accès doivent toujours provenir de la demande de l\u0026rsquo;utilisateur ou du gestionnaire après toutes les validations nécessaires. L\u0026rsquo;équipe d\u0026rsquo;administration ne doit jamais mentionner ou lancer une demande d\u0026rsquo;accès.\nDans la plupart des entreprises, c\u0026rsquo;est au département des ressources humaines de faire les demandes. Par conséquent, ces demandes interviennent généralement lors du processus d\u0026rsquo;embauche ou lorsqu\u0026rsquo;une personne prend un nouveau rôle. Ces demandes relèvent de l\u0026rsquo;une des deux catégories suivantes :\nTâches de provisionnement d\u0026rsquo;utilisateur: Intégration d\u0026rsquo;un nouveau collègue ou ajout de nouveaux droits aux privilèges d\u0026rsquo;accès actuels. Tâches de dé-provisionnement d\u0026rsquo;utilisateur: Détachément de certaines responsabilités en supprimant des droits inutiles aux privilèges d\u0026rsquo;accès actuels. Examinons ces tâches plus en détail. Provisionnement et dé-provisionnement\nLe provisionnement des accès des utilisateurs consiste à gérer le contrôle de l\u0026rsquo;accès de vos utilisateurs aux systèmes nécessaires à leur travail. Ces privilèges d\u0026rsquo;accès sont obtenus soit en accordant des accès en raison de nouvelles responsabilités, soit en supprimant des accès pour la même raison.\nCeci étant dit, la validation constante des accès actuels par identifiant est cruciale. Ces validations permettent de réduire la surface d\u0026rsquo;attaque des sources non fiables. C\u0026rsquo;est une étape critique. Il s\u0026rsquo;agit donc d\u0026rsquo;une bonne pratique à revoir périodiquement lorsque l\u0026rsquo;utilisateur acquiert de nouvelles responsabilités et a besoin de plus d\u0026rsquo;accès.\nLe dé-provisionnement consiste à gérer toutes les étapes nécessaires pour révoquer un accès qui n\u0026rsquo;est pas nécessaire. Il s\u0026rsquo;agit également d\u0026rsquo;alerter tous les services concernés des changements d\u0026rsquo;accès. Les modèles de provisionnement d\u0026rsquo;accès sont les composants qui orchestrent toutes ces demandes de manière transparente. Grâce à ces modèles, vous pouvez étendre les accès existants par de nouveaux accès. Ou vous pouvez mettre hors service un utilisateur particulier si nécessaire.\nTrès bien ! Examinons de plus près les différents modèles ; commençons par RBAC. Modèles de provisionnement d\u0026rsquo;accès : Contrôle d\u0026rsquo;accès basé sur les rôles (RBAC).\nPour comprendre ce que signifie le contrôle d\u0026rsquo;accès basé sur le rôle (RBAC), je veux présenter ce que le rôle signifie réellement pour la gestion des identités et des accès. Pour ce type d\u0026rsquo;accès, les utilisateurs du provisionnement sont étiquetés par le département ou le rôle dont ils font partie.\nIl est bon de préciser que ces étiquettes sont gérées par le provisionnement d\u0026rsquo;accès basé sur les rôles. Toutes ces actions sont liées à l\u0026rsquo;octroi et/ou à la révocation de l\u0026rsquo;accès à l\u0026rsquo;ensemble du groupe auquel appartient l\u0026rsquo;identité. Et cela supprime la nécessité d\u0026rsquo;appliquer des règles pour les identités individuelles.\nEn d\u0026rsquo;autres termes, la politique consiste à contrôler l\u0026rsquo;accès non pas en utilisant un identifiant unique mais en regroupant les mêmes fonctions d\u0026rsquo;utilisateur. Cette approche est plus rapide et plus facile que de faire des jugements individuels basés sur l\u0026rsquo;identité et les demandes.\nImaginons le cas d\u0026rsquo;utilisation suivant : le provisionnement d\u0026rsquo;accès basé sur les rôles d\u0026rsquo;un développeur ou d\u0026rsquo;un gestionnaire.\nLe développeur a besoin d\u0026rsquo;un accès complet à l\u0026rsquo;environnement de développement et d\u0026rsquo;un accès limité aux outils de gestion pour suivre son travail. Les managers, quant à eux, ont besoin de l\u0026rsquo;inverse. Comme les managers ne seront pas aussi pointus sur le plan technique, ils n\u0026rsquo;auront peut-être pas besoin d\u0026rsquo;un accès complet aux environnements de développement. Cependant, les gestionnaires auront besoin d\u0026rsquo;un accès complet aux outils de suivi du travail pour gérer les besoins des nouveaux clients.\nPour un déploiement réussi, toutes les configurations nécessaires par groupe doivent avoir des validations strictes basées sur la portée du groupe géré. Il ne sert donc à rien d\u0026rsquo;ajouter un accès programmatique sur les pipelines de développement à votre équipe financière, par exemple. Ils n\u0026rsquo;ont pas besoin de ce type d\u0026rsquo;accès. Modèles de provisionnement d\u0026rsquo;accès : Approvisionnement d\u0026rsquo;accès basé sur les demandes (RBAP).\nPour commencer, cela diffère du RBAP en termes de fourniture d\u0026rsquo;accès basée sur les demandes. Dans certains systèmes, les demandes directes doivent être adressées au propriétaire du système. Il s\u0026rsquo;agit par exemple de demander l\u0026rsquo;autorisation d\u0026rsquo;accéder à des systèmes réglementés. Nous appelons cela le contrôle d\u0026rsquo;accès discrétionnaire (ou DAC).\nD\u0026rsquo;autre part, lorsque vous utilisez le contrôle d\u0026rsquo;accès obligatoire (ou MAC), les accès aux ressources sont étiquetés en fonction de la classification des informations contenues dans ces systèmes. Cette catégorisation suivra la manière dont votre entreprise classifie vos données en interne. Je vous suggère de jeter un coup d\u0026rsquo;œil ici si vous avez besoin de plus de détails sur les procédures de traitement de la classification des informations.\nUn bon cas d\u0026rsquo;utilisation d\u0026rsquo;un MAC est lorsqu\u0026rsquo;il y a des cas plus restreints, comme le traitement des numéros d\u0026rsquo;assurance sociale. Il est bon de se rappeler que toutes les informations complémentaires relatives aux procédures de sécurité pour traiter ce type d\u0026rsquo;informations sont également fournies avec ces approbations.\nEn revanche, le CED ne requiert que la discrétion du propriétaire du système. Ainsi, si vous parvenez à convaincre le propriétaire de ce système, cette personne vous permettra d\u0026rsquo;y accéder. (Être un bon ami de l\u0026rsquo;administrateur aide aussi !) Approvisionnement hybride\nIl peut arriver que vous ayez besoin d\u0026rsquo;un accès temporaire à un système auquel votre rôle actuel ne vous donne pas accès. Dans ce cas, le provisionnement hybride sera pratique. Il vous permet d\u0026rsquo;avoir le processus d\u0026rsquo;accueil pour le provisionnement d\u0026rsquo;accès normal. Il permet également de répondre à toute demande spéciale nécessitant une longue série d\u0026rsquo;approbations pour le provisionnement d\u0026rsquo;accès ad hoc. Parfois, il peut être nécessaire d\u0026rsquo;étendre rapidement le champ d\u0026rsquo;accès existant pour des tâches ad hoc.\nEnsuite, il est essentiel de définir la durée de vie de cet accès et de fixer une date d\u0026rsquo;expiration pour chaque type d\u0026rsquo;accès. Et cela implique des étapes de validation supplémentaires de la part de niveaux de gestion supplémentaires. Le modèle de provisionnement hybride peut vous aider à le faire. Vous devrez régler les détails au fur et à mesure que vous définirez le plan de sécurité de votre entreprise. Je voudrais énumérer ci-dessous quelques règles empiriques à prendre en compte lorsque vous définissez les privilèges de sécurité.\nPréférez toujours des politiques de provisionnement d\u0026rsquo;accès restrictives. Mettez en place un audit pour renforcer ces politiques de fourniture d\u0026rsquo;accès restrictives. Révoquez tous les accès non utilisés en fonction des résultats de l\u0026rsquo;audit. Révaluez les procédures d\u0026rsquo;onboarding et de offboarding de votre entreprise. (Il est temps d\u0026rsquo;en mettre en place si vous n\u0026rsquo;en avez aucune). Conclusion Aujourd\u0026rsquo;hui, nous avons commencé par définir ce que signifie l\u0026rsquo;identité et ce que sont les accès de provisionnement et de dé-provisionnement. Et j\u0026rsquo;ai poursuivi en présentant les étapes nécessaires pour maintenir le cycle de vie du provisionnement des accès à l\u0026rsquo;aide des modèles de provisionnement. D\u0026rsquo;autre part, il est bon d\u0026rsquo;avoir des outils tiers comme Okta ou Keycloak pour faciliter le provisionnement des accès dans votre écosystème. Okta gère le déploiement manuel de manière plus automatisée, en provisionnant et en dé-provisionnant l\u0026rsquo;accès de manière plus fluide. Pour cela, je vous suggère de consulter nos précédents articles de blog pour savoir comment le configurer correctement sur votre environnement AWS et pour obtenir plus de détails sur les rôles AWS avec la gestion d\u0026rsquo;Okta.\nSi vous avez besoin d\u0026rsquo;aide pour transformer vos données, vous pouvez nous contacter pour toute question et même découvrir de nouvelles informations dont vous n\u0026rsquo;étiez pas conscient grâce à une meilleure compréhension de vos données.\nÀ+!\n","permalink":"https://www.dawrlog.com/fr/articles/14/","summary":"Bonjour à tous, aujourd\u0026rsquo;hui je veux revenir sur ce que sont les phases du cycle de vie du provisionnement des identités et des accès.\nAvec autant de données générées chaque seconde, il est logique que les fuites de données soient l\u0026rsquo;un des plus grands cauchemars de toute entreprise axée sur les données. De nos jours, nous apprenons rapidement que des politiques de sécurité faibles peuvent coûter cher. Cet article présente l\u0026rsquo;une des techniques permettant de maintenir la sécurité de vos données : le cycle de vie du provisionnement des identités et des accès.","title":"Qu'est-ce que ce le cycle de vie des identitées et provisionement d'access?"},{"content":"Salut tout le monde, j\u0026rsquo;espère que tout va bien.\nL\u0026rsquo;un des principaux défis de la gestion des données provient de la massive quantité de données que votre organisation génère très probablement en permanence. Avoir la capacité de réagir rapidement et avec précision devient un élément différentiel parmi les concurrents.\nDe plus en plus d\u0026rsquo;entreprises prennent conscience de la nécessité de gérer les données avec sagesse.\nElles cherchent des moyens de tirer le meilleur parti de leurs informations. Et c\u0026rsquo;est probablement là qu\u0026rsquo;elles entendent parler pour la première fois des entrepôts de données.\nJe souhaite présenter les entrepôts de données et leurs composants dans cet article.\nPour commencer, vous aurez un aperçu de ce qu\u0026rsquo;est un entrepôt de données. Ensuite, nous passerons en revue certaines considérations que je trouve obligatoires lors de la construction d\u0026rsquo;un entrepôt de données.\nPour conclure, je ferai un rapide récapitulatif, puis je vous ferai part de quelques autres préoccupations et éclaircissements. Les bases de l\u0026rsquo;entreposage de données\nLes bases de données des entrepôts de données (DW en abrégé) constituent un système d\u0026rsquo;aide à la décision. Une fois que l\u0026rsquo;on a compris comment elles peuvent faciliter l\u0026rsquo;analyse des données historiques, on comprend aisément pourquoi on les appelle des entrepôts.\nComme son nom l\u0026rsquo;indique, ce système d\u0026rsquo;aide à la décision facilite le processus décisionnel en utilisant les données disponibles au sein de l\u0026rsquo;entreprise.\nDW est le résultat de la résolution de questions autour de sujets spécifiques.\nSon approche est différente de celle des applications opérationnelles typiques, telles que les systèmes de facturation ou de ressources humaines. Ces derniers se concentrent sur les besoins opérationnels tels que la rémunération et l\u0026rsquo;allocation des ressources, par exemple.\nEt en raison des caractéristiques particulières des systèmes DW, comme le fait d\u0026rsquo;être sensibles au temps et de ne pas disposer d\u0026rsquo;informations détaillées comme les numéros de téléphone, ils ne constituent pas une bonne option pour les rapports opérationnels. Rester en sécurité pendant la mise en œuvre de l\u0026rsquo;entrepôt de données\nJe ne saurais trop insister sur l\u0026rsquo;importance de disposer d\u0026rsquo;une politique de sécurité solide pour chaque phase du déploiement de votre entrepôt de données.\nEn termes simples, plus vous autorisez vos utilisateurs à se rapprocher des systèmes sources afin qu\u0026rsquo;ils puissent explorer vos données, plus ils pourraient potentiellement accéder à des données sans les autorisations correctes. Et personne ne souhaite une fuite de données.\nEn plus de vos politiques existantes, votre entrepôt de données doit répondre à des demandes différentes.\nLes bases de données DW peuvent résoudre des tâches critiques d\u0026rsquo;aide à la décision ; cependant, dans certains cas, le seul endroit où ces informations existent est au sein des données sensibles. Dans le meilleur style \u0026ldquo;pour vos yeux seulement\u0026rdquo;, il est parfois bon de restreindre ceux qui n\u0026rsquo;ont pas vraiment besoin d\u0026rsquo;accès. Le plaisir de l\u0026rsquo;entrepôt de données commence\nMaintenant que nous avons couvert nos bases en termes de ce qu\u0026rsquo;est un entrepôt de données, il est temps pour nous de définir qui seront ses utilisateurs et quels sujets il couvrira.\nC\u0026rsquo;est toujours un bon point de départ d\u0026rsquo;informer les parties prenantes du département dès les premières étapes du développement de votre entrepôt de données. Après tout, ils sauront qui est responsable ou intéressé par les indicateurs clés de performance, ou KPI en abrégé.\nN\u0026rsquo;oubliez pas qu\u0026rsquo;ils peuvent vous aider à résoudre des problèmes tels que \u0026ldquo;Où puis-je trouver des données qui pourraient m\u0026rsquo;aider ? \u0026ldquo; ou \u0026ldquo;Qu\u0026rsquo;allons-nous utiliser pour les analyser ? \u0026ldquo;.\nQuelle technique de construction de DW devez-vous utiliser ? Il est également important de choisir la meilleure technique à suivre pour construire votre DW.\nLa plus connue est l\u0026rsquo;approche Kimball. (Je décrirai ses phases un peu plus loin dans cet article.) Ralph Kimball est considéré comme le père du data warehousing. La méthode Inmon est l\u0026rsquo;approche la plus ancienne, mais c\u0026rsquo;est un choix solide. Je tiens également à mentionner la modélisation de la voûte de données et CDP/DMP si vous êtes impliqué dans AdTech. J\u0026rsquo;aime toujours préciser que même si certaines caractéristiques d\u0026rsquo;une plateforme de données clients (CDP) la font ressembler à un entrepôt de données, ce n\u0026rsquo;est pas le cas.\nUne CDP peut toutefois servir de lac de données pour votre lac de données. Pour en savoir plus, consultez le guide CDP.\nDonner des couches à votre entrepôt de données\nJusqu\u0026rsquo;à présent, nous avons clarifié qui est votre équipe et quels sont vos besoins. Maintenant, voyons les couches (ou phases) sur la façon dont vos données circulent de vos applications vers votre entrepôt de données.\nVotre couche OLTP\nÀ ce stade, vous devez commencer par définir ce que vous allez utiliser comme source de données pour créer vos tableaux de bord.\nL\u0026rsquo;objectif est de construire votre couche de traitement des transactions en ligne, également appelée zone de transit, zone d\u0026rsquo;atterrissage ou par son acronyme OLTP. C\u0026rsquo;est là que vous définissez le plan de rétention de ces données.\nGardez à l\u0026rsquo;esprit que cette étape ne doit pas comporter de transformation de données, car les données restent disponibles si un rechargement est nécessaire lors des phases suivantes.\nCette zone vous permet de réaliser des processus simultanés pendant que les applications sources sont en cours d\u0026rsquo;exécution. C\u0026rsquo;est une source de données pour la couche suivante, la zone ODS.\nVotre couche ODS\nOK, disons que vous avez configuré vos sources de données et pompé des données dans votre zone de transit. Super ! Mais vous aurez besoin de donner un certain sens à vos systèmes disparates.\nPensez-y : Comment pouvez-vous certifier que les tables contiennent le même type d\u0026rsquo;informations ? Devrez-vous disposer d\u0026rsquo;un format conforme pour vos données ?\nC\u0026rsquo;est ici que la couche magasin de données opérationnelles (ou ODS) est utile.\nL\u0026rsquo;ODS met en ordre toutes vos sources de données dans le bon format. Il supprime également les données dupliquées.\nEn d\u0026rsquo;autres termes, il vous aide à développer la base de données opérationnelle la meilleure et la plus unifiée possible.\nModèles de cubes et granularité des données\nUne fois que vos sources sont occupées à créer des données utiles pour vos analystes, vous êtes prêt à créer vos modèles de cubes. C\u0026rsquo;est là que vous utiliserez des techniques telles que les requêtes iceberg et les requêtes fenêtre.\nÀ ce stade, vous avez largement dépassé la nature opérationnelle de vos données.\nCes techniques transforment vos données, vous permettant, à vous et aux membres de votre équipe, d\u0026rsquo;atteindre différents niveaux d\u0026rsquo;analyse. Ce que nous appelons la granularité des données vous aide à obtenir des informations plus spécifiques, plus ciblées et plus utiles à partir du même ensemble de données.\nImaginons que vous souhaitiez rechercher l\u0026rsquo;adresse d\u0026rsquo;un détaillant. Cette information est d\u0026rsquo;une haute granularité. En d\u0026rsquo;autres termes, elle a une valeur spécifique à rechercher (l\u0026rsquo;adresse du magasin).\nMais que se passe-t-il si vous voulez savoir combien de magasins se trouvent dans un comté spécifique ?\nDans ce cas, vous aurez des données de faible granularité, et vous devrez compter les adresses pour obtenir ce résultat.\nPour en savoir plus sur la granularité des données, consultez ce lien. Mise en place de l\u0026rsquo;infrastructure, dans le cloud ou sur site\nL\u0026rsquo;idée de la business intelligence vient d\u0026rsquo;une époque plus simple concernant les structures de données et l\u0026rsquo;intérêt d\u0026rsquo;extraire des informations des données.\nLa plupart des sources de données provenaient de formats structurés de bases de données relationnelles ou de formats semi-structurés de services Web (comme le format XML des bons vieux services SOA ou JSON).\nPar le passé, les gens devaient s\u0026rsquo;en remettre à des expressions très régulières pour extraire les données d\u0026rsquo;un seul champ ; il était donc logique d\u0026rsquo;investir dans de gros canons pour stocker vos données, car elles étaient censées augmenter de manière exponentielle avec le temps.\nCe que l\u0026rsquo;on appelle le cloud public a présenté une option attrayante, éliminant la nécessité de ces investissements coûteux.\nLes fournisseurs de cloud, notamment AWS et Azure, acquièrent davantage d\u0026rsquo;évangélistes sur leurs solutions de stockage de données.\nL\u0026rsquo;approche sans serveur est passionnante et permet de gagner du temps. Cependant, la configuration et la gestion de ces nouveaux services pourraient être lourdes pour votre environnement existant.\nC\u0026rsquo;est pourquoi je vous conseille d\u0026rsquo;y aller doucement avec la migration et la reconstruction de vos pipelines existants vers de nouveaux services. Choisissez plutôt des services moins critiques pour commencer, puis passez au cloud. (Certains appellent cela l\u0026rsquo;approche hybride.) Ce que nous avons couvert jusqu\u0026rsquo;à présent sur l\u0026rsquo;entreposage de données\nTrouver la bonne conception pour votre entrepôt de données est un défi en soi.\nMême pour l\u0026rsquo;architecte de données le plus expérimenté, le fait de se concentrer sur des exigences peu claires et de découvrir des exigences plus complexes au cours du développement fait d\u0026rsquo;un tableau de bord simple un rêve lointain.\nPar conséquent, examinons les préoccupations que vous devez avoir à l\u0026rsquo;esprit. Autres approches que Kimball\nDans cet article, je me suis concentré sur l\u0026rsquo;approche Kimball car c\u0026rsquo;est la plus utilisée. Mais que faire si vous êtes intéressé par des techniques différentes ?\nVous pouvez étudier un autre cadre ancien et fiable que nous avons mentionné précédemment - l\u0026rsquo;approche Inmon.\nUne autre approche qui mérite d\u0026rsquo;être découverte est le coffre-fort de données. Il s\u0026rsquo;agit d\u0026rsquo;une excellente option pour les schémas à évolution rapide. C\u0026rsquo;est également ma méthode préférée pour la modélisation des données en continu.\nTrouver les bons outils pour votre entrepôt de données\nIl est parfois difficile de trouver les bons outils pour gérer toutes vos sources de données et leurs entrées de pipeline. En partie parce que les techniques d\u0026rsquo;entrepôt de données sont agnostiques sur le plan technologique par nature. Mais avec une bonne base sur la nature de vos données, vous serez en meilleure position pour comprendre vos données.\nA+ !\n","permalink":"https://www.dawrlog.com/fr/articles/1/","summary":"Salut tout le monde, j\u0026rsquo;espère que tout va bien.\nL\u0026rsquo;un des principaux défis de la gestion des données provient de la massive quantité de données que votre organisation génère très probablement en permanence. Avoir la capacité de réagir rapidement et avec précision devient un élément différentiel parmi les concurrents.\nDe plus en plus d\u0026rsquo;entreprises prennent conscience de la nécessité de gérer les données avec sagesse.\nElles cherchent des moyens de tirer le meilleur parti de leurs informations.","title":"Les défis pour l'architecture d'un Data Warehouse."},{"content":"Bonjour à tous !\nAujourd\u0026rsquo;hui, j\u0026rsquo;aimerais vous parler d\u0026rsquo;un des outils de dataviz qui est de plus en plus adopté : Power BI. Power BI augmente chaque jour son adoption parmi les utilisateurs les plus expérimentés. Grâce à sa convivialité et à la possibilité de se connecter à diverses sources de données, il permet d\u0026rsquo;analyser des sources de données distinctes au sein d\u0026rsquo;une même console.\nDans ce billet, je vais vous montrer comment importer vos données stockées sur PostgreSQL avec une version de bureau de Power BI.\nJe commencerai par expliquer ce qu\u0026rsquo;est Power BI et ce qu\u0026rsquo;il fait. Ensuite, je vous proposerai une option pour créer une instance locale afin d\u0026rsquo;explorer vos données PostgreSQL avec Power BI.\nJe vous suggère de sauter la section \u0026ldquo;Configurer votre environnement local\u0026rdquo; si vous avez déjà PostgreSQL et Power BI en cours d\u0026rsquo;exécution ou si vous êtes familier avec les boîtes Vagrant.\nCependant, je vous encourage à y jeter un œil si vous avez un peu de temps libre. Tirer parti de Vagrant peut être une option précieuse pour créer vos preuves de concept en douceur avec des outils d\u0026rsquo;automatisation.\nUne fois l\u0026rsquo;infrastructure prise en charge, nous allons configurer Power BI pour qu\u0026rsquo;il se connecte à votre instance PostgreSQL existante, explorer quelques données sur votre instance Power BI, et les transformer avant son chargement final, en créant un graphique à partir de vos données nouvellement importées.\nQu\u0026rsquo;est-ce que Power BI ? Power BI est un outil de visualisation de données créé par Microsoft ; il existe différentes versions axées sur d\u0026rsquo;autres cas d\u0026rsquo;utilisation. Vous pouvez essayer PowerBI gratuitement sur Azure, l\u0026rsquo;option Microsoft pour le cloud public, afin d\u0026rsquo;interagir avec vos services existants tels que l\u0026rsquo;entrepôt de données en tant que service Azure Synapse.\nUne autre option consiste à l\u0026rsquo;installer sur vos \u0026ldquo;services sur site\u0026rdquo; en utilisant une passerelle de données sur site.\nAu fur et à mesure que Power BI gagne en maturité, sa popularité augmente naturellement, en grande partie grâce à certaines capacités intéressantes offertes à ses utilisateurs.\nPar exemple, il est facile et rapide de charger vos données structurées comme source dans Power BI. Cela vous permet de comprendre les tendances de vos données et d\u0026rsquo;obtenir les réponses dont vous avez besoin plus rapidement que d\u0026rsquo;habitude, ce qui fait le bonheur des utilisateurs les plus exigeants, car il est possible d\u0026rsquo;explorer davantage de sources de données.\nEt n\u0026rsquo;oubliez pas : de meilleures données donnent de meilleurs résultats!\nCela étant dit, si vous avez encore du mal à consommer vos données correctement, il serait bon d\u0026rsquo;auditer vos pipelines de données. Vous auriez besoin de nettoyer vos données, ce qui consiste à traiter tous les mauvais aspects pour votre analyse. En effet, le format des données depuis leur source peut ne pas être prêt pour l\u0026rsquo;analyse.\nMaintenant que nous avons compris ce qu\u0026rsquo;est Power BI, voyons-la en action. Nous allons nous plonger dans sa connexion à PostgreSQL, l\u0026rsquo;une des options de bases de données relationnelles les plus utilisées.\nConnecter PostgreSQL à Power BI, configurer votre environnement local\nSi vous avez déjà une instance PostgreSQL, passez à la section suivante Créer une connexion à la base de données. Pour ceux qui n\u0026rsquo;ont pas d\u0026rsquo;instance, commençons par en créer une.\nTout d\u0026rsquo;abord, téléchargez PostgreSQL et Power BI (Windows uniquement). Mais attendez, supposez que vous êtes comme moi et que vous n\u0026rsquo;aimez pas avoir une machine Windows ?\nJe vous suggère d\u0026rsquo;utiliser Vagrant.\nVous n\u0026rsquo;avez jamais entendu parler de Vagrant ou ne l\u0026rsquo;avez jamais utilisé ? Pas de problème ! Jetez un œil à cette introduction rapide pour un cours intensif.\nAprès cela, ou si vous êtes déjà familiarisé avec Vagrant, vous êtes prêt à suivre les étapes suivantes :\nPour commencer, vous devez télécharger Vagrant et la machine Windows utilisée comme image de base. Une fois que Vagrant est configuré, vous pouvez télécharger Power BI Desktop de l\u0026rsquo;intérieur ; un client VirtualBox peut faire l\u0026rsquo;affaire. Pour suivre, téléchargez la base de données que nous allons utiliser depuis ici. Ainsi, vous pourrez suivre l\u0026rsquo;évolution de la situation avec une instance locale de Power BI sur votre machine. Créez une connexion à la base de données\nPour commencer, sélectionnez l\u0026rsquo;option Obtenir des données après avoir ouvert Power BI ; cela ouvrira une fenêtre dans laquelle vous indiquerez le type de stockage ou de base de données dans lequel vos données sont stockées. La popup montrera toutes les sources de données auxquelles Power BI peut accéder.\nPour notre exercice, nous allons sélectionner la base de données PostgreSQL, comme indiqué ci-dessous.\nOption de base de données PostgreSQL sur la fenêtre \u0026ldquo;Get Data\u0026rdquo;\n!Option de base de données PostgreSQL sur la fenêtre \u0026ldquo;Get Data\u0026rdquo; (Obtenir des données)\nUne fois sélectionnée, Power BI vous demandera de confirmer le nom d\u0026rsquo;utilisateur et le mot de passe la première fois que vous vous connecterez à cette base de données. Il n\u0026rsquo;est pas nécessaire de répéter cette validation lorsque vous vous connectez en tant que même utilisateur sur le même serveur déjà utilisé.\nAssurez-vous simplement que l\u0026rsquo;utilisateur PostgreSQL spécifié dispose des permissions requises sur la base de données où réside votre table.\nUne fois authentifié sur le serveur, vous devrez spécifier le serveur et la base de données. Pour cela, il suffit d\u0026rsquo;ajouter l\u0026rsquo;adresse de votre serveur PowerBI, ou localhost si vous avez une instance locale de PostgreSQL, pour accéder à la base de données où vous allez sélectionner vos tables. Nous allons sélectionner l\u0026rsquo;option Import pour le mode de connectivité des données puisque nous allons transformer certaines données avant de les utiliser.\nL\u0026rsquo;option DirectConnect, comme son nom l\u0026rsquo;indique, ne crée pas ce cache nécessaire ; je recommande toujours d\u0026rsquo;utiliser Import au lieu de DirectQuery. L\u0026rsquo;un des défauts les plus importants de DirectQuery est qu\u0026rsquo;il ne permet pas la transformation des données pendant le chargement.\nLa base de données PostgreSQL obtient l\u0026rsquo;écran de données.\nPour faire court, laissez les options avancées inchangées et cliquez sur OK.\nPower BI vous informera avant de continuer dans le cas où votre connexion n\u0026rsquo;est pas cryptée. Si vous cliquez sur Cancel, l\u0026rsquo;importation sera interrompue. Je vous encourage à explorer davantage l\u0026rsquo;option SQL Statement, car elle offre plus de choix pour transformer vos données tout en les chargeant.\nVous pouvez trouver plus de détails et même un exemple fonctionnel sur cette page sur les native database queries de Microsoft.\nTransformer les données pendant leur chargement dans votre datastore\nMaintenant que nous avons pris soin de notre connexion, il est temps de la faire travailler pour nous. Sur l\u0026rsquo;écran suivant, vous pouvez sélectionner vos tableaux. Une fois que vous les aurez sélectionnés, vous aurez la possibilité de les charger tels quels ou de les transformer avant de les utiliser. Dans ce tutoriel, je vais séparer le premier mot des autres en utilisant le délimiteur le plus à gauche ; en choisissant transformer, une nouvelle fenêtre (comme celle ci-dessous) apparaîtra.\nDans le champ Sélectionner ou entrer le délimiteur, laissez les options restantes avec délimiteur d\u0026rsquo;espace et caractère de citation inchangées. Cela transformera le champ chaîne de caractères en deux champs enfants.\nOptions de fractionnement pour l\u0026rsquo;importation de données\nUne fois que vous aurez confirmé la modification, vous reviendrez à l\u0026rsquo;écran précédent. Il reflétera maintenant les changements que vous avez appliqués sur l\u0026rsquo;écran Split Columns.\nIl est bon de rappeler que vous pouvez toujours supprimer ces modifications, même après la confirmation précédente.\nCes résultats ne sont pas définitifs sur les sources de données utilisées par les tableaux de bord ; ils sont plutôt sur des composants temporaires. Vous pouvez donc toujours revenir en arrière si vous souhaitez ajouter ou modifier certains détails supplémentaires.\nUne fois que tous vos paramètres sont comme vous le souhaitez, sélectionnez l\u0026rsquo;option Close \u0026amp; Apply pour valider vos changements, comme indiqué ci-dessous.\n!- Preview Get Data after transformation changes Aperçu de Get Data après les changements de transformation.\nFélicitations ! Vous venez d\u0026rsquo;importer vos données de PostgreSQL dans Power BI.\nDémarrez votre voyage de données\nAujourd\u0026rsquo;hui, nous avons effleuré l\u0026rsquo;intégration de Power BI et de PostgreSQL et expliqué comment Power BI pouvait améliorer votre exploration des données sur les bases de données PostgreSQL grâce à son interface très conviviale.\nPower BI vous permet d\u0026rsquo;interagir avec les données à leurs débuts. Pourquoi ne pas enrichir vos données directement à partir de la réplication de la source ? N\u0026rsquo;oubliez pas quels sont vos réels besoins.\nIl peut être difficile d\u0026rsquo;avoir une vue complète de votre historique analyse des tendances des données.\nCela dit, une bonne chose à avoir à l\u0026rsquo;esprit est d\u0026rsquo;éviter, ou du moins de limiter lorsque c\u0026rsquo;est possible, l\u0026rsquo;utilisation de Power BI sur des données non traitées.\nLes données à l\u0026rsquo;état brut sont plutôt destinées à des fins opérationnelles. Pour les analyses sensibles au facteur temps, vous devriez toujours choisir d\u0026rsquo;utiliser des informations traitées qui donnent de meilleurs aperçus des données d\u0026rsquo;application dans leur état brut.\nÀ+!\n","permalink":"https://www.dawrlog.com/fr/articles/7/","summary":"Bonjour à tous !\nAujourd\u0026rsquo;hui, j\u0026rsquo;aimerais vous parler d\u0026rsquo;un des outils de dataviz qui est de plus en plus adopté : Power BI. Power BI augmente chaque jour son adoption parmi les utilisateurs les plus expérimentés. Grâce à sa convivialité et à la possibilité de se connecter à diverses sources de données, il permet d\u0026rsquo;analyser des sources de données distinctes au sein d\u0026rsquo;une même console.\nDans ce billet, je vais vous montrer comment importer vos données stockées sur PostgreSQL avec une version de bureau de Power BI.","title":"Comment fouiller vos données utilisant Power BI."},{"content":"Salut tout le monde, aujourd\u0026rsquo;hui je voudrais vous présenter comment explorer vos données brutes stockées par AWS S3 en utilisant les services analytiques gérés par AWS. Aujourd\u0026rsquo;hui, nous allons couvrir à la fois AWS Redshift Spectrum et AWS Athena dans notre tutoriel.\nL\u0026rsquo;un des principaux défis auxquels sont confrontées les entreprises axées sur les données est l\u0026rsquo;intégration de différents systèmes d\u0026rsquo;application.\nLes principaux fournisseurs de clouds publics, tels que Amazon Web Services ou Google Cloud Platform, proposent des produits robustes prêts à répondre à vos besoins en matière d\u0026rsquo;analyse. Mais lorsqu\u0026rsquo;il s\u0026rsquo;agit d\u0026rsquo;explorer vos données, les choses ne sont pas toujours aussi simples.\nVos données sources proviennent souvent de fichiers dont les formats de données sont inconnus, ce qui fait du travail de l\u0026rsquo;analyste un cauchemar.\nCertains cas pourraient être plus fluides dans cette intégration, comme lorsque vos données ont des valeurs imbriquées. L\u0026rsquo;explosion de structures complexes, comme les fichiers JSON, en un format tabulaire, peut consommer la majeure partie de votre temps lorsque vous explorez de nouvelles données.\nC\u0026rsquo;est là que AWS Redshift Spectrum et AWS Athena se distinguent. Elles vous permettent d\u0026rsquo;utiliser SQL pour analyser les données sans les modifier à la source. Il n\u0026rsquo;y a pas besoin de code Python complexe si vous ne voulez pas l\u0026rsquo;utiliser pour les tâches initiales de profilage des données.\nPas mal, non ?\nCet article vous montrera comment explorer vos données sur Amazon S3 à l\u0026rsquo;aide d\u0026rsquo;Athena et de Redshift Spectrum. Vous trouverez ci-dessous les étapes nécessaires pour créer une table sur le catalogue AWS Glue et l\u0026rsquo;utiliser pour accéder à vos données sur Amazon S3. Comment Redshift Spectrum, AWS Athena \u0026amp; AWS S3 s\u0026rsquo;accordent-ils ? Si les produits de données d\u0026rsquo;Amazon ne sont pas aussi étendus que sa célèbre boutique de commerce électronique, il y a quand même beaucoup de choses à faire. Redshift Spectrum Spectrum est un composant Amazon Redshift qui vous permet d\u0026rsquo;interroger des fichiers stockés dans Amazon S3. Pour ce faire, il suffit de créer une nouvelle base de données pointant vers votre seau de stockage AWS S3.\nVotre équipe peut affiner sa recherche en interrogeant uniquement les colonnes nécessaires à votre analyse. Il est également possible de consulter les tables existantes de votre cluster Redshift, ce qui signifie qu\u0026rsquo;au lieu d\u0026rsquo;interroger la table complète en permanence, vous pouvez sélectionner les colonnes requises pour votre rapport à l\u0026rsquo;aide de SQL.\nAinsi, lorsque vous interrogez vos données, vous obtenez uniquement les colonnes nécessaires au lieu de renvoyer des champs et des lignes inutiles. Cette fonction offre également la possibilité d\u0026rsquo;interroger des données stockées directement sur Amazon S3. AWS Athena Athena facilite la création de requêtes SQL partageables entre vos équipes, contrairement à Spectrum, qui nécessite Redshift. Vous pouvez ensuite créer et exécuter vos classeurs sans aucune configuration de cluster.\nAthena permet de faire plus avec moins, et il est plus économique d\u0026rsquo;explorer vos données avec moins de gestion que Redshift Spectrum.\nAWS S3 AWS S3 est l\u0026rsquo;option de stockage d\u0026rsquo;objets géré proposée par Amazon. C\u0026rsquo;est la meilleure option pour stocker vos données semi-structurées, comme les journaux de serveur de vos applications.\nS3 permet également la protection contre la suppression et le contrôle de version de vos objets, ce qui rend vos données plus sûres et plus faciles à retracer jusqu\u0026rsquo;à leur source d\u0026rsquo;origine.\nComment créer des tables à partir de fichiers Maintenant que vous avez une idée générale de chaque produit, il est temps de mettre la main à la pâte et de créer quelques tableaux !\nNous utiliserons un exemple de données CSV pour notre tutoriel, que vous pouvez télécharger ici.\nNous supposons également que votre cluster Redshift est prêt et que les rôles IAM nécessaires y sont attachés (si vous utilisez Redshift Spectrum).\nVous devez également avoir votre AWS bucket configuré avec vos données et les autorisations requises pour créer votre catalogue de données ; vous trouverez plus de détails sur la page Athena IAM Data Catalog policy.\nBon, jusqu\u0026rsquo;ici tout va bien ! Passons à la création des tables.\nTout d\u0026rsquo;abord, vous devez créer la base de données dans laquelle les tables seront stockées.\nPour ce tutoriel, nous comptons sur AWS Glue Data Catalog pour cette tâche. N\u0026rsquo;oubliez pas que d\u0026rsquo;autres options sont disponibles, comme le Hive metastore.\nAWS Glue Data Catalog est une meilleure option si vous souhaitez avoir une intégration fluide avec des sources de données supplémentaires sans avoir à démarrer des services supplémentaires.\nVoici comment créer une base de données sur AWS Athena :\nCREATE DATABASE IF NOT EXISTS ext_data_suppliers COMMENT \u0026#39;Landing Zone for S3 buckets loaded by external Data Suppliers\u0026#39; LOCATION \u0026#39;s3://test-12343210/\u0026#39;; Voici la version Redshift Spectrum de celui-ci :\ncreate external schema ext_data_suppliers from data catalog database \u0026#39;ext_data_suppliers\u0026#39; iam_role \u0026#39;arn:aws:iam::123456789012:role/RSRoleApiData\u0026#39; create external database if not exists; Comme vous pouvez le voir dans les deux cas, votre code créera une base de données de catalogue Glue si elle n\u0026rsquo;existe pas déjà.\nUne fois que vous l\u0026rsquo;avez, vous aurez besoin de la définition de la table, qui vous permettra d\u0026rsquo;interroger les données directement à partir du fichier.\nA ce stade, je recommande de ne pas faire de transformations sur les données car une modification mineure. Comme sur cette couche de données nous voulons être aussi proche que possible de sa source de données, même une simple conversion de type de données peut entraîner la perte de données. Évitons donc cela, surtout dans les premières étapes.\nMaintenant, créons une définition de table qui contiendra les données.\nCi-dessous, vous pouvez voir la version Athena :\nCREATE EXTERNAL TABLE IF NOT EXISTS ext_data_suppliers.zillow_sample_file ( `index` int, `liv_space_in_sqft` int, `beds` int, `baths` int, `zip` int, `year` int, `list_price_in_usd` int ) ROW FORMAT SERDE \u0026#39;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\u0026#39; WITH SERDEPROPERTIES ( \u0026#39;serialization.format\u0026#39; = \u0026#39;,\u0026#39;, \u0026#39;field.delim\u0026#39; = \u0026#39;,\u0026#39;) LOCATION \u0026#39;s3://test-12343210/\u0026#39; TBLPROPERTIES (\u0026#39;has_encrypted_data\u0026#39;=\u0026#39;false\u0026#39;); Et voici la version Spectrum :\nCREATE EXTERNAL TABLE ext_data_suppliers.zillow_sample_file ( index int, liv_space_in_sqft int, beds int, baths int, zip int, year int, list_price_in_usd int ) ROW FORMAT SERDE \u0026#39;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\u0026#39; WITH SERDEPROPERTIES ( \u0026#39;serialization.format\u0026#39; = \u0026#39;,\u0026#39;, \u0026#39;field.delim\u0026#39; = \u0026#39;,\u0026#39;) LOCATION \u0026#39;s3://test-12343210/\u0026#39;; Alors, lequel choisir : Spectrum ou Athena ? La plupart des données provenant des fournisseurs de données ne sont pas optimisées pour répondre aux questions que vous vous posez. Comme ces données sont à l\u0026rsquo;état brut, des outils comme AWS Athena ou AWS Redshift Spectrum rendront vos tâches d\u0026rsquo;exploration beaucoup plus simples.\nCes deux outils vous permettent d\u0026rsquo;explorer vos données sans les charger dans une base de données. Tout ce que vous avez à faire est de dire quelle est votre structure de données et où elle réside. Après cela, vous êtes prêt à partir - plus de retard sur vos pipelines de données pour commencer à créer vos tableaux de bord. Dès que vos données sont disponibles sur votre seau Amazon S3, votre équipe peut les consommer immédiatement.\nVous pouvez exécuter des requêtes fédérées sur les deux services. Ces requêtes permettent d\u0026rsquo;utiliser le même script SQL pour corréler des données provenant de sources relationnelles structurées, telles que MySQL et Postgres.\nMon conseil ? Choisissez Athena si vous n\u0026rsquo;avez pas déjà un cluster Redshift en place.\nAvec Athena, il devient plus facile de créer des requêtes partageables au sein de votre équipe sans gérer de services supplémentaires et augmenter inutilement votre facture de cloud. Résumé et approfondi\nPour résumer, nous avons abordé deux sujets importants :\nLes avantages de disposer d\u0026rsquo;un outil d\u0026rsquo;exploration des données qui permet à vos analystes d\u0026rsquo;exécuter des commandes SQL au-dessus de votre solution de type stockage objet. Exécution de commandes SQL sur des fichiers stockés dans Amazon S3, en utilisant Athena et Redshift Spectrum.\nComme vous l\u0026rsquo;avez vu, les deux scripts sont très similaires. Dans les deux, nous avons utilisé serialization/deserialization (SerDe en abrégé) pour créer correctement une structure de type tableau. Cette structure vous permet d\u0026rsquo;accéder aux données au format CSV sans les charger dans des tableaux natifs.\nVoici une autre chose dont j\u0026rsquo;aimerais que vous vous souveniez : N\u0026rsquo;accordez que les permissions nécessaires aux services. En limitant l\u0026rsquo;accès à vos services, vous pourrez mieux dormir.\nSi vous avez besoin d\u0026rsquo;aide pour transformer vos données, vous pouvez nous contacter pour toute question et même découvrir de nouvelles informations dont vous n\u0026rsquo;étiez pas conscient grâce à une meilleure compréhension de vos données.\nÀ+!\n","permalink":"https://www.dawrlog.com/fr/articles/11/","summary":"Salut tout le monde, aujourd\u0026rsquo;hui je voudrais vous présenter comment explorer vos données brutes stockées par AWS S3 en utilisant les services analytiques gérés par AWS. Aujourd\u0026rsquo;hui, nous allons couvrir à la fois AWS Redshift Spectrum et AWS Athena dans notre tutoriel.\nL\u0026rsquo;un des principaux défis auxquels sont confrontées les entreprises axées sur les données est l\u0026rsquo;intégration de différents systèmes d\u0026rsquo;application.\nLes principaux fournisseurs de clouds publics, tels que Amazon Web Services ou Google Cloud Platform, proposent des produits robustes prêts à répondre à vos besoins en matière d\u0026rsquo;analyse.","title":"Le connaissance de base pour l'exploration des données utilisant AWS Redshift Spectrum, Athena et S3."}]