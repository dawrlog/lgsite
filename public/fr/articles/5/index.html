<!DOCTYPE html>
<html lang="fr" dir="fr">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Examples des ETL utilisant des workflows Argo. | Logistique Dataware</title>
<meta name="keywords" content="Kubernetes, Argo, Development, Data Warehouse">
<meta name="description" content="Bonjour à tous,
Ce post explore l&rsquo;utilisation des Argo Workflows pour orchestrer vos pipelines de données. Pour commencer, rappelons ce qu&rsquo;est l&rsquo;ETL tout en concevant l&rsquo;architecture de haut niveau de notre travail. Je montrerai comment configurer vos pipelines de données pour qu&rsquo;ils suivent plus naturellement la structure. Ensuite, nous verrons comment obtenir le même résultat en utilisant des graphes acycliques dirigés (DAGs). Enfin, je résumerai ce que nous avons vu et présenterai des raisons pour guider votre choix d&rsquo;approche en fonction de la complexité de votre projet.">
<meta name="author" content="Daniel Paes">
<link rel="canonical" href="https://pipekit.io/blog/argo-workflows-etl-examples">
<meta name="google-site-verification" content="4599922566">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<link rel="icon" href="https://www.dawrlog.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://www.dawrlog.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://www.dawrlog.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://www.dawrlog.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://www.dawrlog.com/apple-touch-icon.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="fr" href="https://www.dawrlog.com/fr/articles/5/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Examples des ETL utilisant des workflows Argo." />
<meta property="og:description" content="Bonjour à tous,
Ce post explore l&rsquo;utilisation des Argo Workflows pour orchestrer vos pipelines de données. Pour commencer, rappelons ce qu&rsquo;est l&rsquo;ETL tout en concevant l&rsquo;architecture de haut niveau de notre travail. Je montrerai comment configurer vos pipelines de données pour qu&rsquo;ils suivent plus naturellement la structure. Ensuite, nous verrons comment obtenir le même résultat en utilisant des graphes acycliques dirigés (DAGs). Enfin, je résumerai ce que nous avons vu et présenterai des raisons pour guider votre choix d&rsquo;approche en fonction de la complexité de votre projet." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.dawrlog.com/fr/articles/5/" />
<meta property="og:image" content="https://user-images.githubusercontent.com/78096758/217079803-11a4c93a-7ae1-4273-b067-499a0d67023a.png" /><meta property="article:section" content="articles" />
<meta property="article:published_time" content="2022-08-14T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-08-14T00:00:00+00:00" /><meta property="og:site_name" content="Logistique Dataware" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://user-images.githubusercontent.com/78096758/217079803-11a4c93a-7ae1-4273-b067-499a0d67023a.png" />
<meta name="twitter:title" content="Examples des ETL utilisant des workflows Argo."/>
<meta name="twitter:description" content="Bonjour à tous,
Ce post explore l&rsquo;utilisation des Argo Workflows pour orchestrer vos pipelines de données. Pour commencer, rappelons ce qu&rsquo;est l&rsquo;ETL tout en concevant l&rsquo;architecture de haut niveau de notre travail. Je montrerai comment configurer vos pipelines de données pour qu&rsquo;ils suivent plus naturellement la structure. Ensuite, nous verrons comment obtenir le même résultat en utilisant des graphes acycliques dirigés (DAGs). Enfin, je résumerai ce que nous avons vu et présenterai des raisons pour guider votre choix d&rsquo;approche en fonction de la complexité de votre projet."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Articles",
      "item": "https://www.dawrlog.com/fr/articles/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Examples des ETL utilisant des workflows Argo.",
      "item": "https://www.dawrlog.com/fr/articles/5/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Examples des ETL utilisant des workflows Argo.",
  "name": "Examples des ETL utilisant des workflows Argo.",
  "description": "Bonjour à tous,\nCe post explore l\u0026rsquo;utilisation des Argo Workflows pour orchestrer vos pipelines de données. Pour commencer, rappelons ce qu\u0026rsquo;est l\u0026rsquo;ETL tout en concevant l\u0026rsquo;architecture de haut niveau de notre travail. Je montrerai comment configurer vos pipelines de données pour qu\u0026rsquo;ils suivent plus naturellement la structure. Ensuite, nous verrons comment obtenir le même résultat en utilisant des graphes acycliques dirigés (DAGs). Enfin, je résumerai ce que nous avons vu et présenterai des raisons pour guider votre choix d\u0026rsquo;approche en fonction de la complexité de votre projet.",
  "keywords": [
    "Kubernetes", "Argo", "Development", "Data Warehouse"
  ],
  "articleBody": "Bonjour à tous,\nCe post explore l’utilisation des Argo Workflows pour orchestrer vos pipelines de données. Pour commencer, rappelons ce qu’est l’ETL tout en concevant l’architecture de haut niveau de notre travail. Je montrerai comment configurer vos pipelines de données pour qu’ils suivent plus naturellement la structure. Ensuite, nous verrons comment obtenir le même résultat en utilisant des graphes acycliques dirigés (DAGs). Enfin, je résumerai ce que nous avons vu et présenterai des raisons pour guider votre choix d’approche en fonction de la complexité de votre projet. Alors, commençons.\nQue signifie ETL ? Tout d’abord, rappelons ce qu’est l’ETL avant de commencer nos exemples. L’extraction, la transformation et le chargement consistent en des tâches visant à nettoyer vos données et à regrouper les données de vos applications dans une base de données conforme. Imaginez cette base de données conforme comme la source unique de vérité de vos données. Ce référentiel centralisé vous aide à mieux connaître vos produits et vos clients.\nCependant, chaque application a sa propre structure pour traiter les données. Chaque tâche ETL rend les données de l’application plus attrayantes pour l’analyse, en ayant des dépendances explicites à mesure que votre processus de data wrangling devient plus robuste. Notre code va déployer quatre tâches ETL et leur relation, comme le reflète l’image ci-dessous :\nArchitecture de haut niveau des tâches ETL\nComprendre chaque tâche ETL Ce flux de travail prend en charge deux formats de données différents : colonne ou ligne. Ces différents formats nécessitent des analyseurs différents : parquet pour les colonnes et avro pour les lignes. Nous pouvons également nous y référer en tant que batch ou stream, respectivement.\nLe flux de travail commence donc par un gestionnaire de requête qui identifie le type de données. En fonction de la valeur d’un indicateur de type de données, il transmet la tâche aux balises d’analyse batch ou stream.\nLes analyseurs transmettent leurs résultats à la tâche Load Data, où ils sont chargés dans le stockage persistant, en fonction de la source et du type de données.\nBien qu’il semble s’agir d’un seul flux de travail avec deux chemins de code différents, il y a des avantages à traiter les deux types de données dans le même ensemble de tâches. Le partage d’une tâche de chargement entre les flux de données facilite la gestion des chevauchements et des relations. Par exemple, le magasin de données en colonnes peut avoir besoin d’être mis à jour avec des clés étrangères provenant de nouvelles données en lignes. Il est également plus facile de partager un seul flux ETL entre différentes équipes.\nMaintenant que nous savons ce que nous allons construire, commençons par voir comment le mettre en œuvre en utilisant les étapes des workflows Argo.\nConstruire un pipeline ETL en utilisant les étapes des workflows Argo Dans cette approche, le pipeline de données va suivre une liste d’étapes pour nettoyer et traiter les données de vos sources de données. Votre code ETL devient encore plus robuste lorsque nous utilisons des conditionnels pour informer quel flux ETL vos données doivent prendre. Cela semble bien, non ? Alors, mettons nos mains dans le cambouis et voyons comment cela fonctionne en pratique.\nLe code ci-dessous va créer un workflow dans un espace de nom appelé argo. Cet espace de nom doit exister avant que le workflow ne soit exécuté avec argo submit. Cela permet d’éviter les problèmes de sécurité, comme le fait que votre utilisateur n’ait pas le droit de créer des espaces de noms. Cela évitera également les messages d’erreur vous avertissant de ne pas casser votre déploiement Kubernetes. Pour notre exemple, nous allons générer une valeur aléatoire sur une machine Linux et charger les données à venir en fonction de cette valeur.\nBien que les deux étapes d’analyse syntaxique soient déclenchées simultanément, seule celle informée par l’étape de traitement des demandes s’exécutera. L’utilisation d’un code automatisé comme celui-ci réduira les chances d’avoir des problèmes avec notre flux de données ETL. L’automatisation de vos étapes de flux de travail gère les erreurs courantes telles que les types de mismatch dans votre base de données.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: stream-batch-parser- namespace: argo spec: entrypoint: sbp templates: - name: sbp steps: - - name: request-handler template: edge-input - - name: parser-stream template: stream when: \"{{steps.handle-requests.outputs.result}} \u003c= 163883\" - name: parser-batch template: parquet when: \"{{steps.handle-requests.outputs.result}} \u003e 163883\" - name: stream steps: - - name: avro-parser template: avro - - name: wrapper template: wrapper - name: batch steps: - - name: parquet-parser template: parquet - - name: wrapper template: wrapper - name: edge-input container: image: alpine:3.6 command: [sh, -c] args: [\"echo ${RANDOM}\"] - name: parquet container: image: alpine:3.6 command: [sh, -c] args: [\"echo \\\"code to parse Parquet\\\"\"] - name: avro container: image: alpine:3.6 command: [sh, -c] args: [\"echo \\\"code to parse Avro\\\"\"] - name: wrapper container: image: alpine:3.6 command: [sh, -c] args: [\"echo \\\"code to load into Staging\\\"\"] Sauvegardez le fichier ci-dessus en tant que `etl_steps.yml` et démarrez votre flux de travail avec cette commande : argo submit -n argo etl_steps.yml Nous pouvons maintenant obtenir le statut de notre workflow en exécutant la commande argo get suivante :\nargo get -n argo stream-batch-parser-xxvks Les cinq derniers chiffres seront différents dans chaque environnement. Et en exécutant la commande précédente, votre journal de sortie devrait être similaire à l’image ci-dessous ; comme indiqué, notre workflow exécutera la tâche de flux d’analyseur en fonction de la valeur renvoyée par la tâche de traitement des demandes.\nSortie des étapes ETL du workflow Argo Etapes ETL retournées par l’exécution de la commande argo get. Maintenant que nous avons vu comment construire un ETL avec des tâches, nous allons explorer comment utiliser les DAGs pour votre ETL.\nConstruire un pipeline ETL avec des DAGs au lieu d’étapes Maintenant, explorons comment réaliser le même travail en utilisant des modèles de DAGs au lieu d’étapes dans Argo Workflows. Même si le DSL semble similaire à première vue, les DAGs vous donnent plus de pouvoir pour spécifier les dépendances entre les étapes et exécuter les tâches en parallèle.\nDans un DAG, toute tâche peut être exécutée lorsque ses dépendances sont satisfaites. Si les dépendances de plus d’une tâche sont satisfaites, toutes les tâches seront exécutées en parallèle. Si une tâche n’a pas de dépendances, elle s’exécutera dès que le workflow sera lancé. Les DAGs sont excellents pour traiter l’ETL, et je vous suggère fortement de vous familiariser avec toutes les options qu’une tâche DAG peut fournir en consultant la documentation officielle d’Argo.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: dag-orchestrate- namespace: argo spec: entrypoint: sbp archiveLogs: true templates: - name: sbp dag: tasks: - name: request-handler template: edge-input - name: stream-flow template: stream when: \"{{tasks.handle-requests.outputs.result}} \u003c= 163883\" depends: handle-requests - name: batch-flow template: batch when: \"{{tasks.handle-requests.outputs.result}} \u003e 163883\" depends: handle-requests - name: stream steps: - - name: avro-parser template: avro - - name: wrapper template: wrapper - name: batch steps: - - name: parquet-parser template: parquet - - name: wrapper template: wrapper - name: edge-input container: image: alpine:3.6 command: [sh, -c] args: [\"echo ${RANDOM}\"] - name: parquet container: image: alpine:3.6 command: [sh, -c] args: [\"echo \\\"code to parse Parquet\\\"\"] - name: avro container: image: alpine:3.6 command: [sh, -c] args: [\"echo \\\"code to parse Avro\\\"\"] - name: wrapper container: image: alpine:3.6 command: [sh, -c] args: [\"echo \\\"code to load into Staging\\\"\"] Enregistrez le fichier ci-dessus en tant que etl_dag.yml et soumettez votre flux de travail pour le lancer :\nargo submit -n argo etl_dag.yml Comme démontré ci-dessous, vous pouvez vérifier son évolution avec argo get :\nargo get -n argo dag-orchestrate-ctpkl Dans ce scénario, notre workflow a exécuté la tâche de flux batch au lieu du flux stream en fonction de la valeur renvoyée par la tâche handle requests. Félicitations pour votre travail ! Vous pouvez maintenant concevoir vos flux de données ETL en utilisant un DAG ou une liste structurée d’étapes dans Argo Workflows.\nN’oubliez pas de nettoyer votre environnement avec argo delete -n argo your-workflow, où vous devez informer le workflow désiré comme votre-workflow.\nargo delete -n argo your-workflow Conclusion Bien qu’il soit généralement utilisé pour la gestion de l’infrastructure, Argo Workflows peut également orchestrer vos tâches ETL. En l’utilisant ainsi, vous n’avez plus besoin de recourir à différents outils pour atteindre le même objectif, c’est-à-dire Argo pour le CI/CD et Airflow pour les tâches ETL.\nL’approche DAG est souvent meilleure que l’approche par étapes pour l’exécution des pipelines ETL. Pour commencer, le traitement des tâches DAG est optimisé au moment de l’exécution. Vous aurez moins de points de décision pour certains de vos pipelines simplement en informant le flux de données souhaité.\nPour les tâches simples, les flux séquentiels (comme vous obtenez avec l’approche par étapes dans les workflows Argo) fonctionnent bien. Cependant, ils deviennent plus difficiles à maintenir dans les cas où vous devez cibler un sous-ensemble de votre flux de données et gérer des dépendances complexes dans le temps.\nUn autre avantage de l’utilisation des DAGs est de pouvoir spécifier l’étape exacte au moment de l’exécution. L’exécution vous donne plus de liberté pour créer un code conditionnel avec moins de boucles indentées tout en optimisant le code et les ressources de l’infrastructure.\nJe vous invite à approfondir la documentation d’Argo Workflows sur les DAGs. Maîtriser le fonctionnement des DAGs peut augmenter la qualité de vos pipelines ETL, en vous permettant de gérer vos tâches ETL de manière plus dynamique par rapport à la méthode des étapes.\nPour des moyens plus optimisés de gérer vos ressources Kubernetes, explorez comment Pipekit peut vous aider à orchestrer l’ensemble de votre déploiement Argo Workflows.\nÀ+!\n",
  "wordCount" : "1590",
  "inLanguage": "fr",
  "image":"https://user-images.githubusercontent.com/78096758/217079803-11a4c93a-7ae1-4273-b067-499a0d67023a.png","datePublished": "2022-08-14T00:00:00Z",
  "dateModified": "2022-08-14T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Daniel Paes"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://www.dawrlog.com/fr/articles/5/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Logistique Dataware",
    "logo": {
      "@type": "ImageObject",
      "url": "https://www.dawrlog.com/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://www.dawrlog.com/fr/" accesskey="h" title="Logistique Dataware (Alt + H)">Logistique Dataware</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="https://www.dawrlog.com/" title="English"
                            aria-label=":us:">🇺🇸</a>
                    </li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://www.dawrlog.com/fr/apropos" title="À Propos">
                    <span>À Propos</span>
                </a>
            </li>
            <li>
                <a href="https://www.dawrlog.com/fr/articles/" title="Articles">
                    <span>Articles</span>
                </a>
            </li>
            <li>
                <a href="https://calendly.com/dawrlog" title="Contactez-nous">
                    <span>Contactez-nous</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://www.dawrlog.com/fr/">Accueil</a>&nbsp;»&nbsp;<a href="https://www.dawrlog.com/fr/articles/">Articles</a></div>
    <h1 class="post-title">
      Examples des ETL utilisant des workflows Argo.
    </h1>
    <div class="post-meta"><span title='2022-08-14 00:00:00 +0000 UTC'>août 14, 2022</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;Daniel Paes

</div>
  </header> 
<figure class="entry-cover"><img loading="lazy" src="https://user-images.githubusercontent.com/78096758/217079803-11a4c93a-7ae1-4273-b067-499a0d67023a.png" alt="">
        
</figure><div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table des matières</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#h2que-signifie-etl--h2br-"><h2>Que signifie ETL ? </h2><br /></a></li>
        <li><a href="#h2comprendre-chaque-tâche-etl-h2br-"><h2>Comprendre chaque tâche ETL </h2><br /></a></li>
        <li><a href="#h2construire-un-pipeline-etl-en-utilisant-les-étapes-des-workflows-argoh2-br-"><h2>Construire un pipeline ETL en utilisant les étapes des workflows Argo</h2> <br /></a></li>
        <li><a href="#h2construire-un-pipeline-etl-avec-des-dags-au-lieu-détapes-h2br-"><h2>Construire un pipeline ETL avec des DAGs au lieu d&rsquo;étapes </h2><br /></a></li>
        <li><a href="#h2conclusion-h2br-"><h2>Conclusion </h2><br /></a></li>
      </ul>
    </li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><p>Bonjour à tous,</p>
<p>Ce post explore l&rsquo;utilisation des <a href="https://argoproj.github.io/argo-workflows/">Argo Workflows</a> pour orchestrer vos pipelines de données. Pour commencer, rappelons ce qu&rsquo;est l&rsquo;ETL tout en concevant l&rsquo;architecture de haut niveau de notre travail. Je montrerai comment configurer vos pipelines de données pour qu&rsquo;ils suivent plus naturellement la structure. Ensuite, nous verrons comment obtenir le même résultat en utilisant des <a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">graphes acycliques dirigés</a> (DAGs). Enfin, je résumerai ce que nous avons vu et présenterai des raisons pour guider votre choix d&rsquo;approche en fonction de la complexité de votre projet. Alors, commençons.</p>
<h3 id="h2que-signifie-etl--h2br-"><h2>Que signifie ETL ? </h2><br /><a hidden class="anchor" aria-hidden="true" href="#h2que-signifie-etl--h2br-">#</a></h3>
<p>Tout d&rsquo;abord, rappelons ce qu&rsquo;est l&rsquo;ETL avant de commencer nos exemples. <a href="https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/etl-architecture-34-subsystems/">L&rsquo;extraction, la transformation et le chargement</a> consistent en des tâches visant à nettoyer vos données et à regrouper les données de vos applications dans une base de données conforme. Imaginez cette base de données conforme comme la source unique de vérité de vos données. Ce référentiel centralisé vous aide à mieux connaître vos produits et vos clients.</p>
<p>Cependant, chaque application a sa propre structure pour traiter les données. Chaque tâche ETL rend les données de l&rsquo;application plus attrayantes pour l&rsquo;analyse, en ayant des dépendances explicites à mesure que votre processus de <a href="https://en.wikipedia.org/wiki/Data_wrangling">data wrangling</a> devient plus robuste. Notre code va déployer quatre tâches ETL et leur relation, comme le reflète l&rsquo;image ci-dessous :</p>
<p><img loading="lazy" src="https://user-images.githubusercontent.com/78096758/217080436-2a286a21-2aff-40ab-9d83-1b874c8e5c6b.png#center" alt="exemples d&amp;rsquo;ETL"  />

<em>Architecture de haut niveau des tâches ETL</em></p>
<h3 id="h2comprendre-chaque-tâche-etl-h2br-"><h2>Comprendre chaque tâche ETL </h2><br /><a hidden class="anchor" aria-hidden="true" href="#h2comprendre-chaque-tâche-etl-h2br-">#</a></h3>
<p>Ce flux de travail prend en charge deux formats de données différents : colonne ou ligne. Ces différents formats nécessitent des analyseurs différents : parquet pour les colonnes et avro pour les lignes. Nous pouvons également nous y référer en tant que batch ou stream, respectivement.</p>
<p>Le flux de travail commence donc par un gestionnaire de requête qui identifie le type de données. En fonction de la valeur d&rsquo;un indicateur de type de données, il transmet la tâche aux balises d&rsquo;analyse batch ou stream.</p>
<p>Les analyseurs transmettent leurs résultats à la tâche Load Data, où ils sont chargés dans le stockage persistant, en fonction de la source et du type de données.</p>
<p>Bien qu&rsquo;il semble s&rsquo;agir d&rsquo;un seul flux de travail avec deux chemins de code différents, il y a des avantages à traiter les deux types de données dans le même ensemble de tâches. Le partage d&rsquo;une tâche de chargement entre les flux de données facilite la gestion des chevauchements et des relations. Par exemple, le magasin de données en colonnes peut avoir besoin d&rsquo;être mis à jour avec des clés étrangères provenant de nouvelles données en lignes. Il est également plus facile de partager un seul flux ETL entre différentes équipes.</p>
<p>Maintenant que nous savons ce que nous allons construire, commençons par voir comment le mettre en œuvre en utilisant les étapes des workflows Argo.</p>
<p><img loading="lazy" src="https://user-images.githubusercontent.com/78096758/217080713-bbd06f86-f41d-45bf-87d6-c1981b97776c.png#center" alt="Souvenez-vous"  />

<br /></p>
<h3 id="h2construire-un-pipeline-etl-en-utilisant-les-étapes-des-workflows-argoh2-br-"><h2>Construire un pipeline ETL en utilisant les étapes des workflows Argo</h2> <br /><a hidden class="anchor" aria-hidden="true" href="#h2construire-un-pipeline-etl-en-utilisant-les-étapes-des-workflows-argoh2-br-">#</a></h3>
<p>Dans cette approche, le pipeline de données va suivre une liste d&rsquo;étapes pour nettoyer et traiter les données de vos sources de données. Votre code ETL devient encore plus robuste lorsque nous utilisons des conditionnels pour informer quel flux ETL vos données doivent prendre. Cela semble bien, non ? Alors, mettons nos mains dans le cambouis et voyons comment cela fonctionne en pratique.</p>
<p>Le code ci-dessous va créer un workflow dans un espace de nom appelé <code>argo</code>. Cet espace de nom doit exister avant que le workflow ne soit exécuté avec <code>argo submit</code>. Cela permet d&rsquo;éviter les problèmes de sécurité, comme le fait que votre utilisateur n&rsquo;ait pas le droit de créer des espaces de noms. Cela évitera également les messages d&rsquo;erreur vous avertissant de ne pas casser votre déploiement Kubernetes. Pour notre exemple, nous allons générer une valeur aléatoire sur une machine Linux et charger les données à venir en fonction de cette valeur.</p>
<p>Bien que les deux étapes d&rsquo;analyse syntaxique soient déclenchées simultanément, seule celle informée par l&rsquo;étape de traitement des demandes s&rsquo;exécutera. L&rsquo;utilisation d&rsquo;un code automatisé comme celui-ci réduira les chances d&rsquo;avoir des problèmes avec notre flux de données ETL. L&rsquo;automatisation de vos étapes de flux de travail gère les erreurs courantes telles que les types de mismatch dans votre base de données.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">apiVersion: argoproj.io/v1alpha1 
</span></span><span class="line"><span class="cl">kind: Workflow 
</span></span><span class="line"><span class="cl">metadata: 
</span></span><span class="line"><span class="cl">  generateName: stream-batch-parser-  
</span></span><span class="line"><span class="cl">  namespace: argo
</span></span><span class="line"><span class="cl">spec: 
</span></span><span class="line"><span class="cl">  entrypoint: sbp 
</span></span><span class="line"><span class="cl">  templates: 
</span></span><span class="line"><span class="cl">  - name: sbp 
</span></span><span class="line"><span class="cl">    steps: 
</span></span><span class="line"><span class="cl">    - - name: request-handler 
</span></span><span class="line"><span class="cl">        template: edge-input 
</span></span><span class="line"><span class="cl">    - - name: parser-stream 
</span></span><span class="line"><span class="cl">        template: stream 
</span></span><span class="line"><span class="cl">        when: <span class="s2">&#34;{{steps.handle-requests.outputs.result}} &lt;= 163883&#34;</span> 
</span></span><span class="line"><span class="cl">      - name: parser-batch 
</span></span><span class="line"><span class="cl">        template: parquet 
</span></span><span class="line"><span class="cl">        when: <span class="s2">&#34;{{steps.handle-requests.outputs.result}} &gt; 163883&#34;</span> 
</span></span><span class="line"><span class="cl">  - name: stream 
</span></span><span class="line"><span class="cl">    steps: 
</span></span><span class="line"><span class="cl">    - - name: avro-parser 
</span></span><span class="line"><span class="cl">        template: avro 
</span></span><span class="line"><span class="cl">    - - name: wrapper
</span></span><span class="line"><span class="cl">        template: wrapper 
</span></span><span class="line"><span class="cl">  - name: batch 
</span></span><span class="line"><span class="cl">    steps: 
</span></span><span class="line"><span class="cl">    - - name: parquet-parser 
</span></span><span class="line"><span class="cl">        template: parquet 
</span></span><span class="line"><span class="cl">    - - name: wrapper
</span></span><span class="line"><span class="cl">        template: wrapper 
</span></span><span class="line"><span class="cl">  - name: edge-input
</span></span><span class="line"><span class="cl">    container: 
</span></span><span class="line"><span class="cl">      image: alpine:3.6 
</span></span><span class="line"><span class="cl">      command: <span class="o">[</span>sh, -c<span class="o">]</span>
</span></span><span class="line"><span class="cl">      args: <span class="o">[</span><span class="s2">&#34;echo </span><span class="si">${</span><span class="nv">RANDOM</span><span class="si">}</span><span class="s2">&#34;</span><span class="o">]</span> 
</span></span><span class="line"><span class="cl">  - name: parquet 
</span></span><span class="line"><span class="cl">    container: 
</span></span><span class="line"><span class="cl">      image: alpine:3.6 
</span></span><span class="line"><span class="cl">      command: <span class="o">[</span>sh, -c<span class="o">]</span>
</span></span><span class="line"><span class="cl">      args: <span class="o">[</span><span class="s2">&#34;echo \&#34;code to parse Parquet\&#34;&#34;</span><span class="o">]</span> 
</span></span><span class="line"><span class="cl">  - name: avro 
</span></span><span class="line"><span class="cl">    container: 
</span></span><span class="line"><span class="cl">      image: alpine:3.6 
</span></span><span class="line"><span class="cl">      command: <span class="o">[</span>sh, -c<span class="o">]</span>
</span></span><span class="line"><span class="cl">      args: <span class="o">[</span><span class="s2">&#34;echo \&#34;code to parse Avro\&#34;&#34;</span><span class="o">]</span>
</span></span><span class="line"><span class="cl">  - name: wrapper
</span></span><span class="line"><span class="cl">    container: 
</span></span><span class="line"><span class="cl">      image: alpine:3.6 
</span></span><span class="line"><span class="cl">      command: <span class="o">[</span>sh, -c<span class="o">]</span>
</span></span><span class="line"><span class="cl">      args: <span class="o">[</span><span class="s2">&#34;echo \&#34;code to load into Staging\&#34;&#34;</span><span class="o">]</span> 
</span></span></code></pre></div><br />
Sauvegardez le fichier ci-dessus en tant que `etl_steps.yml` et démarrez votre flux de travail avec cette commande :
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">argo submit -n argo etl_steps.yml
</span></span></code></pre></div><p>Nous pouvons maintenant obtenir le statut de notre workflow en exécutant la commande <code>argo get</code> suivante :</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">argo get -n argo stream-batch-parser-xxvks
</span></span></code></pre></div><p>Les cinq derniers chiffres seront différents dans chaque environnement. Et en exécutant la commande précédente, votre journal de sortie devrait être similaire à l&rsquo;image ci-dessous ; comme indiqué, notre workflow exécutera la tâche de flux d&rsquo;analyseur en fonction de la valeur renvoyée par la tâche de traitement des demandes.</p>
<p>Sortie des étapes ETL du workflow Argo
Etapes ETL retournées par l&rsquo;exécution de la commande <code>argo get</code>.
Maintenant que nous avons vu comment construire un ETL avec des tâches, nous allons explorer comment utiliser les DAGs pour votre ETL.</p>
<h3 id="h2construire-un-pipeline-etl-avec-des-dags-au-lieu-détapes-h2br-"><h2>Construire un pipeline ETL avec des DAGs au lieu d&rsquo;étapes </h2><br /><a hidden class="anchor" aria-hidden="true" href="#h2construire-un-pipeline-etl-avec-des-dags-au-lieu-détapes-h2br-">#</a></h3>
<p>Maintenant, explorons comment réaliser le même travail en utilisant des modèles de DAGs au lieu d&rsquo;étapes dans Argo Workflows. Même si le DSL semble similaire à première vue, les DAGs vous donnent plus de pouvoir pour spécifier les dépendances entre les étapes et exécuter les tâches en parallèle.</p>
<p>Dans un DAG, toute tâche peut être exécutée lorsque ses dépendances sont satisfaites. Si les dépendances de plus d&rsquo;une tâche sont satisfaites, toutes les tâches seront exécutées en parallèle. Si une tâche n&rsquo;a pas de dépendances, elle s&rsquo;exécutera dès que le workflow sera lancé. Les DAGs sont excellents pour traiter l&rsquo;ETL, et je vous suggère fortement de vous familiariser avec toutes les options qu&rsquo;une tâche DAG peut fournir en consultant la <a href="https://argoproj.github.io/argo-workflows/fields/#dagtask">documentation officielle d&rsquo;Argo</a>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">apiVersion: argoproj.io/v1alpha1
</span></span><span class="line"><span class="cl">kind: Workflow
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">  generateName: dag-orchestrate-
</span></span><span class="line"><span class="cl">  namespace: argo
</span></span><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">  entrypoint: sbp
</span></span><span class="line"><span class="cl">  archiveLogs: <span class="nb">true</span>
</span></span><span class="line"><span class="cl">  templates:
</span></span><span class="line"><span class="cl">    - name: sbp
</span></span><span class="line"><span class="cl">      dag:
</span></span><span class="line"><span class="cl">        tasks:
</span></span><span class="line"><span class="cl">          - name: request-handler
</span></span><span class="line"><span class="cl">            template: edge-input
</span></span><span class="line"><span class="cl">          - name: stream-flow
</span></span><span class="line"><span class="cl">            template: stream
</span></span><span class="line"><span class="cl">            when: <span class="s2">&#34;{{tasks.handle-requests.outputs.result}} &lt;= 163883&#34;</span> 
</span></span><span class="line"><span class="cl">            depends: handle-requests
</span></span><span class="line"><span class="cl">          - name: batch-flow
</span></span><span class="line"><span class="cl">            template: batch
</span></span><span class="line"><span class="cl">            when: <span class="s2">&#34;{{tasks.handle-requests.outputs.result}} &gt; 163883&#34;</span> 
</span></span><span class="line"><span class="cl">            depends: handle-requests
</span></span><span class="line"><span class="cl">    - name: stream 
</span></span><span class="line"><span class="cl">      steps: 
</span></span><span class="line"><span class="cl">      - - name: avro-parser 
</span></span><span class="line"><span class="cl">          template: avro 
</span></span><span class="line"><span class="cl">      - - name: wrapper
</span></span><span class="line"><span class="cl">          template: wrapper 
</span></span><span class="line"><span class="cl">    - name: batch 
</span></span><span class="line"><span class="cl">      steps: 
</span></span><span class="line"><span class="cl">      - - name: parquet-parser 
</span></span><span class="line"><span class="cl">          template: parquet 
</span></span><span class="line"><span class="cl">      - - name: wrapper
</span></span><span class="line"><span class="cl">          template: wrapper             
</span></span><span class="line"><span class="cl">    - name: edge-input
</span></span><span class="line"><span class="cl">      container: 
</span></span><span class="line"><span class="cl">        image: alpine:3.6 
</span></span><span class="line"><span class="cl">        command: <span class="o">[</span>sh, -c<span class="o">]</span>
</span></span><span class="line"><span class="cl">        args: <span class="o">[</span><span class="s2">&#34;echo </span><span class="si">${</span><span class="nv">RANDOM</span><span class="si">}</span><span class="s2">&#34;</span><span class="o">]</span> 
</span></span><span class="line"><span class="cl">    - name: parquet 
</span></span><span class="line"><span class="cl">      container: 
</span></span><span class="line"><span class="cl">        image: alpine:3.6 
</span></span><span class="line"><span class="cl">        command: <span class="o">[</span>sh, -c<span class="o">]</span>
</span></span><span class="line"><span class="cl">        args: <span class="o">[</span><span class="s2">&#34;echo \&#34;code to parse Parquet\&#34;&#34;</span><span class="o">]</span> 
</span></span><span class="line"><span class="cl">    - name: avro 
</span></span><span class="line"><span class="cl">      container: 
</span></span><span class="line"><span class="cl">        image: alpine:3.6 
</span></span><span class="line"><span class="cl">        command: <span class="o">[</span>sh, -c<span class="o">]</span>
</span></span><span class="line"><span class="cl">        args: <span class="o">[</span><span class="s2">&#34;echo \&#34;code to parse Avro\&#34;&#34;</span><span class="o">]</span>   
</span></span><span class="line"><span class="cl">    - name: wrapper
</span></span><span class="line"><span class="cl">      container: 
</span></span><span class="line"><span class="cl">        image: alpine:3.6 
</span></span><span class="line"><span class="cl">        command: <span class="o">[</span>sh, -c<span class="o">]</span>
</span></span><span class="line"><span class="cl">        args: <span class="o">[</span><span class="s2">&#34;echo \&#34;code to load into Staging\&#34;&#34;</span><span class="o">]</span> 
</span></span></code></pre></div><p>Enregistrez le fichier ci-dessus en tant que <code>etl_dag.yml</code> et soumettez votre flux de travail pour le lancer :</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">argo submit -n argo etl_dag.yml
</span></span></code></pre></div><p>Comme démontré ci-dessous, vous pouvez vérifier son évolution avec argo get :</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">argo get -n argo dag-orchestrate-ctpkl
</span></span></code></pre></div><p>Dans ce scénario, notre workflow a exécuté la tâche de flux batch au lieu du flux stream en fonction de la valeur renvoyée par la tâche handle requests.
<br /></p>
<p><img loading="lazy" src="https://user-images.githubusercontent.com/78096758/217081047-efbc4d79-5f38-4e9f-8c2c-d00be60521ea.png#center" alt="etl examples"  />

<br /></p>
<p>Félicitations pour votre travail ! Vous pouvez maintenant concevoir vos flux de données ETL en utilisant un DAG ou une liste structurée d&rsquo;étapes dans Argo Workflows.</p>
<p>N&rsquo;oubliez pas de nettoyer votre environnement avec <code>argo delete -n argo your-workflow</code>, où vous devez informer le workflow désiré comme <code>votre-workflow</code>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">argo delete -n argo your-workflow
</span></span></code></pre></div><h3 id="h2conclusion-h2br-"><h2>Conclusion </h2><br /><a hidden class="anchor" aria-hidden="true" href="#h2conclusion-h2br-">#</a></h3>
<p>Bien qu&rsquo;il soit généralement utilisé pour la gestion de l&rsquo;infrastructure, Argo Workflows peut également orchestrer vos tâches ETL. En l&rsquo;utilisant ainsi, vous n&rsquo;avez plus besoin de recourir à différents outils pour atteindre le même objectif, c&rsquo;est-à-dire Argo pour le CI/CD et Airflow pour les tâches ETL.</p>
<p>L&rsquo;approche DAG est souvent meilleure que l&rsquo;approche par étapes pour l&rsquo;exécution des pipelines ETL. Pour commencer, le traitement des tâches DAG est optimisé au moment de l&rsquo;exécution. Vous aurez moins de points de décision pour certains de vos pipelines simplement en informant le flux de données souhaité.</p>
<p>Pour les tâches simples, les flux séquentiels (comme vous obtenez avec l&rsquo;approche par étapes dans les workflows Argo) fonctionnent bien. Cependant, ils deviennent plus difficiles à maintenir dans les cas où vous devez cibler un sous-ensemble de votre flux de données et gérer des dépendances complexes dans le temps.</p>
<p>Un autre avantage de l&rsquo;utilisation des DAGs est de pouvoir spécifier l&rsquo;étape exacte au moment de l&rsquo;exécution. L&rsquo;exécution vous donne plus de liberté pour créer un code conditionnel avec moins de boucles indentées tout en optimisant le code et les ressources de l&rsquo;infrastructure.</p>
<p>Je vous invite à approfondir la documentation d&rsquo;Argo Workflows sur les DAGs. Maîtriser le fonctionnement des DAGs peut augmenter la qualité de vos pipelines ETL, en vous permettant de gérer vos tâches ETL de manière plus dynamique par rapport à la méthode des étapes.</p>
<p>Pour des moyens plus optimisés de gérer vos ressources Kubernetes, explorez comment Pipekit peut vous aider à orchestrer l&rsquo;ensemble de votre déploiement Argo Workflows.</p>
<p>À+!</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://www.dawrlog.com/fr/articles/13/">
    <span class="title">« Page précédente</span>
    <br>
    <span>gRPC vs REST: Lequel dois-je choisir pour mon application et pourquoi?</span>
  </a>
  <a class="next" href="https://www.dawrlog.com/fr/articles/6/">
    <span class="title">Page suivante »</span>
    <br>
    <span>Les 10 optimales workflows Argo; avec examples.</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Examples des ETL utilisant des workflows Argo. on twitter"
        href="https://twitter.com/intent/tweet/?text=Examples%20des%20ETL%20utilisant%20des%20workflows%20Argo.&amp;url=https%3a%2f%2fwww.dawrlog.com%2ffr%2farticles%2f5%2f&amp;hashtags=Kubernetes%2cArgo%2cDevelopment%2cDataWarehouse">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Examples des ETL utilisant des workflows Argo. on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fwww.dawrlog.com%2ffr%2farticles%2f5%2f&amp;title=Examples%20des%20ETL%20utilisant%20des%20workflows%20Argo.&amp;summary=Examples%20des%20ETL%20utilisant%20des%20workflows%20Argo.&amp;source=https%3a%2f%2fwww.dawrlog.com%2ffr%2farticles%2f5%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Examples des ETL utilisant des workflows Argo. on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fwww.dawrlog.com%2ffr%2farticles%2f5%2f&title=Examples%20des%20ETL%20utilisant%20des%20workflows%20Argo.">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Examples des ETL utilisant des workflows Argo. on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fwww.dawrlog.com%2ffr%2farticles%2f5%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Examples des ETL utilisant des workflows Argo. on whatsapp"
        href="https://api.whatsapp.com/send?text=Examples%20des%20ETL%20utilisant%20des%20workflows%20Argo.%20-%20https%3a%2f%2fwww.dawrlog.com%2ffr%2farticles%2f5%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Examples des ETL utilisant des workflows Argo. on telegram"
        href="https://telegram.me/share/url?text=Examples%20des%20ETL%20utilisant%20des%20workflows%20Argo.&amp;url=https%3a%2f%2fwww.dawrlog.com%2ffr%2farticles%2f5%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://www.dawrlog.com/fr/">Logistique Dataware</a>; <br /> created with 🧡 on 🐧 machines running ☸️ pods.</span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a><script src='https://storage.ko-fi.com/cdn/scripts/overlay-widget.js'></script>
<script>
  kofiWidgetOverlay.draw('dawrlog', {
    'type': 'floating-chat',
    'floating-chat.donateButton.text': 'Tip jar',
    'floating-chat.donateButton.background-color': '#794bc4',
    'floating-chat.donateButton.text-color': '#fff'
  });
</script>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'Copier';

        function copyingDone() {
            copybutton.innerHTML = 'Copié !';
            setTimeout(() => {
                copybutton.innerHTML = 'Copier';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
