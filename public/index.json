[{"content":"Hey everyone, hope everything is well.\nEven the most potent database is still prone to the terrifying error any database system can face: query bottlenecks.\nSometimes the systems that manage our queries can\u0026rsquo;t process in the same way as we do. So our questions return accurate results as a result of extra processing power.\nIn this article, I\u0026rsquo;ll present how Redshift query queues work. We\u0026rsquo;ll also cover how concurrency works at a high level on Redshift.\nThis intel will become handy as we go further down the rabbit hole and oversee the deployment of a good—though sometimes threatening—friend, workload management (or WLM) on Redshift.\nWe\u0026rsquo;ll finish with a quick conclusion of what we saw and some techniques for query optimizations.\nQuery optimizations are very handy as it helps on cleaning bad code. Then increasing your transformations\u0026rsquo; speed, making it possible to explore more data with less. In short, good old data frugality.\nWhat is Redshift, and how does query processing work?\nRedshift is Amazon\u0026rsquo;s flavor of a massively parallel processing (or MPP) database system.\nThis kind of database is the top choice for storing data warehouses thanks to its capabilities of processing massive amounts of data in parallel. As for the other two major cloud providers, there\u0026rsquo;s BigQuery on Google Cloud Platform and Synapse Analytics on Microsoft Azure.\nNow let\u0026rsquo;s look more in-depth at the process of querying.\nQuery processing Once the query is started (either by the console or by pragmatic access—and both API and CLI commands follow the same way), it generates a query plan, which is the query translation made by the parser while extracting the data from the nodes.\nWith this execution blueprint, we can start to inspect the compute bottlenecks on your code.\nHere\u0026rsquo;s an example of an EXPLAIN Redshift command:\nexplain select lastname, catname, venuename, venuecity, venuestate, eventname, month, sum(pricepaid) as buyercost, max(totalprice) as maxtotalprice from category join event on category.catid = event.catid join venue on venue.venueid = event.venueid join sales on sales.eventid = event.eventid join listing on sales.listid = listing.listid join date on sales.dateid = date.dateid join users on users.userid = sales.buyerid group by lastname, catname, venuename, venuecity, venuestate, eventname, month having sum(pricepaid)\u0026gt;9999 order by catname, buyercost desc; From the EXPLAIN results above, I want to identify the tables: category, venue, sales, listing, date, and users. Each one of them uses the INNER JOIN clause.\nRedshift\u0026rsquo;s power relies on heavy processing, so the bigger those tables are, the better for you, computationally speaking.\nBut how can you know that? Going deeper, how can you identify the smaller tables for you to take the necessary metrics and measures so important for your KPIs?\nHere\u0026rsquo;s where the query plan comes in handy; below, you can see all the steps Redshift executes based on the SQL you wrote. So here\u0026rsquo;s where Redshift tells you if what you wrote is what Redshift understood.\nHere is the result of the above EXPLAIN command:\nQuery plan results.\nNow that we know how to create the query plan, we can go deeper into query optimization—which is nothing more than refactoring your queries to lower the processing costs described by the query plan steps.\nWhat is workload management, or WLM?\nOne of the faster ways of managing your query workflows is called workload management. With this feature on, you won\u0026rsquo;t sacrifice being able to answer quick questions due to long-running processes, as it enables flexibility while managing your workloads.\nLet\u0026rsquo;s imagine the following scenario:\nYour lead data scientist is deploying some machine learning models to detect possible fraudulent activities. Those activities need to be cross-referenced with the geographical location of the last transactions. Then those chaotic independent microservices start to run on your Redshift clusters at the exact time that your KPIs trigger new processes on the same Redshift cluster. Fun times, right?\nWLM comes to the rescue, as it creates what is called query queues at runtime. WLM groups these queues by a query group label defined by the user before the query execution.\nThese queues have concurrency levels, meaning the number of workloads started at the same time. Let\u0026rsquo;s get used to Workload Manager, or WLM\nWLM comes with two types of implementations; automatic where Redshift takes the charge of handling your query memory and concurrency allocation, and manual where you provide those values instead.\nBelow I want to share the helpful system tables and views that should be used as starting points when needing to enhance, or simply audit, your WLM workloads.\nSTL_WLM_ERROR STL_WLM_QUERY STV_WLM_CLASSIFICATION_CONFIG STV_WLM_QUERY_QUEUE_STATE STV_WLM_QUERY_STATE STV_WLM_QUERY_TASK_STATE STV_WLM_SERVICE_CLASS_CONFIG STV_WLM_SERVICE_CLASS_STATE The do\u0026rsquo;s and don\u0026rsquo;ts of concurrency scaling\nOnce you have your WLM turned on, you enable one handy feature used on massive processing: concurrent scaling! You will perceive the gains from the writing process into the target for consistency on higher throughput with multiple sessions requests.\nStill, when the feature is active, concurrency scaling is applied for both read and write operations. It also supports SQL Data Manipulation Language statements, the good ol\u0026rsquo; INSERT, DELETE and UPDATE. It also supports the CREATE statement and Redshift COPY; which gets you cover for most of your daily data loads.\nThe more critical limitations from concurrency scaling are its limitation of ANALYZE for the COPY commands and not being able to use COPY from Redshift Spectrum or when you\u0026rsquo;re querying data from your HDFS storage in your EMR clusters. Being that the COPY command is the suggested option to import massive data into native Redshift tables, not running it in parallel can limit the utilization of the COPY command for your use case.\nIt\u0026rsquo;s also relevant to remember that you need to confirm if the region where your Redshift cluster resides has the concurrency scaling feature; for a quick reference, take a look here.\nChange the Concurrency Scaling Mode option to auto on your WLM queue, and by doing so, you\u0026rsquo;ll enable the routing of your queries into the concurrency scaling clusters. Getting statistics for your WLM\nWith your WLM enabled and your queues with the \u0026ldquo;Concurrency Scaling Mode\u0026rdquo; turned to auto, you can track what\u0026rsquo;s going on in your cluster.\nTo do so, you can go to your query editor of choice or the Redshift console and execute the following query: Query for monitoring concurrent queues\nSELECT w.service_class AS queue , q.concurrency_scaling_status , COUNT( * ) AS queries , SUM( q.aborted ) AS aborted , SUM( ROUND( total_queue_time::NUMERIC / 1000000,2 ) ) AS queue_secs , SUM( ROUND( total_exec_time::NUMERIC / 1000000,2 ) ) AS exec_secs FROM stl_query q JOIN stl_wlm_query w USING (userid,query) WHERE q.userid \u0026gt; 1 AND q.starttime \u0026gt; # Time on format: \u0026#39;YYYY-MM-DD 24HH:MI:SS\u0026#39; AND q.endtime \u0026lt; # Time on format: \u0026#39;YYYY-MM-DD 24HH:MI:SS\u0026#39; GROUP BY 1,2 ORDER BY 1,2; The query results will provide you with a full spectrum of what\u0026rsquo;s happening on your cluster, granting all the necessary information from the Redshift cluster management perspective. All this information is helpful when investigating which sessions are active or not on your sets (you can check how the result would look like below).\nQuery results\nIn addition to the metrics collected by AWS CloudWatch and AWS CloudTrail, you\u0026rsquo;ll have a fully compliant environment. All of it is using native AWS services, saving you some extra headaches.\nThis configuration will work as a consistency layer in addition to your well-managed and mature data pipelines.\nConclusion\nIn this article, we oversaw how concurrency on Redshift works. We also looked at how to analyze and improve queries load plans: making them faster while consuming less processing power (you can interpret this as making your cloud bill smaller).\nJust remember the mentioned control tables and views, and you\u0026rsquo;ll be all set. Mastering them will also help while verifying any conflicts with the tables on the query.\nIt\u0026rsquo;s also a good idea to run the ANALYZE and VACUUM commands periodically.\nFor further help transforming your data, you can reach out to us with any questions you have and even discover new info you weren\u0026rsquo;t aware of with a better insight from better understanding your data.\nSee you guys next time!\n","permalink":"https://www.dawrlog.com/posts/9/","summary":"Hey everyone, hope everything is well.\nEven the most potent database is still prone to the terrifying error any database system can face: query bottlenecks.\nSometimes the systems that manage our queries can\u0026rsquo;t process in the same way as we do. So our questions return accurate results as a result of extra processing power.\nIn this article, I\u0026rsquo;ll present how Redshift query queues work. We\u0026rsquo;ll also cover how concurrency works at a high level on Redshift.","title":"Redshift Query Queues: The Complete Guide"},{"content":" These days good data is a need that any company can afford, and we help you to achieve it. At Dataware Logistics (aka Dawrlog), we are passionate about where good information can lead us. And fueled by it, we are focused on helping our clients with a clear goal: provide the best for them regarding data storage and consumption, compute architecture pipelines, application migration, and how to get the most out of it.\nWe count on professionals with more than a decade of data projects. Responsible for architecting new data flows from scratch, onboarding new ones into existing ones, and database migration. With more than a decade of delivering value for our customers, Dataware Logistics can guide you on the cost optimization of your existing stack by migrating it into the cloud. It could be in the early stages of your Proof of Concept or migrating your mature applications into the cloud.\nWe are focused on providing the foundations for the data practice for your company. We help you to get more for less by providing technical expertise on data ingestion, consumption, and distribution using cloud-native technologies. Our mission is to help your company with its data/infrastructure needs with the best technical stack you would need.\nOur professionals have more than a decade of experience in data analytics, infrastructure migrations, and data integration; within different business domains as we focus on what you can achieve with your data by providing resilient and reliable architectures based on your business requirements. We provide the best practices and quality work based on your technology choices, in case you have already chosen it. You do your thing, and we will help make sense of all those techy stuff.\nWe also help you build new expertise on technology trends by providing ad-hoc measured training for your internal teams, creating a real technology champion among your ranks. Here at Dawrlog, we understand that each individual has different learning approaches. With this in mind, we can provide additional training on other formats. We provide live enterprise training, video capsules, and technical whitepapers. We are keen to provide all the necessary information for your team to make the right technology choice confidently.\nAt Dataware Logistics, we support your company with your data and infrastructure logistics for your analytics platforms. That goes beyond migrating your systems into the cloud and advising on how this new data should be ingested by its nature and your adaptability to onboarding it. Or Our consultants have you are there to propose new ways of handling. Our team will help by guiding how to absorb it mellowly on your existing infrastructure.\nOur Expertise. Content Creation Technical posts Whitepapers Technical Enablement Live training Video capsules Professional Services Optimization of existing platforms Enterprise solutions for cloud Cloud/Data Migration Data Presentation Data Ingestion\nOur consultants have years of experience on the following domains. Retail Banking Insurance Telecom Digital Market Agencies\nOur clients trusts us on the following tech stack Modern Data Stack ","permalink":"https://www.dawrlog.com/about/","summary":"These days good data is a need that any company can afford, and we help you to achieve it. At Dataware Logistics (aka Dawrlog), we are passionate about where good information can lead us. And fueled by it, we are focused on helping our clients with a clear goal: provide the best for them regarding data storage and consumption, compute architecture pipelines, application migration, and how to get the most out of it.","title":"Hello, we are Dataware Logistics! Or simply Dawrlog."},{"content":"Hey everyone, hope everything is well.\nA data warehouse aims to make sense of a specific subject over time by analyzing historical data. This system, also called the decision-making support system, can tackle trends as diverse as the percentage of client churn or beer consumption trends within a geographical area.\nIncremental loading is one of those crucial issues you need to consider when defining your load pipelines.\nIn this post, I\u0026rsquo;ll explain what incremental loading is and why it\u0026rsquo;s so important. I\u0026rsquo;ll continue by defining some key components and end with a short conclusion\nWhat is incremental loading, and why do we need it? Also known as change data capture and delta load, the incremental load is responsible for taking a snapshot of the most recent data. It\u0026rsquo;s essential to make sure your data warehouse is consistent and reliable.\nWith that said, to understand the benefits of an incremental load, you need to understand what a data warehouse is.\nSo I\u0026rsquo;ll start with some basics on the data pipeline steps you need to take when loading your data warehouses databases. Doing so should give you a crisp and clear understanding of why it\u0026rsquo;s essential.\nPut into simple terms, data warehouses are databases that support the decision-making process. It is safer to say that it helps you to make sense of your historical data; as one of the DW goals is make it possible for its users to analyze data over time.\nBelow is a quick refresher of the most-used data warehouses phases under the Kimball approach: Data warehouse phases using Kimball\nOnline Transactional Processing (OLTP / Staging area / Landing Zone): Data is extracted from its source systems into transitional storage. This is data in its raw state, as close as possible to its source. Operational Data Stores (also known as ODS): A database with all necessary quality processes and correlations among different sources. The data here is in a state curated for users. Data Warehouse (also known as DW / EDW): Here data is stored in tables called dimensions or facts. Data redundancy is not an issue as it will show its evolution over time. Online Analytical Processing cubes (also known as OLAP / Presentation Layer): This is the final data that users access for their reports, with specific metrics as needed. You can\u0026rsquo;t (or at least you shouldn\u0026rsquo;t) have detailed information such as email addresses in your data warehouse. As the staging area is not ready for database transactions, you should interrogate your data in the early transactional stage for this type of request. The ODS works better for operational inquiries such as \u0026ldquo;who did what where?\u0026rdquo;\nAre you still with me? Great, it will be worth it!\nNow that we have our bases covered, we can move on to what you need to consider when designing your load structure.\nHow your data warehouse is loaded Every application with meaningful data for your data warehouse is called a data source.\nEach data source pump has its own constraints when loading data into a data warehouse, subdivided into two major groups: streaming and batch loads.\nWe use the streaming approach when data is available continuously. Picture your health tracker data coming from your smartwatch. We use batch techniques when your data source provides bulk data, such as a retailer application\u0026rsquo;s point-of-sale data. The second is more common when your source system has a specific loading time to avoid concurrency of internal processes.\nWhat is called micro-batch is a mixture of the two as it combines the continuous load approach with the periodicity found in the batch approach. Micro-batches are convenient when your data comes in a constant flow but not longer than ten minutes in smaller batches.\nNow that you have a high-level understanding of how your data warehouse gets filled, we can move on to the next step: identifying the business keys. What are business keys?\nEvery data source has what is called business keys. These keys help us identify what gives each record its uniqueness from a business perspective for that specific application.\nIt\u0026rsquo;s important to have a solid central repository where you can find all information about your data sources. A data catalog can assist here. It\u0026rsquo;s a central repository with all your data sources that you can explore.\nIt will help if you have business keys to identify unique records. Due to the high level of available details, the table below has bigger cardinality or high data granularity. Cardinality example:\nID Name Profession Address 1 Adam Smith Fireman 4389 Boul. Faraway App 6 2 John Smith Croupier 4389 Boul. Faraway App 1 3 Juliette Bull Saleswoman 64 Holest Nawy App 6 4 Lucille Croupier 1234 Runaway Train 5 Samantha Policewoman 4389 Galway Road We can use a combination of Name and Profession in this example. Those are the business keys or the fields that give unicity for business purposes.\nFor example, we could use the Identifier or ID as the unique primary key, but it won\u0026rsquo;t help much if it references a different record than those in the example above.\nOnce the business keys are clear, we can define our attributes and metrics: upon which we can base understand the data trends in our data sources, such as the relevant metrics and attributes that yield accurate insights. How do business keys relate to incremental keys?\nOnce you know how to identify your data source\u0026rsquo;s business keys, you must determine which field will fit your load\u0026rsquo;s incremental key. In addition to the business keys, the incremental key is responsible for loading only the new version of your data.\nBased on both business and incremental keys, you\u0026rsquo;ll correctly be able to load the last version of the data only. This version will update its existing version in the staging/ODS phase.\nDoing so will update your data warehouse with a new version of your record, leaving the previous version with a deprecated flag on it.\nIn this scenario, I am not talking about slowly changing dimensions. The business and integration keys are the ones that give us what we call the unicity of the record, or what makes it unique, creating what is called data versioning.\nData versioning tells us to differentiate between the record\u0026rsquo;s current state and its previous states. It enables something called temporal analysis of the record. The data versioning on source systems is expected to exist on the ODS phase of your data pipelines processes.\nConclusion\nIn this article, we covered incremental load and its importance for data integrity.\nWe started by presenting other names for it—the delta load and change data capture—and reviewed what comprises a data warehouse based on Kimball methodology, followed by the kinds of approaches for continuous or bulk data loads.\nWe ended by explaining business and incremental keys and distinguishing them from system keys.\nIt is always good to advise you to use the incremental load option whenever possible when interrogating your data. Doing so removes the duplicated views of your data snapshot, reducing the burden when managing your data ingestion pipelines.\nThe incremental load is strongly recommended (even mandatory) when defining and developing your data pipelines, especially in the ODS phase. It can help you load the data from your data sources correctly by loading your table properly and even scaling your data ingestion peak time. By splitting the data which gives these bottlenecks on your load processes into different pipelines.\nSee you guys next time!\n","permalink":"https://www.dawrlog.com/posts/8/","summary":"Hey everyone, hope everything is well.\nA data warehouse aims to make sense of a specific subject over time by analyzing historical data. This system, also called the decision-making support system, can tackle trends as diverse as the percentage of client churn or beer consumption trends within a geographical area.\nIncremental loading is one of those crucial issues you need to consider when defining your load pipelines.\nIn this post, I\u0026rsquo;ll explain what incremental loading is and why it\u0026rsquo;s so important.","title":"Incremental Loading: The Smarter Way to Update Data"},{"content":"Hello everyone, today we revisit the REST architecture, while comparing it to the gRPC calls.\nA microservices framework sometimes looks like a dream, but it can be a nightmare when processing data. Transiting the data with less latency is not enough nowadays. It also needs to easily connect with systems. And not only does the speed matter, but it also needs to scale to fit your computing needs while not bleeding your infrastructure costs. And the more an application is used, the harder it is to maintain it using a modular approach. Today I want to help you choose between gRPC and REST APIs as message pipelines between your services. I\u0026rsquo;ll start by presenting REST and which problems it can solve. Then I\u0026rsquo;ll dig into gRPC and some of its neat out-of-the-box capabilities and follow by talking about the upside of gRPC architecture. I want to end this post with a guide to choosing one or the other for your application. And I\u0026rsquo;ll show you which is better than the other when it comes to each point. I\u0026rsquo;ll end by summarizing what to keep in mind when architecting your microservices. In what consists the REST architecture?\nLet\u0026rsquo;s start with the well known REST architecture. The RESTful architecture follows a stateless pattern, which means that each service carries all its metadata within it. Its messages, also called payloads, are stored using JSON format.\nIn addition, it\u0026rsquo;s easier to catch unwanted requests from your clients using REST. The architecture uses http methods for its service interactions, and methods are components that follow the same CRUD operations found on relational database applications to retain your transactions indenpendency and data consistency.\nWhen you use these methods, your data remains consistent in every layer of your application. And thanks to those characteristics, REST calls can handle your resources maintanance in a smoother way, even where network connectivity could be an issue.\nRESTful applications are also capable of storing locally the most-used data locally on the client side; technique called caching. By doing so, REST services will access the most recurrent services, which results in reduced latency and bandwidth. It becomes faster since the requested data is locally stored while the session is active. In this case, your web service returns the cached version of your information gathered by a previous call. There are some security control procedures to mitigate some problems. These procedures are not only related to the persistent caching policy to avoid the usage of non-relevant data and manage the infrastructure and data protection for your data in transit and your data at rest. Take a look at the high-level RESTful architecture below.\nRESTFul web service architecture.\nI hope that works as an introduction to or refresher on how RESTful architecture works. Next, let\u0026rsquo;s explore the problems it can solve. Challenges We Can Solve with REST\nWe can start by stating that REST API has been around for a while, and it\u0026rsquo;s more mature than gRPC. Naturally, it has better support and higher-quality documentation since it has gone through multiple stress tests over the years. Providing you easier integration of new data sources into your data pipelines; allowing it to remain closer to your data sources. Reducing problems related to network within your ecosystem and data transit in and out of your data flows.\nUsing the REST framework, you must develop each publisher and subscriber code, which allows extra data transformation logic needed to transmit your data between services properly. Another perk comes from a more robust framework, making onboarding your team more manageable by using technologies proven to handle your workload. When your messages are in a human-friendly format like JSON, auditing your pipelines becomes less painful when a failure occurs.\nEach endpoint embeds its documentation in the code, which makes a REST API easier for humans to interact with. Its actions follow a straightforward structure, with activities described by verbs. It makes more sense, for example, that the method GET is used to access records. And REST APIs are remarkably well structured, helping their maintainers and users enhance existing applications. Now that we have seen what REST can do for us let\u0026rsquo;s look at gRPC. What Is gRPC?\nThe gRPC framework is an extension of remote procedure calls. In addition to using simple messages, this framework allows multiple message requests on the same connection. And it gets better with the support of bidirectional data streaming. This feature turns the gRPC into a powerful ally. You will be able to handle multiple independent, nonrelated threads.\nThe gRPC framework uses protocol buffers (or protobuf) under the hood, allowing evolutive schema support for your streaming data. You can avoid some of the trouble of having different JSON payloads from your sources as there is no need for client libraries to handle data cleansing jobs like auditing missing JSON fundamental values.\nIn addition to removing some of your business logic while extracting data from your API calls, protocol buffers are more optimal. Your data pipelines become more performant, and as a result, you can feel the network latency of your data ingestion channels lowered even with the smaller chunks of data. It becomes easier to correlate different data sources with adaptive client-server libraries generation based on your original protobuf file. gRPC connectivity overview.\nSee the Protocol Buffers language guide for further analysis on configuring your message. Next, let\u0026rsquo;s now look at what we can solve using gRPC. Challenges We Can Solve with gRPC.\ngRPC has one significant advantage: it can create one client for a couple of clients on different programming languages out of the box. That by itself makes the adoption of your sources more accessible from a development standpoint, and you won\u0026rsquo;t need a whole development pipeline for changes in your client libraries. By simply changing your proto file, the programming language or platform of your choice can easily replicate its changes. Not only that, but your proto file can even define message routing, removing the need to configure it on each of the producer or consumer codes.\nAnother attractive characteristic of gRPC is that it\u0026rsquo;s significantly faster than HTTPS and has lightweight message sizes compared to JSON. This makes it essential when different programming languages consume your service. Using gRPC, you won\u0026rsquo;t have to struggle as much when managing your communication. While this can make it quite tempting, a successful deployment will depend on whether or not your ecosystem is mature enough to enable it. Conclusion\nI covered communication protocols used on two microservice architectures, the gRPC and the REST frameworks. It may seem attractive to adopt gRPC since it\u0026rsquo;s faster and more quickly adopted, but you can create a lot of trouble for yourself when adjusting your processing pipelines. The main reason comes from how gRPC handles the messages. You can tackle this with the out-of-the-box protobuf JSON mapping, but you will still need to change your existing REST services so that they interact with the new gRPC services. Keep in mind that not every browser supports all gRPC capabilities, so it\u0026rsquo;s not an excellent user-facing framework.\nYou\u0026rsquo;ll need a well-designed technical blueprint to change your architecture from REST to gRPC. But you\u0026rsquo;ll get a better observability plan. Having your data in a non-human format can bolster your security as your data won\u0026rsquo;t be in plain text.\nIt is also good to reinforce that Neither approach will work well without a robust security analysis, it is a good idea to revisit your rules based on the metrics of your data traffic.\nFor further help transforming your data, you can reach out to us with any questions you have and even discover new info you weren\u0026rsquo;t aware of with a better insight from better understanding your data.\nSee you next time!\n","permalink":"https://www.dawrlog.com/posts/13/","summary":"Hello everyone, today we revisit the REST architecture, while comparing it to the gRPC calls.\nA microservices framework sometimes looks like a dream, but it can be a nightmare when processing data. Transiting the data with less latency is not enough nowadays. It also needs to easily connect with systems. And not only does the speed matter, but it also needs to scale to fit your computing needs while not bleeding your infrastructure costs.","title":"gRPC vs REST: Which Is Right for Your Application and Why?"},{"content":"This post explores using Argo Workflows to orchestrate your data pipelines. To start, let\u0026rsquo;s refresh on what ETL is while designing our work\u0026rsquo;s high-level architecture. I\u0026rsquo;ll demonstrate how to set up your data pipelines to follow the structure more naturally. Then, we\u0026rsquo;ll see how to achieve the same result using directed acyclic graphs (DAGs). Last, I\u0026rsquo;ll summarize what we saw and present reasons to guide your choice of approach based on your project complexity. So, let\u0026rsquo;s start.\nWhat Does ETL Stand for? First, let\u0026rsquo;s remember what ETL is before starting our examples. Extract, transform and load consists of tasks to clean your data, and it wrangles the data from your applications into a conformed database. Imagine this conformed database as the single source of truth of your data. This centralized repository helps you gain insights into your products and your customers.\nHowever, each application has its structure to handle the data. Each ETL task makes the application data more palatable for analysis, having explicit dependencies as your data wrangling process becomes more robust. Our code will deploy four ETL tasks and their relationship, as reflected by the image below:\nHigh-level architecture of ETL tasks\nUnderstanding Each ETL Task This workflow supports two different data formats: column or row-based. These different formats require different parsers: parquet for columns and avro for rows. We can also refer to them as batch or stream, respectively.\nSo, the workflow starts with a request handler that identifies the data type. Based on the value of a data type flag, it passes the job to either the batch or stream parsing tags.\nThe parsers pass their output to the Load Data task, where it is loaded in persistent storage, based on the source and data type.\nWhile it seems like this is a single workflow with two different code paths, there are advantages to processing both data types in the same set of tasks. Sharing a loading tasks between the data streams makes it easier to manage overlaps and relationships. For example, the columnar data store may need to be updated with foreign keys from new row-based data. It’s also easier to share a single ETL workflow between different teams\nNow that we know what we\u0026rsquo;ll be building, let\u0026rsquo;s start by seeing how to implement it using Argo Workflows steps.\nBuild an ETL Pipeline Using steps in Argo Workflows In this approach, the data pipeline will follow a list of steps to clean and treat the data from your data sources. Your ETL code becomes even more robust when we use conditionals to inform which ETL flow your data should take. Sounds good, right? So, let\u0026rsquo;s get our hands dirty and see how it works in practice.\nThe code below will create a workflow in a namespace called argo. This namespace must exist before this workflow is executed with argo submit. Doing so will avoid security issues, such as your user not having permission to create namespaces. It will also prevent error messages warning you not to break your Kubernetes deployment. For our example, we\u0026rsquo;ll generate a random value on a Linux machine and load the upcoming data based on this value.\nWhile both parser steps are triggered simultaneously, only the one informed by the handle requests step will execute. Using automated code like this will reduce the chances of having problems with our ETL data flow. Automation on your workflow steps handles common errors such as mismatch types in your database.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: stream-batch-parser- namespace: argo spec: entrypoint: sbp templates: - name: sbp steps: - - name: request-handler template: edge-input - - name: parser-stream template: stream when: \u0026#34;{{steps.handle-requests.outputs.result}} \u0026lt;= 163883\u0026#34; - name: parser-batch template: parquet when: \u0026#34;{{steps.handle-requests.outputs.result}} \u0026gt; 163883\u0026#34; - name: stream steps: - - name: avro-parser template: avro - - name: wrapper template: wrapper - name: batch steps: - - name: parquet-parser template: parquet - - name: wrapper template: wrapper - name: edge-input container: image: alpine:3.6 command: [sh, -c] args: [\u0026#34;echo ${RANDOM}\u0026#34;] - name: parquet container: image: alpine:3.6 command: [sh, -c] args: [\u0026#34;echo \\\u0026#34;code to parse Parquet\\\u0026#34;\u0026#34;] - name: avro container: image: alpine:3.6 command: [sh, -c] args: [\u0026#34;echo \\\u0026#34;code to parse Avro\\\u0026#34;\u0026#34;] - name: wrapper container: image: alpine:3.6 command: [sh, -c] args: [\u0026#34;echo \\\u0026#34;code to load into Staging\\\u0026#34;\u0026#34;] Save the file above as `etl_steps.yml` and start your workflow with this command: argo submit -n argo etl_steps.yml We can now get our workflow status by executing the following argo get command:\nargo get -n argo stream-batch-parser-xxvks ‍The last five digits will differ in each environment. And by running the previous command, your output log should be similar to the image below; as stated, our workflow will execute the parser stream task based on the value returned by the handle requests task.\nArgo workflow etl steps get output ETL steps returned by running argo get command Now that we\u0026rsquo;ve seen how to build an ETL with tasks, let\u0026rsquo;s explore how to use DAGs for your ETL.\nBuilding an ETL Pipeline with DAGs Instead of Steps Now, let\u0026rsquo;s explore how to achieve the same work using DAG templates instead of steps in Argo Workflows. Even though the DSL looks similar at first glance, DAGs give you more power to specify dependencies between steps and run tasks in parallel.\nIn a DAG, any task can run when its dependencies are satisfied. If more than one task’s dependencies are fulfilled, all of them will run in parallel. If a task has no dependencies, it will run as soon as the workflow is started. DAGs are excellent for processing ETL, and I strongly suggest you familiarize yourself with all options that a DAG task can provide by looking at the Argo official documentation.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: dag-orchestrate- namespace: argo spec: entrypoint: sbp archiveLogs: true templates: - name: sbp dag: tasks: - name: request-handler template: edge-input - name: stream-flow template: stream when: \u0026#34;{{tasks.handle-requests.outputs.result}} \u0026lt;= 163883\u0026#34; depends: handle-requests - name: batch-flow template: batch when: \u0026#34;{{tasks.handle-requests.outputs.result}} \u0026gt; 163883\u0026#34; depends: handle-requests - name: stream steps: - - name: avro-parser template: avro - - name: wrapper template: wrapper - name: batch steps: - - name: parquet-parser template: parquet - - name: wrapper template: wrapper - name: edge-input container: image: alpine:3.6 command: [sh, -c] args: [\u0026#34;echo ${RANDOM}\u0026#34;] - name: parquet container: image: alpine:3.6 command: [sh, -c] args: [\u0026#34;echo \\\u0026#34;code to parse Parquet\\\u0026#34;\u0026#34;] - name: avro container: image: alpine:3.6 command: [sh, -c] args: [\u0026#34;echo \\\u0026#34;code to parse Avro\\\u0026#34;\u0026#34;] - name: wrapper container: image: alpine:3.6 command: [sh, -c] args: [\u0026#34;echo \\\u0026#34;code to load into Staging\\\u0026#34;\u0026#34;] Save the file above as etl_dag.yml and submit your workflow to start it:\nargo submit -n argo etl_dag.yml As demonstrated below, you can check its evolution with argo get:\nargo get -n argo dag-orchestrate-ctpkl ‍In this scenario, our workflow executed the batch stream task instead of the stream-flow based on the value returned by the handle requests task.\nCongratulations on your work! You can now design your ETL data flows using a DAG or structured list of steps within Argo Workflows.\nDon\u0026rsquo;t forget to clean your environment with argo delete -n argo your-workflow, where you should inform the desired workflow as your-workflow.\nargo delete -n argo your-workflow Conclusion While it\u0026rsquo;s commonly used for infrastructure management, Argo Workflows can also orchestrate your ETL tasks. Using it like this removes the need for different tools to achieve the same goal, i.e. Argo for CI/CD and Airflow for ETL jobs.\nThe DAG approach is often better than the steps approach for running ETL pipelines. For starters, DAG task processing is optimized at runtime. You\u0026rsquo;ll have fewer decision points for some of your pipelines simply by informing the desired data flow.\nFor simple tasks, sequential flows (as you get with the steps approach in Argo Workflows) work fine. However, they become harder to maintain in cases where you need to target a subset of your data flow and manage complex dependencies over time.\nAnother perk of using DAGs is to specify the exact step at runtime. Running it gives you more liberty to create conditional code with fewer indented loops while optimizing the code and the infrastructure resources.\nI urge you to go deeper into Argo Workflows\u0026rsquo; documentation around DAGs. Mastering how DAGs work can increase the quality of your ETL pipelines, allowing you to manage your ETL tasks more dynamically compared to the steps method.\nFor more optimized ways of managing your Kubernetes resources, explore how Pipekit can help you orchestrate your whole Argo Workflows deployment.\nSee you next time!\n","permalink":"https://www.dawrlog.com/posts/5/","summary":"This post explores using Argo Workflows to orchestrate your data pipelines. To start, let\u0026rsquo;s refresh on what ETL is while designing our work\u0026rsquo;s high-level architecture. I\u0026rsquo;ll demonstrate how to set up your data pipelines to follow the structure more naturally. Then, we\u0026rsquo;ll see how to achieve the same result using directed acyclic graphs (DAGs). Last, I\u0026rsquo;ll summarize what we saw and present reasons to guide your choice of approach based on your project complexity.","title":"Argo Workflow ETL Examples."},{"content":"We all know how Argo Workflows makes it easy to orchestrate parallel jobs on Kubernetes. While it’s most often associated with data processing and ETL projects, it’s useful for a lot more! These 10 workflows will change the way you see this Kubernetes orchestrator.\nLet’s dive in!\nArgo Workflows Setup\nIf you don\u0026rsquo;t currently have a workflow running, I suggest you create your first Argo Workflow to understand what we\u0026rsquo;ll discuss in this post. To do so, follow the instructions here to create a local Argo Workflows deployment on your cluster. I also suggest using k3d for your local Kubernetes control plane; this tutorial uses a k3d cluster named argo. Feel free to reproduce the command below to create it in your environment:\nk3d cluster create argo kubectl create -n argo -f https://raw.githubusercontent.com/argoproj/argo-workflows/master/manifests/quick-start-postgres.yaml kubectl -n argo port-forward deployment/argo-server 2746:2746 \u0026amp; Now let\u0026rsquo;s jump into looking at our first example!\n1. Enhancing Your Workflow Using Parameters\nArgo uses custom resource definitions stored on YAML files to manage its deployments. So no need to learn new specs to manage your infrastructure; you can follow the same pattern used on your Kubernetes and Kustomize scripts, which helps you remain consistent. Below we can see how to use parameters on your workflows, and passing parameters is handy when your configuration uses runtime values. As a result, you will know some components only after creating them, such as access tokens.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: hello-world-parameters- spec: entrypoint: whalesay arguments: parameters: - name: message value: Message string default value templates: - name: whalesay inputs: parameters: - name: message container: image: docker/whalesay command: [cowsay] args: [\u0026#34;{{inputs.parameters.message}}\u0026#34;] In our template, the parameter message will have the default value of Message string default value. However, this value can be overwritten at runtime, as we can see by running the command below:\nargo submit -n argo param.yml -p message=\u0026#34;This ran from your container\u0026#34; --watch We can validate the output from the Argo Workflows Logs UI. (You can access the UI by default at https://localhost:2746/ if you quickly follow the port forwarding instructions while creating your cluster.)\n2. Pulling Images From Your Secured Repository\nOne of the features I like when automating an ecosystem is using rotational access keys while managing my services\u0026rsquo; access. This is useful in cases where your company uses private container repositories to host your container images. Argo Workflows helps you achieve this with the native support of Kubernetes secrets. In our example, we can see that the secret docker-registry-secret will pull the image docker/whalesay:latest.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: hello-world- spec: entrypoint: whalesay imagePullSecrets: - name: docker-registry-secret templates: - name: whalesay container: image: docker/whalesay:latest command: [cowsay] args: [\u0026#34;hello world\u0026#34;] 3. Using Sidecar Containers One of my favorite things to do is to use sidecars while starting my pods. Kubernetes sidecars are useful helpers that can handle recurring tasks, such as syncing your Git repositories with git-sync as shown here. Argo Workflows has this covered with neat support for sidecar containers out of the box.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: sidecar-nginx- spec: entrypoint: sidecar-nginx-example templates: - name: sidecar-nginx-example container: image: appropriate/curl command: [sh, -c] args: [\u0026#34;until `curl -G \u0026#39;http://127.0.0.1/\u0026#39; \u0026gt;\u0026amp; /tmp/out`; do echo sleep \u0026amp;\u0026amp; sleep 1; done \u0026amp;\u0026amp; cat /tmp/out\u0026#34;] sidecars: - name: nginx image: nginx:1.13 To deploy it, save the above code as sidecar-nginx.yml and submit it:\nargo submit -n argo sidecar-nginx.yml --watch And as a result, you\u0026rsquo;ll deploy an NGINX\u0026rsquo;s reverse proxy sidecar instance.\n**Pro-tip: You might need to pay some extra attention to your workflows if you’re using Istio as Service mesh. Check out at this GitHub thread if you\u0026rsquo;re thinking of using it. **\n4. Archiving Your Current Workflow State on Persistent Storage\nWorkflow Archive is a nice feature that Argo Workflows provides so you can have previous workflow states stored on a relational database (Postgres or MySQL for now). However, Argo\u0026rsquo;s archive won\u0026rsquo;t keep detailed execution logs; you\u0026rsquo;ll need to configure an Object storage as artifact repository, it can be an opensource option like MinIO, or AWS S3; to mention a cloud provider option from AWS.\nTo use the archive feature, you\u0026rsquo;ll first need to configure your Argo server\u0026rsquo;s persistent storage option. You\u0026rsquo;ll need more information about how to configure it to do so. Following this link will help you with the authentication piece required for the Argo archive; then base your configuration on Controller configmap. You\u0026rsquo;ll need to have them appropriately configured with your Argo server to benefit from this feature. Once it\u0026rsquo;s configured, you can store your workflows with the spec.archiveLocation.archiveLogs as demonstrated below.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: archive-location- spec: entrypoint: whalesay templates: - name: whalesay container: image: docker/whalesay:latest command: [cowsay] args: [\u0026#34;hello world\u0026#34;] archiveLocation: archiveLogs: true 5. Passing a Git Repository as an Input Artifact\nAnother cool feature that Argo Workflows provides out of the box is the possibility to sync your Git repository without the need for extra sidecars or init containers. The code below connects to the Argo repository on Github. You can choose from HTTP or SSH pull requests for the authentication piece. In the first template, git-clone, you\u0026rsquo;ll need to use the combination of usernameSecret and passwordSecret Kubernetes secrets to access a URL in its HTTP format. You can see an example of an HTTP Git configuration in the code below.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: input-artifact-git- spec: entrypoint: git-clone templates: - name: git-clone inputs: artifacts: - name: argo-source path: /src git: repo: https://github.com/argoproj/argo-workflows.git revision: \u0026#34;v2.1.1\u0026#34; usernameSecret: name: github-creds key: username passwordSecret: name: github-creds key: password container: image: golang:1.10 command: [sh, -c] args: [\u0026#34;git status \u0026amp;\u0026amp; ls \u0026amp;\u0026amp; cat VERSION\u0026#34;] workingDir: /src Argo Workflows also supports SSH connectivity (e.g., git@github.com:argoproj/argo-workflows.git). However, it needs the URL format following the SSH connectivity and the sshPrivateKeySecret Kubernetes secret instead of the usernameSecret and passwordSecret ones.\n6. Creating Directed Acyclic Graph Workflows\nI feel the directed acyclic graph (DAG) is now getting the attention it deserves on the analytics domains because of how it impressively handles data processing workload steps on Apache Spark and its use as a common data orchestration pattern with Apache Airflow. With Argo Workflows, you\u0026rsquo;ll have a Kubernetes-friendly interface instead of the need to configure a Kubernetes executor for Airflow which is less stable.\nI suggest checking this link to learn more about how a DAG works. Below, you can see how Argo Workflows instantiates it.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: dag-target- spec: entrypoint: dag-target arguments: parameters: - name: target value: E templates: - name: dag-target dag: target: \u0026#34;{{workflow.parameters.target}}\u0026#34; tasks: - name: A template: echo arguments: parameters: [{name: message, value: A}] - name: B depends: \u0026#34;A\u0026#34; template: echo arguments: parameters: [{name: message, value: B}] - name: C depends: \u0026#34;A\u0026#34; template: echo arguments: parameters: [{name: message, value: C}] - name: D depends: \u0026#34;B \u0026amp;\u0026amp; C\u0026#34; template: echo arguments: parameters: [{name: message, value: D}] - name: E depends: \u0026#34;C\u0026#34; template: echo arguments: parameters: [{name: message, value: E}] - name: echo inputs: parameters: - name: message container: image: alpine:3.7 command: [echo, \u0026#34;{{inputs.parameters.message}}\u0026#34;] Each task will be passed to the Argo server using the target parameter name, with the target names separated by spaces. Argo Workflows will execute only the ones you specify; however, it\u0026rsquo;ll run each dependency until it reaches the informed targets. In plain English, say we save our file as dag-targets.yml and execute using the following command:\nargo submit -n argo dag-targets.yml -p target=\u0026#34;B E\u0026#34; --watch It will skip only target D, as demonstrated below:\nArgo Workflows DAG execution results\n7. Execute Python Scripts\nContainers already make it easy to manage runtime environments. So, it’s easy to build a Python container with the libraries and version you need for your Python-based workflow steps.\nWith Argo Workflows you can call a Python script that’s already installed on the container by name, or pass in code via a source field in workflow description. You can specify any valid code in the source block.\nHere’s an example:\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: name: python-random-number-generator namespace: argo spec: entrypoint: generator arguments: parameters: - name: min value: 0 - name: max value: 10 templates: - name: generator inputs: parameters: - name: min - name: max script: image: python:3.8 command: [python] source: | from random import randrange range_min = {{inputs.parameters.min}} range_max = {{inputs.parameters.max}} random_number = randrange(range_min, range_max) print(\u0026#34;Random number: {}\u0026#34;.format(random_number)) 8. Implementing a Retry Strategy\nSometimes, multiple targets can implement some retry logic, and Argo Workflows configures your retry strategy on the Workflow level.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: retry-container- spec: entrypoint: retry-container templates: - name: retry-container retryStrategy: limit: \u0026#34;3\u0026#34; retryPolicy: \u0026#34;OnError\u0026#34; container: image: python:alpine3.6 command: [\u0026#34;python\u0026#34;, -c] # fail with a 66% probability args: [\u0026#34;import random; import sys; exit_code = random.choice([0, 1, 1]); sys.exit(exit_code)\u0026#34;] In our example, the target retry-container will try to restart three times in the cases that it finishes with an Error status on Kubernetes.\n9. Adding Conditional Workflows Conditional workflows are also among my favorites and are so simple to implement. You can deploy your architecture based on the return statuses of previous steps, which is very handy when orchestrating a set of containers. Argo Workflows grants you the possibility of executing targets based on a boolean condition. Under the hood, it uses govaluate to allow you to use Golang\u0026rsquo;s expr statements.\nSo you\u0026rsquo;ll be able to orchestrate your conditions in the same way you handle your Golang helpers on your Kubernetes ecosystem—another nice extra benefit of using Kubernetes CRDs.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: conditional-parameter- labels: workflows.argoproj.io/test: \u0026#34;true\u0026#34; annotations: workflows.argoproj.io/version: \u0026#39;\u0026gt;= 3.1.0\u0026#39; spec: entrypoint: main templates: - name: main steps: - - name: flip-coin template: flip-coin - - name: heads template: heads when: \u0026#34;{{steps.flip-coin.outputs.result}} == heads\u0026#34; - name: tails template: tails when: \u0026#34;{{steps.flip-coin.outputs.result}} == tails\u0026#34; outputs: parameters: - name: stepresult valueFrom: expression: \u0026#34;steps[\u0026#39;flip-coin\u0026#39;].outputs.result == \u0026#39;heads\u0026#39; ? steps.heads.outputs.result : steps.tails.outputs.result\u0026#34; - name: flip-coin script: image: python:alpine3.6 command: [ python ] source: | import random print(\u0026#34;heads\u0026#34; if random.randint(0,1) == 0 else \u0026#34;tails\u0026#34;) - name: heads script: image: python:alpine3.6 command: [ python ] source: | print(\u0026#34;heads\u0026#34;) - name: tails script: image: python:alpine3.6 command: [ python ] source: | print(\u0026#34;tails\u0026#34;) Saving the above code as cond.yml and executing with argo submit will give the following output:\nargo submit -n argo cond.yml --watch 10. Managing Kubernetes Resources From Your Workflow Argo Workflows can create Kubernetes components; this is very handy when you need to develop temporary kubelet actions in a declarative way. This feature follows the same principle of the inline scripts to deploy Kubernetes components responsible for applying patches to your environment. However, Argo Workflows handles this code\u0026rsquo;s Kubernetes CRD YAML inline files.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: k8s-jobs- spec: entrypoint: pi-tmpl templates: - name: pi-tmpl resource: action: create successCondition: status.succeeded \u0026gt; 0 failureCondition: status.failed \u0026gt; 3 manifest: | apiVersion: batch/v1 kind: Job metadata: generateName: pi-job- spec: template: metadata: name: pi spec: containers: - name: pi image: perl command: [\u0026#34;perl\u0026#34;, \u0026#34;-Mbignum=bpi\u0026#34;, \u0026#34;-wle\u0026#34;, \u0026#34;print bpi(2000)\u0026#34;] restartPolicy: Never backoffLimit: 4 This feature covers you as you directly run all kubectl actions, which allows you to create/update/delete any Kubernetes resource on your cluster using inline Kubernetes API groups definitions.\nConclusion\nThe advances we’ve seen in systems management and development give us many reasons to be optimistic. For instance, infrastructure as code allows you to have the same infrastructure on your scalable servers and your local workstation. Tools like Argo Workflows help us create scalable production-ready infrastructure on our local workstation, and that by itself is something to cherish.\nWith constant infrastructure requirement changes such as dynamic DNS, you need to adapt your deployments to a more modular approach. These workflows are the must-haves for any DevOps admin. But this list is only the beginning. I would highly suggest implementing these scripts in your development and data pipelines.\nUntil next time!\n","permalink":"https://www.dawrlog.com/posts/6/","summary":"We all know how Argo Workflows makes it easy to orchestrate parallel jobs on Kubernetes. While it’s most often associated with data processing and ETL projects, it’s useful for a lot more! These 10 workflows will change the way you see this Kubernetes orchestrator.\nLet’s dive in!\nArgo Workflows Setup\nIf you don\u0026rsquo;t currently have a workflow running, I suggest you create your first Argo Workflow to understand what we\u0026rsquo;ll discuss in this post.","title":"Top 10 Argo Workflows Examples."},{"content":"Hello everyone,\nToday I would like to cover how to debug your Argo workflows, this comes handy when you need more details from your environment. Let’s check the two different approaches to debugging your Argo Workflow deployments.\nFirst, we’ll use the Argo Command Line Interface, or CLI for short. Argo’s CLI commands cover many of the more common errors you see with workflows, such as misconfigurations. Then, we’ll see how to debug workflows using the Argo Workflows UI. After we’re finished with Argo’s native tools, we’ll wrap up with how to debug your environment using kubectl.\nThe Argo Workflows Command Line Interface\nArgo Workflows command line interface (CLI) makes interactions with your cluster simple. Below I will present how to run Argo commands directly from your Kubernetes cluster on a namespace called argo. Remember that you can quickly adapt the code shown below to your environment. To reproduce it in your environment, change the option to -n namespace where namespace is the Kubernetes namespace where you have Argo deployed.\nWorkflow Status with argo list I want to cover two commands: argo list and argo get. With argo list you can quickly identify the status of all the workflows deployed on the supplied Kubernetes namespace. And you can take a close look at each workflow with the argo get command. While both controls aren\u0026rsquo;t dependent on each other, their combination should answer most of your questions about running pods. I suggest using the -A flag as shown below. Doing so will widen the search of all your namespaces.\nargo list -A As you can see, the command returns in a tabular format an updated status of all workflows in your environment by namespace. Having it presented like this is useful for automated health checks of your services. In cases where you want to focus your search on a specific namespace, the argo list command can also look for a single namespace thanks to the -n tag. This is useful when you have similar workflows deployed on different namespaces, a widespread scenario when managing multi-tenant environments. So, in cases like these, it\u0026rsquo;s best to run argo list -n , where is the audited namespace.\nAuditing Your Workflows with kubectl\nThere are scenarios where the Argo CLI output doesn\u0026rsquo;t provide enough information for complete analysis of a pod.\nFor example, in some cases, your Argo server won\u0026rsquo;t be able to access your token key, and It would be helpful to see how your control plane resolved this value. To do so, you can use kubectl commands to explore the configuration and health of your Argo Workflows instance. If dependent services aren\u0026rsquo;t accessible, this method will help you.\nArgo Workflows deploys three containers in each pod, and all are accessible using kubectl commands, as mentioned before.\nGetting your Argo deployment details with Kubernetes native commands Here we’re looking at the k8s cluster as a whole. Using the command below, you can retrieve high-level info about the health of your deployed services.\nkubectl get svc,po -n argo -o wide This presents you with information about the k8s control plane for your argo namespace. From here, you can use kubectl describe for more detailed information.\nArgo and Kubernetes use the same pod name for their deployed components. Bear that in mind if you plan to automate your pipelines with a mix of kubectl and Argo Workflows CLI combinations for your observability strategy.\nSo, use kubectl describe to view how Kubernetes sees your pod deployment. This should resemble what you see on the Argo console UI.\nKubectl generates a lot of output, so pipe it through more or less.\nkubectl describe -n argo pod/dag-sum-pm8rp-2964331963 | less It is good to note that the pod name will be the same in both kubectl and Argo CLI commands. You won\u0026rsquo;t have any surprises when choosing which one you prefer to use in your analysis as long as you use the same pod name. So, running kubectl logs -n argo pod/dag-swtgb-320908401 -c main or argo logs -n argo dag-swtgb dag-swtgb-320908401 -c main; will then print the audit trace of your main container inside the Kubernetes pod dag-swtgb-320908401 but using different command-line interfaces.\nYou can also explore the init and wait containers the same way as using the Argo workflow CLI commands. Though slightly different, it will return the detailed data from Kubernetes deployments. It\u0026rsquo;s a personal choice whether you want to use kubectl or Argo native commands.\nMore Detail with argo get\nOnce you have the service you need and its namespace, you can see more details with the argo get command. Executing the following command will give you a consolidated message per step of the Argo workflow. In our example, the workflow dag-swtgb exists on the argo namespace.\nargo get -n argo dag-swtgb Running the same command without the -o yaml option will return an output like the one below with a more consolidated view. It\u0026rsquo;s helpful to see the message produced by your problematic pods.\nArgo get command output with errors\n‍Go Deeper with argo logs Argo deployments share the workflow deployments of every pod over the main, init, and wait containers. I will cover how to access them using kubectl in a bit, but you can also access them using Argo logs on the desired pod.\nI should also point out that if your pod didn\u0026rsquo;t start, those containers wouldn\u0026rsquo;t start either.\nYou can follow a workflow’s logs with argo logs.\nConsider this command line session:\nargo -n argo submit sum.yaml and its output:\nName: dag-sum-gm5sv Namespace: argo ServiceAccount: unset (will run with the default ServiceAccount) Status: Pending Created: Tue Apr 05 12:53:30 -0400 (now) Progress: This workflow does not have security context set. You can run your workflow pods more securely by setting it. Learn more at https://argoproj.github.io/argo-workflows/workflow-pod-security-context/ When you submit a new workflow, Argo gives you its name.\nPass that name to argo logs with the namesapce and the –-follow option:\nargo -n argo logs dag-sum-gm5sv --follow Output:\ndag-sum-gm5sv-899595302: time=\u0026#34;2022-04-05T16:53:32.791Z\u0026#34; level=info msg=\u0026#34;capturing logs\u0026#34; argo=true dag-sum-gm5sv-899595302: 2 dag-sum-gm5sv-508066804: time=\u0026#34;2022-04-05T16:53:33.072Z\u0026#34; level=info msg=\u0026#34;capturing logs\u0026#34; argo=true dag-sum-gm5sv-508066804: 2 dag-sum-gm5sv-844819: time=\u0026#34;2022-04-05T16:53:42.984Z\u0026#34; level=info msg=\u0026#34;capturing logs\u0026#34; argo=true dag-sum-gm5sv-844819: 4 Argo will echo the logs to the screen as the workflow progresses.\nIf you don’t want to use the command line, you can do this via the Argo Workflows UI, too.\nView Your Argo Workflows Events in the Console UI\nLast, but not least; you can also debug your Argo environment using the console UI it provides. This service is accessible by following one of the steps mentioned in their docs, but in this case, we will do a simple port forward between the Kubernetes deployment and the host. The code presented here can run flawlessly on Linux and macOS machines. These environments allow you to bind the port between your workstation and your Kubernetes cluster as a background process with this command: kubectl -n argo port-forward svc/argo-server 2746:2746 \u0026amp;. With the service accessible from the host, you can point any web browser to the address https://localhost:2746.\nConclusion\nYou saw how to debug Argo Workflows components using the Argo CLI and Argo UI, or kubectl commands. The steps I described here can help you understand what\u0026rsquo;s happening in your environment.\nSee you next time!\n","permalink":"https://www.dawrlog.com/posts/4/","summary":"Hello everyone,\nToday I would like to cover how to debug your Argo workflows, this comes handy when you need more details from your environment. Let’s check the two different approaches to debugging your Argo Workflow deployments.\nFirst, we’ll use the Argo Command Line Interface, or CLI for short. Argo’s CLI commands cover many of the more common errors you see with workflows, such as misconfigurations. Then, we’ll see how to debug workflows using the Argo Workflows UI.","title":"Ways to Debug an Argo Workflow."},{"content":"Hello everyone, I hope you are doing well!\nIf you’re familiar with Argo Workflows, you already know that it can drive your CI/CD pipelines, manage your ETL processes, and orchestrate any set of tasks you can imagine for a Kubernetes cluster. But did you know that Argo knows how to archive the results of its workflows to a SQL database, too?\nIn this post, I\u0026rsquo;ll show how Argo Workflows archives workflow state into persistent storage using a Postgres database. To do so, I\u0026rsquo;ll present a quick summary of Argo components while showing what it means to have your workflow archived. We\u0026rsquo;ll deploy Argo Workflows alongside a Postgres database on a local Kubernetes instance using k3d. Finally, we’ll discuss some important security considerations for your Argo Workflows deployment.\nSo, let\u0026rsquo;s get started.\nWhat is the Archive Option in Argo Workflows? The ability to store past workflow runs provides you with an accurate record of your past workflow states. This blueprint is a game changer, as it makes it possible to provision your task sets based on real-time metrics, such as spikes in processing needs from past deployments.\nThe workflow archive option stores your workflow states in either MySQL or Postgres. Once you have archives configured, you can use them to better understand how your jobs run and where you might be able to improve.\nFor example, they can help you know when it\u0026rsquo;s a good idea to scale your traffic with the help of temporary instances, which will also have their states stored on the same database. With all your states held over time, you can apply rules to adjust your cluster size based on previous usage; a good time series analysis could even save you some money in the end.\nThe archive stores only the previous workflow states, not their detailed instance logs. Another thing to keep in mind is detailed audit logs. The artifact store option handles the detailed logs persistent option, storing it locally by [MinIO](https://min.io/). But you can also configure any other object storage option. This is covered in the [Argo official documetation](https://argoproj.github.io/argo-workflows/configure-artifact-repository/), where you can see how to use options such as [Google Cloud Storage](https://cloud.google.com/storage) buckets or [AWS S3](https://aws.amazon.com/s3/). But before we start on the technical implementation, let\u0026rsquo;s have a quick refresher on the components of Argo Workflows. It\u0026rsquo;s necessary to know how they correlate with the persistent storage for your archived workflows; this image from the Argo Workflows documentation presents an overview of the environment where a workflow resides:\nSource: Argo Workflow Github repository\nHow to Deploy Argo Workflows with Persistent Storage\nNow that we know what\u0026rsquo;s in store for us let\u0026rsquo;s get started. We\u0026rsquo;ll be using k3d to manage our local Kubernetes environment (instead of minikube and VirtualBox). In addition to k3d, you\u0026rsquo;ll need to install Docker as an additional dependency. Using kubectl to interact with your Kubernetes cluster works fine, too. As for our tutorial, we\u0026rsquo;ll be using local Kubernetes deployment scripts.\nFirst, we\u0026rsquo;ll start our local control plane with the following command:\nk3d cluster create cluster-demo The successful creation will provide a log similar to this one:\nCreating the cluster\nOnce we have our `cluster-demo`, we'll deploy our Argo Workflows instance. To install Argo Workflows, you'll need to execute the following commands: kubectl create ns argo kubectl apply -n argo -f https://raw.githubusercontent.com/argoproj/argo-workflows/master/manifests/quick-start-postgres.yaml The first one creates a namespace called argo in your cluster, and the following line will deploy the Argo Workflows components on your cluster, as you can see below:\nDeployments made on the cluster\nCreating the workflow-controller To run the workflow with the archive option, you must first change the persistence configuration to archive:true on your Argo server deployment. Changing it will tell your Argo server to store your workflow execution states into the database reported by the key postgresql.\nWe\u0026rsquo;ll apply a new ConfigMap into our current Kubernetes argo namespace with the Postgres instance to store your archived workflows. You can then archive your workflows by using the archiveLogs option.\nWe had a Postgres instance deployed with the Quickstart YAML we used earlier. You\u0026rsquo;ll need it only to apply the following configuration to your deployment. Changing this configuration enables your Argo server deployment to accept the archiveLocation.archiveLogs notation while creating your workflows. We\u0026rsquo;ll start by creating a new workflow-controller-configmap.yml with the following content and saving it locally:\napiVersion: v1 kind: ConfigMap metadata: name: workflow-controller-configmap data: persistence: | connectionPool: maxIdleConns: 100 maxOpenConns: 0 connMaxLifetime: 0s nodeStatusOffLoad: true archive: true archiveTTL: 7d postgresql: host: postgres port: 5432 database: postgres tableName: argo_workflows userNameSecret: name: argo-postgres-config key: username passwordSecret: name: argo-postgres-config key: password retentionPolicy: | completed: 10 failed: 3 errored: 3 Deploy Your Environment with kubectl We\u0026rsquo;ll expose the Argo Workflows web UI using a load balancer on our argo namespace. The load balancer will expose the pod executing the web-facing component to connections made from outside Kubernetes.\nkubectl apply -n argo -f workflow-controller-configmap.yml Your Argo server will restart with the new configuration in a couple of minutes. Feel free to check its status by running kubectl get -n argo svc,pod on your Kubernetes cluster.\nkubectl get -n argo svc,pod You can then bind your Kubernetes cluster and your host to port 2746 by running the following on your cluster:\nkubectl -n argo port-forward deployment/argo-server 2746:2746 \u0026amp; Congratulations, you just deployed Argo Workflows on a k3d cluster. To confirm that your local instance is up and running, go to https://localhost:2746.\nArgo Workflows user info UI page Testing Your Deployment\nCongratulations on installing your Argo Workflows instance on your local Kubernetes cluster with the archive option. And now that we have checked that off our list, let’s archive our workflows. Adding the archiveLogs annotation lets you specify which ones you want to archive, as demonstrated in the following template, which we\u0026rsquo;ll call workflow-archive.yml.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: archive-location- spec: archiveLogs: true # enables logs for this workflow entrypoint: whalesay templates: - name: whalesay container: image: docker/whalesay:latest command: [cowsay] args: [\u0026#34;hello world\u0026#34;] We need to execute argo submit -n argo --watch -f workflow-archive.yml on a terminal to deploy it.\nargo submit -n argo --watch -f workflow-archive.yml By doing so, you\u0026rsquo;ll start the archive-location workflow under the argo namespace; the following output confirms that our example ran successfully:\nArgo Workflows archive example run output log It doesn\u0026rsquo;t change on the command line; however, as we have persistent storage for our workflows, you can see their previous states on the console UI. That\u0026rsquo;ll give you the previous workflow states that ran with the archive options enabled—and going to the Argo Workflows console UI at https://localhost:2746, as we saw before, you can access the archived workflow UI option from the left menu bar’s icons. Once you are there, you can see all the past executions of a workflow. Your workflow history can be found in the UI under “Archived Workflows” (see below).\nArgo Workflows archived workflow console Security Best Practices for Archiving Argo Workflows in Postgres\nIn our work, we deployed an Argo instance with the archive option configured with a Postgres database. As mentioned previously, this code isn\u0026rsquo;t production-ready. As a next step, I suggest managing your access tokens to secure your Argo instance.\nA good practice is to avoid hardcoded values for server runtime information when possible. Your infrastructure should generate data like your Postgres hostname on runtime instead of having it hardcoded. Your infrastructure should use secrets to store sensitive information like repository access keys.\nPostgres Archiving\nTake a look at this introduction about Secrets and configmaps in Kubernetes, for more details on what Kubernetes information should be discreet. Adopting security best practices like this in the early stages is easier for both your users and developers as you start to scale. In addition, having your configuration automated ends up narrowing the attack surface of your environment while also reducing infrastructure management tasks.\nHere’s a helpful blog post with more Argo security best practices from the Argo Workflows maintainer, Alex Collins.\nConclusion\nWe deployed Argo Workflows locally and archived a workflow using a Postgres database in this post. The scripts here are good starting points to understand and experiment with the archive option of Argo Workflows, but keep in mind that some critical factors are missing for a fully cloud native environment.\nSee you guys next time!\n","permalink":"https://www.dawrlog.com/posts/2/","summary":"Hello everyone, I hope you are doing well!\nIf you’re familiar with Argo Workflows, you already know that it can drive your CI/CD pipelines, manage your ETL processes, and orchestrate any set of tasks you can imagine for a Kubernetes cluster. But did you know that Argo knows how to archive the results of its workflows to a SQL database, too?\nIn this post, I\u0026rsquo;ll show how Argo Workflows archives workflow state into persistent storage using a Postgres database.","title":"Archiving Argo Workflows: Postgres Database Setup"},{"content":"It\u0026rsquo;s frustrating how a project\u0026rsquo;s delivery dates are sometimes so tight that you neglect the quality of your development application. Thanks to weak security policies, it\u0026rsquo;s even worse when your security team can\u0026rsquo;t catch the damage before it\u0026rsquo;s too late. To help you with that, I want to talk about targeted DDoS attacks and what they do to your application programming interface, also known as API endpoints. I\u0026rsquo;ll explain what DDoS is and the problems it can cause. Then, I\u0026rsquo;ll explain how to check out whether you got attacked using Wireshark. It\u0026rsquo;s one of the best-known network analyzers on the market. I\u0026rsquo;ll then show how you can narrow your environment\u0026rsquo;s attack surface. After that, I\u0026rsquo;ll end with a recap of what we analyzed. So, let\u0026rsquo;s first understand what DDoS is and what it can do to your API endpoints. What Is a DDoS, and How Does It Affect Your API Requests? Let\u0026rsquo;s go over some concepts related to API and explore what a DDoS attack means. DDoS stands for distributed denial of service. It consists of stuffing your network connection to your services. These requests, made on layer 7 of the OSI model, are called non-valid. Layer 7 is also known as the application layer, which floods your server with ghost requests, which in turn creates what is called a zombie network. So there are cases where your machine hosts attack a targeted server without your consent. This is another reason to ensure that even your home computer\u0026rsquo;s security is more robust. Now that we\u0026rsquo;ve reviewed a DDoS, let\u0026rsquo;s go over what an API request is. There are some other types of attacks targeting API requests, as shown by this Github repo. DDoS attacks focus not only on the server where your API is running but also on each endpoint of your API service. Your API service is attacked on both the server and the API service itself in more advanced attacks. This yields drastic results for the health of your API server in the case of a successful attack. With that said, let\u0026rsquo;s check how to identify a compromised network with Wireshark, a network analyzer. Spotting Compromised Network Traffic With Wireshark Wireshark is a handy tool for your network forensics. It\u0026rsquo;s also a versatile tool that you should have under your belt if you\u0026rsquo;re serious about getting into the detail of your traffic. Let\u0026rsquo;s look at an example of a compromised network. On our example we will go to this link to access a Wireshark log named sec-sickclient.pcapng as demonstrated below.\nsec-sickclient.pcapng wireshark log\nThe log confirms that the requests made by the IP 10.129.211.13 on port 1047 can\u0026rsquo;t reach the server 216.234.235.165 on port 18067. The first thing to notice is the unusual port number. DDoS attacks usually target non-regular ports. The attacker\u0026rsquo;s goal is to flood the server with non-valid requests, which will be concurrent with valid ones. Another tip while checking the validity of API calls is to verify whether the checksum is correct. On the extract, you can see that the checksum of an invalid request is incorrect—invalid requests like this one flood the server, which becomes unresponsive. Now that you understand what a DDoS attack is and how to track it down, let\u0026rsquo;s see some approaches that help mitigate your services\u0026rsquo; attack surface. We\u0026rsquo;ll start by mitigating your attack surface by filtering your upstream traffic requests. The Filtered Upstream Requests Approach There are ways to filter your requests. I prefer the content delivery network, or CDN for short. The CDN hides your application\u0026rsquo;s source code while serving the application layer data with its cached content. It works as a security upstream defense option by filtering requests on your applications and helping your users with low latency data cached. You can have third-party tools offering CDN solutions, such as AWS CloudFront. Still, it\u0026rsquo;s good to have a minimal response plan before reaching your ISP providers. Having your user-facing services accessing your web content, such as videos and music, on secured cached storage can also help. This approach filters the traffic before it reaches your network, which smooths the management of your servers. But this approach still needs something extra to protect you if your environment is discovered and compromised. Here\u0026rsquo;s where a honeypot can help.\nThe Honeypot Approach I find that your environment is the best data source for your mitigation plan. You\u0026rsquo;ll have accurate data from your attacks with a malware honeypot that could mock both your front-end and back-end environments. Your honeypot can work as a rat trap if you deliberately leave some vulnerabilities open for attackers to exploit. It\u0026rsquo;s a risky game, as your honeypot must be identical to your production environment. Otherwise, you\u0026rsquo;ve invited your attackers to explore your environment. But when deployed correctly, it becomes a powerful tool to secure your domain. A good honeypot can also show how well your defense systems are stopping attacks. Another perk is that it shows which data must have more security measures. Even with an exposed honeypot, your network can suffer without the excellent management of your API requests. To make sure you\u0026rsquo;re covered in this respect, you can limit your network resources. Limiting Your Network Resources You can configure your network interface controller to handle maximum traffic per session. What\u0026rsquo;s known as rate limiting can be done by either software or hardware. While the first manages the number of concurrent calls, the latter will handle your switch and router configurations. Rate limiting your network resources grants you the certainty of having your application in a healthy state, despite some users experiencing higher latency from your attacked services. A good response plan comes with multiple security layers. Now, we\u0026rsquo;ll see how you can benefit from a content delivery network with a honeypot. How a Malware Honeypot and a Content Delivery Network Can Improve Your Defenses As mentioned before, the CDN will serve your application-layer content, covering only one part of your security plan. You can benefit by having a honeypot as a first attack surface, and it should be in a controlled environment where your application resides. Your security plan should use a mix of services focused on different application domains, and the security guidance principle reinforces the security of interconnected parts. So, combining your CDN and a malware honeypot can help your team apply the response plan in place, mitigating the slowness and nonavailability of your services. It\u0026rsquo;ll then give you enough time to reiterate your degraded benefits more securely, without opening new threats. Let\u0026rsquo;s review by checking the topics that we covered today. Conclusion DDoS attacks make your environment unstable, and the attacks do that by firing service calls to a targeted service with non-valid requests. Although there are many types of DDoS attacks, we often overlook the ones focusing on the health of your API services. I suggest revisiting the OWASP API security hints. Depending on your services\u0026rsquo; data flow and accessibility, you can adopt extra measures. The idea is to narrow your attack surface. However, you don\u0026rsquo;t want to build a black box. The security and usability of your components must be balanced for your service\u0026rsquo;s adoption by your developers and users.\nSee you all next time!\n","permalink":"https://www.dawrlog.com/posts/3/","summary":"It\u0026rsquo;s frustrating how a project\u0026rsquo;s delivery dates are sometimes so tight that you neglect the quality of your development application. Thanks to weak security policies, it\u0026rsquo;s even worse when your security team can\u0026rsquo;t catch the damage before it\u0026rsquo;s too late. To help you with that, I want to talk about targeted DDoS attacks and what they do to your application programming interface, also known as API endpoints. I\u0026rsquo;ll explain what DDoS is and the problems it can cause.","title":"How to Mitigate DDoS Attacks on Your APIs."},{"content":"Hey everyone, today I would like to talk about Bigquery, the datawarehouse managed service by Google Cloud. And I would like to explore its datatypes.\nGoogle Cloud Platform has long been the cloud provider of choice for web analytics, with the impressive BigQuery being Google\u0026rsquo;s massively parallel processing (aka MPP) option. MPP databases are very common on Hadoop ecosystems.\nBigQuery is very friendly to its users (let\u0026rsquo;s agree that BigQuery is as pleasant as its SQL knowledge), which fosters new users. In part, thanks to a more detailed exploration of Google Analytics data, as described on this example.\nThis article aims to present BigQuery and its existing data types per SQL categories. I\u0026rsquo;ll also give one practical use case where knowing these data types can be helpful: data verification when ingesting data.\nKnowing which kind of BigQuery data type to use will help you create your data pipelines more easily. Once you better understand what data types are available, the quality of your reports will drastically increase in no time. SQL types in BigQuery\nBigQuery operates using structured query language (SQL), subdivided into legacy and standard categories.\nIt\u0026rsquo;s good to point out that each subgroup has different data types and ways of questioning the database. When the hint is not informed, BigQuery will use standard SQL functions and methods. Check out what I am talking about at SQL hints if you are new to it and want to learn a little more about them.\nTo present what a legacy query looks like, I\u0026rsquo;ll show a running example of a SQL command using the legacy category type.\nSQL hint example\n#legacySQL -- This hint informs that the query will use Legacy SQL SELECT weight_pounds, state, year, gestation_weeks FROM [bigquery-public-data:samples.natality] ORDER BY weight_pounds DESC; Standard SQL is the type that you should be aiming to use when you don\u0026rsquo;t have any constraints, such as legacy code on your lift-and-shift deployment.\nStandard SQL also allows very nifty features like window functions and a more robust user-defined functions structure, and it\u0026rsquo;s more flexible to create on standard SQL rather than legacy SQL.\nIt\u0026rsquo;s also recommended, as best practice, to migrate your existing legacy SQL into standard SQL.\nGoogle even offers a helpful page with guidance on this conversion. You can check out how to start it correctly at this page.\nStandard SQL data types The below table presents the possible data types when using the standard SQL category. I like to point out that standard SQL is the preferred category by Google, which means you can correctly assume that it has better performance and more options when compared to the legacy SQL category.\nBigquery Standard SQL Datatypes Standard SQL data typesStandard SQL data types\nThe standard SQL category accepts more complex data types such as ARRAY, STRUCT, and GEOGRAPHY data types. Any of the mentioned data types can order (or group) the results of any dataset; one of its severe limitations as the data types STRUCT and ARRAY are heavily used on streaming data ingestions.\nAnother example comes from the ARRAY data type not comparing its values with any field, which happens in STRUCT and GEOGRAPHY.\nWe can use ST_EQUALS as a \u0026ldquo;workaround\u0026rdquo; of comparing geographical values directly.\nAll other data types can filter SQL JOIN clauses and be used to order the results; always remember to cast the columns used to avoid unwanted effects explicitly.\nStandard SQL also allows the usage of what is called stored procedures.\nThe stored procedure allows the execution of repeatable functions—very useful for shareable business logic transformations between different departments.\nFor example, how the human resources department calculates profit benefits could be useful for the marketing department for calculating campaigns.\nThe benefit of well-defined data formats starts with your stored procedures—as options on the earlier stage of your analytics pipelines empower your analysts with a shorter reaction time to analyze your data.\nLegacy SQL data types\nLegacy SQL uses the same data types as those used with standard SQL, excluding ARRAY, STRUCT, and GEOGRAPHY.\nThe NUMERIC type fields provide limited support, making it necessary to use an explicit conversion using the cast function when interacting with these fields.\nThe table below lists all the possible data types available when using the legacy SQL query category.\nYou can still access nested data using the dot notation, but it doesn\u0026rsquo;t allow nice tricks like generating your array on the runtime.\nLegacy SQL will enable you to create reusable, shareable functions between different queries. It grants this possibility with user-defined functions (or UDFs); below, you have an example of a simple one.\nUDF on legacy SQL\n// UDF definition function urlDecode(row, emit) { emit({title: decodeHelper(row.title), requests: row.num_requests}); } // Helper function with error handling function decodeHelper(s) { try { return decodeURI(s); } catch (ex) { return s; } } // UDF registration bigquery.defineFunction(\u0026#39;urlDecode\u0026#39;, // Name used to call the function from SQL [\u0026#39;title\u0026#39;, \u0026#39;num_requests\u0026#39;], // Input column names // JSON representation of the output schema [{name: \u0026#39;title\u0026#39;, type: \u0026#39;string\u0026#39;},{name: \u0026#39;requests\u0026#39;, type: \u0026#39;integer\u0026#39;}], urlDecode // The function reference ); So, since it has fewer available data types and some limitations like not being able to create shareable business logic as the standard SQL category does, the legacy SQL category is not a viable option. Validating the target data type when inserting.\nTo have a better understanding of the data types, let\u0026rsquo;s look at some code.\nWe\u0026rsquo;ll tackle the insert statement in the standard SQL category since it\u0026rsquo;s the suggested category by the documentation, with a focus on the STRUCT data type. This data type can be challenging and very common when ingesting data from REST API payloads.\nAlso, I believe you might get bored with manipulations with Integers and Strings only. The following command inserts some data into the table DetailedInventory under the schema dataset.\nThe following SQL statement, written using standard SQL, will insert values on the mentioned table with some STRUCT types.\nInsert statement\nINSERT dataset.DetailedInventory VALUES (\u0026#39;top load washer\u0026#39;, 10, FALSE, [(CURRENT_DATE, \u0026#34;comment1\u0026#34;)], (\u0026#34;white\u0026#34;,\u0026#34;1 year\u0026#34;,(30,40,28))), (\u0026#39;front load washer\u0026#39;, 20, FALSE, [(CURRENT_DATE, \u0026#34;comment1\u0026#34;)], (\u0026#34;beige\u0026#34;,\u0026#34;1 year\u0026#34;,(35,45,30))); As simple as demonstrated, it inserts without any complexity (you can see how the inserted records look below).\nInsert results When interacting with your data, you need to be aware of handling each data type properly.\nOne common mistake comes from comparing the time and time stamp data formats without the correct care. Although the two data types might resemble one another a lot, this mistake can result in inaccurate datasets.\nAlso, confirm that the function that you\u0026rsquo;re using is under the right Bigquery SQL category.\nOne good example is the cast function under legacy SQL and its reference under standard SQL, so know your terrain before making any changes to your code.\nConclusion\nAs we saw, the type of SQL used can block the usage of some data types.\nDue to different ways of processing the data, there is a significant difference between legacy SQL and standard SQL, making things far more complex than a simple \u0026ldquo;hint\u0026rdquo; at the beginning of the SQL.\nUtilizing the correct data type can help you audit your data in its earlier stages. This could mean adding an automated reaction to insufficiently formatted data, saving your production support team some investigation. As the same response could take hours to identify the root problem, in addition to the time it takes to solve it once identified.\nSometimes you may even need to apply some predefined rules to treat your data based on learned processing problems.\nStandard SQL should be preferred over the use of legacy SQL.\nThe latter doesn\u0026rsquo;t have cool tricks such as windowing functions or better lexical support when creating your SQL statements (way better than the simple dot notation from legacy SQL).\nThis intel is killer when analyzing why your data still has bottlenecks during its ingestion.\nFor further help transforming your data, you can reach out to us with any questions you have and even discover new info you weren\u0026rsquo;t aware of with a better insight from better understanding your data.\nSee you guys next time!\n","permalink":"https://www.dawrlog.com/posts/10/","summary":"Hey everyone, today I would like to talk about Bigquery, the datawarehouse managed service by Google Cloud. And I would like to explore its datatypes.\nGoogle Cloud Platform has long been the cloud provider of choice for web analytics, with the impressive BigQuery being Google\u0026rsquo;s massively parallel processing (aka MPP) option. MPP databases are very common on Hadoop ecosystems.\nBigQuery is very friendly to its users (let\u0026rsquo;s agree that BigQuery is as pleasant as its SQL knowledge), which fosters new users.","title":"BigQuery Data Types Examined \u0026 Explained"},{"content":"Hello everyone, today we will check how to properly value stream your DataOps initiative.\nEnhancements on data ingestion made evident the amount of data lost when generating insights. However, without guidance from methodologies like The DataOps Manifesto, some companies are still struggling to blend data pipelines from legacy databases, such as an ADABAS database, with new ones, such as MongoDB or Neo4j.\nAnd it’s always good to point out that these legacy systems aren’t easy to let go of. Most of them are responsible for core processing (like the bank transactions in Cobol, for example). As such, it’s not attractive to change them because it’s too problematic.\nWith that said, I believe the integration of new applications and legacy databases is one of the biggest challenges for data-driven companies. However, there have been significant advancements in the methodologies and frameworks for data ingestion over the last few years. Why Is It So Essential to Have Precise Data? With optimized data ingestion pipelines, you can become better at decision-making. At the same time, updating your subject-focused legacy systems into modular applications sounds promising, right?\nThe good news is, doing so isn’t a far-fetched dream! Thanks to advancements around computing power, it’s now possible to do way more with very little. We can now deconstruct bloated applications into slimmer versions.\nUsing concepts that used to be possible only in theory, we can now ingest data from unusual places. For example, we can have an automated pipeline to ingest data from digitized documents thanks to Optical Character Recognition, or OCR. How cool is that?\nIn this post, I want to help you understand how your organization will benefit from a mature DataOps environment. First, I’ll present what DataOps is and why it’s not only DevOps for data. Then, I’ll explain some significant benefits that come from mapping the value streams for your own DataOps practice. Last, I’ll present some considerations when applying the DataOps framework to your team. What Is DataOps? It’s hard to provide your users with the most accurate metrics for existing dashboards with multiple data suppliers. It gets worse when your legacy database has so much technical debt that carrying out something as simple as creating a Key Process Indicator, or KPI, is an absolute nightmare.\nAnd here’s where DataOps comes to the rescue! It makes it possible for your users to consume your data faster and in an automated environment integrated with the most updated version of your data suppliers. Pretty neat, right?\nI like to remember that this isn’t something new, as the decision support system (DSS) consists of an automated environment for your analytics pipelines. And I believe that DSSs always played an essential part in any company. The benefits for stakeholders include getting a complete understanding of your supply chain process or knowing faster which department or product is more profitable, to list a few.\nThe data suppliers load these systems, which can be any source of information for your needs. Some examples of information provided by data suppliers include\nHuman resources data Agency campaign data Supply chain IoT devices Billing systems Server and traffic logs DataOps vs. Decision Support Systems. You often see orchestrated data pipelines from different data suppliers in legacy DSSs. So, each data supplier loads its data into this centralized database. But these independent data flows cause inconsistent data, making it hard to trust the DSS’s presented results.\nWhat DataOps facilitates is better results with an optimized DSS! Thanks to the agile approach, DataOps reinforces a more customer-centric approach when compared to DSS because of its modular architecture. In addition, DataOps eases constraints on scalability, automation, and security thanks to the reuse of components used by other data pipelines.\nThese components can vary from simple database connectivity to a business rule used by the finance department that human resources could benefit from. All of this is thanks to repositories that are centralized, standardized, and reusable. Where DataOps Intersects and Where It Strays From DevOps\nWhile DataOps seems like DevOps for data, I like to remember that the goals of DevOps and DataOps are different, although the methodologies share the same principles.\nDevOps focuses on loosening development to include operations. On the other hand, the goal of DataOps is to make analytics development more functional. DataOps uses statistical process control, the mathematical approach used in lean manufacturing, DevOps disciplines, and best practices from the Agile Manifesto.\nWith these strategies in place, you can decouple your environment; DataOps makes it easier to detach your business logic and technical constraints from your data pipelines. As a result, you’re more confident in your data while enabling your teams to react better to data trends.\nYou’ll benefit from the efficiency and results faster while designing your data pipeline components. And your teams will be more focused on delivering value rather than being tied down by technical decisions made in the past.\nAlso, I always like to bring up the security gains resulting from proper deployment of DataOps: robust, secure systems that are less prone to breaches and leaks! Benefits to Consider While Mapping Your Value Streams\nNow that we know what DataOps is about, I want to present the benefits of correctly mapping your value streams.\nMy focus here is the benefits for your data pipelines, although they can also apply to your application deployments. From the business perspective, the principal value added by DataOps adoption is the ability to explore new data sources rapidly.\nHere are the advantages of the value added to the customer when mapping your value streams. Enhanced Control Thanks to Technical Decoupling.\nAs I mentioned before, data suppliers are any source applications with relevant data for your analysis. In other words, they’re data entry points that feed your data pipeline.\nThese applications produce data in a raw state. And as the name implies, this data should be left as untouched as possible. It’s beneficial in cases of reprocessing or data lineage analysis.\nThis data needs additional processing steps to remove unnecessary noise from it, as its original form might not be a good fit for your needs. However, from this output, you can extract the necessary metrics to cover the needs of your users.\nI also want to bring up one of the values from conscious decoupling: its robust automated control of each component’s data pipeline. This orchestration brings up extra quality measures, making it possible to increase productivity given that your users won’t need to perform repetitive tasks. Fast Exploration of Existing and New Data Suppliers.\nOn legacy systems, the development of new pipelines is chaotic, as previously mentioned. The DataOps approach also enables fast exploration of your data suppliers.\nDataOps makes it easier to create conformed components thanks to its modular deployment. What I mean by that is that you can reuse already deployed and tested components.\nIn other words, DataOps enables a mindset of continuous improvement. Therefore, it will drastically lower your development costs and increase your return on investment at the same time.\nYour team will handle more challenging tasks on bringing business value, not with daily data wrangling activities. As a result, you get an instant productivity boost thanks to the automated deployment of your application. Reliable Data Governance.\nAs a result of the automated data pipelines deployed by DataOps, it becomes easier to trace how your users consume your data. This information can help you quickly answer recurring questions.\nYour users can see where the business logic is in a blink of an eye. Moreover, the reference between its canonical and technical implementation name becomes easy to absorb since new analysts getting onboarded to your projects makes it an appealing source for your data profiling.\nAs a result, a solid datalog of your data suppliers is a mandatory step to think about when mapping your value streams, in my opinion. It becomes easy to manage your data pipelines when you create an enterprise-level conformed business catalogue. All this structured information about your data suppliers creates an intuitive data catalogue.\nThis information, also known as metadata, provides the business context where this data gives its value. So, in other words, your insights would become more accurate. Important Considerations About Your Own DataOps Deployment.\nWhat we’ve seen so far shows how conformed detached modules enable a more robust data catalogue for your company. In addition, the coherence among your analytics components clarifies where you can enhance your data ingestion.\nI always like to remember that these improvements don’t come for free. As you’ll react faster and more wisely, be ready to reshape some of your company’s internal processes. Bear in mind that it’s hard to teach an old dog new tricks. So, expect some resistance from more experienced teammates.\nA self-healing data pipeline scales horizontally or vertically when needed. So, you can add more units for processing power (known as horizontal scaling) or enhance your clusters (known as vertical scaling) when your sets start to have some bottleneck issues while processing your data; Remember the rule of thumb:\nIt gets easier to enrich your outputs when you’re fully aware of your data scope. So, in addition to its modular components, DataOps enables actions like masking and obfuscating your data to occur more fluidly in the early stages of its processing.\nWith DataOps, you’re building up a trustworthy data pipeline capable of reacting to new concepts and technologies at the same speed as your business’s evolution. Final Thoughts.\nIn this post, I gave an overview of what you’ll gain by correctly deploying DataOps. The result is a mix of business and technical added value because you’ll have slim and robust orchestrated data pipelines.\nI started by presenting what DataOps is and what business value it brings. Then, I explained where it intersects and where its goals differ from agile and DevOps methodologies.\nWe also took a quick look at what I believe to be the short-term benefits from the correct deployment of a mature DataOps implementation and how automated deployments can add technical value.\nLast, we saw some challenges that your team may face when you adopt DataOps. For example, your unit may be resistant to adopting new technologies and methodologies. However, we also saw the benefits of a correct deployment as a concise data catalogue.\nJust remember that you must completely implement DataOps’s requirements. So, don’t expect to have a reliable DataOps implementation with partial deployments of agile or DevOps disciplines.\nFor further help transforming your data, you can reach out to us with any questions you have and even discover new info you weren\u0026rsquo;t aware of with a better insight from better understanding your data.\nSee you next time!\n","permalink":"https://www.dawrlog.com/posts/12/","summary":"Hello everyone, today we will check how to properly value stream your DataOps initiative.\nEnhancements on data ingestion made evident the amount of data lost when generating insights. However, without guidance from methodologies like The DataOps Manifesto, some companies are still struggling to blend data pipelines from legacy databases, such as an ADABAS database, with new ones, such as MongoDB or Neo4j.\nAnd it’s always good to point out that these legacy systems aren’t easy to let go of.","title":"How to Value Stream DataOps?"},{"content":"Hello everyone, today I want to revisit what is Identity and Access provisioning lifecycle phases.\nWith so much data being generated every second, it makes sense that data leakages are one of the biggest nightmares of any data-driven company. These days, we learn fast that weak security policies can be costly. This article will present one of the techniques to maintain your data secured: the identity and access provisioning lifecycle.\nTo help you understand this lifecycle, I\u0026rsquo;ll present the action steps of its two states categories: provisioning and de-provisioning. I\u0026rsquo;ll start by identifying the tasks and explaining what \u0026ldquo;identity and access provisioning\u0026rdquo; means. Then I\u0026rsquo;ll explain which actions to take to correctly achieve those states, from understanding the needs and requesting access for your identities (the provisioning state) or removing the unnecessary ones (the de-provisioning state).\nI\u0026rsquo;ll follow by introducing how to implement the access provisioning lifecycle. You\u0026rsquo;ll get a look at the access provisioning models and how different provisioning models deploy the concepts I introduced in the previous paragraph. I\u0026rsquo;ll describe how different types of access provisioning models work. Let\u0026rsquo;s start with the basics. What Is the Identity and Access Provisioning Lifecycle?\nIn every company, there are multiple systems with different sorts of data. Some data may be in the public domain. Other data requires different privacy rules. That\u0026rsquo;s because its exposure could cause some problems, such as having your competitors become aware of your next steps.\nIt\u0026rsquo;s essential to correctly onboard any new team member with the correct privileges to do their jobs. You can make this onboarding process easier by providing a unique identifier related to this new member.\nThis could be a physical identifier, such as an identification badge or two-factor authentication device. Or it could be a logical identifier. These identifiers, such as the UID or the Unique Identity, are used for physical access to the company\u0026rsquo;s facilities.\nWhen an alert is triggered, confirmations to remove the no longer required accesses are revoked. The admin team should never confirm access requests. Instead, other peers should validate these requests. Access requests must always come from the user or manager request following all the necessary validations. The administration team should never mention or start an access request.\nIn most companies, it\u0026rsquo;s up to the Human Resources department to make the requests. As a result, those requests usually happen during the hiring process or when a person takes on a new role. Those requests fall under one of the two categories listed below:\nUser provisioning tasks: Onboarding of a new colleague or adding new rights to current access privileges. User de-provisioning tasks: Offboarding some responsibilities by removing unnecessary rights to current access privileges. Let\u0026rsquo;s get into these in more depth. Provisioning and De-Provisioning\nUser access provisioning means managing the access control of your users to the necessary systems for their work. Those access privileges come by either granting accesses due to new responsibilities or removing accesses for the same reason.\nWith that being said, the constant validation of current access per identifier is crucial. These validations help to narrow the attack surface from untrusted sources. It\u0026rsquo;s a critical step. So, it\u0026rsquo;s a good practice to periodically revisit as the user gains new responsibilities and needs more access.\nDe-provisioning means handling all the necessary steps for revoking access that isn\u0026rsquo;t needed. It also involves alerting all the required departments of the changes in access. The access provisioning models are the components that orchestrate all these requests seamlessly. Using the models, you can expand existing accesses with new ones. Or you can de-commission a particular user as needed.\nAll right! Let\u0026rsquo;s take a closer look at the different models; let\u0026rsquo;s start with RBAC. Access Provisioning Models: Role-Based Access Control (RBAC).\nTo understand what role-based access control (RBAC) means, I want to present what role actually means for identity and access management. For this kind of access, provisioning users are labeled by either the department or role that they\u0026rsquo;re part of.\nIt\u0026rsquo;s good to point out that these labels are managed by role-based access provisioning. All these are related actions of granting and/or revoking access to the whole group that the identity belongs to. And that removes the need to apply rules for individual identities.\nIn other words, the policy is to control access not by using a single identifier but by grouping the same user functions. This approach is faster and easier than making judgments individually based on identity and requests.\nLet\u0026rsquo;s imagine an the following use-case; a developer\u0026rsquo;s or manager\u0026rsquo;s role-based access provisioning.\nWhere as the developer needs to have full access to the development environment and limited access to management tools to track their work. The managers on the other hand, needs the opposite. As managers won\u0026rsquo;t be as deep on the technical; they might not need full access to the development environments. However, managers will require full access to the working tracking tools to manage new clients\u0026rsquo; needs.\nFor successful deployment, all necessary configurations per group must have strict validations based on the scope of the group managed. So there\u0026rsquo;s no point in adding programmatic access on the development pipelines to your financial team, for example. They don\u0026rsquo;t need that kind of access. Access Provisioning Models: Request-Based Access Provisioning (RBAP).\nFor starters, this differs from RBAP in terms of the request-based access provisioning. In some systems, direct requests need to go to the system owner. One example is asking for permission to access regulated systems. We call this discretionary access control (or DAC).\nOn the other hand, when you\u0026rsquo;re using mandatory access control (or MAC), the resource accesses are tagged by how classified the information on these systems is. This categorization will follow how your business classifies your data internally. I suggest taking a look here if you need more details about information classification handling procedures.\nA good use case of a MAC is when there are some more restricted cases, such as handling social insurance numbers. It\u0026rsquo;s good to remember that all complementary information related to security procedures to handle this kind of information also comes with those approvals.\nIn contrast, the DAC would require only the discretion of the system owner. So, if you manage to convince the owner of that system, then that person will provide you access to it. (Being a good friend of the administrator also helps!) Hybrid Provisioning\nThere may be cases when you need temporary access to a system that your existing role doesn\u0026rsquo;t provide access to. In that case, hybrid provisioning will be handy. It allows you to have the onboarding process for normal access provisioning. It also enables any special requests with a long line of approvals for ad hoc access provisioning. Sometimes, expanding the existing access scope quickly for ad hoc tasks might be necessary.\nSecondly, It\u0026rsquo;s essential to define the life span of this access and set an expiration date for each type of access. And that means having extra validation steps from additional levels of management. The hybrid provisioning model can help you do this. You\u0026rsquo;ll need to figure out the details as you define the security blueprint of your company. I want to list below a few rules of thumb to think about when you figure out security privileges.\nAlways prefer restrictive access provisioning policies. Have an audit in place to reinforce those restrictive access provisioning policies. Revoke any unused accesses based on the auditing results. Revalidate the onboarding and offboarding procedures in your company. (It\u0026rsquo;s time to set some If you don\u0026rsquo;t have any of those in place.) Conclusion Today, we started out by defining what identity means and what provisioning and de-provisioning accesses are. And I followed up by presenting the necessary steps to maintain the access provisioning lifecycle with the help of the provisioning models. On the other hand, it\u0026rsquo;s good to have third-party tools like Okta or Keycloak to help with access provisioning on your ecosystem. Okta manages manual deployment in a more automated way, provisioning and de-provisioning access in a more fluid manner. For that, I suggest looking at our previous blog posts at how to correctly configure it on your AWS environment and more details on AWS roles with Okta management.\nFor further help transforming your data, you can reach out to us with any questions you have and even discover new info you weren\u0026rsquo;t aware of with a better insight from better understanding your data.\nSee you next time!\n","permalink":"https://www.dawrlog.com/posts/14/","summary":"Hello everyone, today I want to revisit what is Identity and Access provisioning lifecycle phases.\nWith so much data being generated every second, it makes sense that data leakages are one of the biggest nightmares of any data-driven company. These days, we learn fast that weak security policies can be costly. This article will present one of the techniques to maintain your data secured: the identity and access provisioning lifecycle.","title":"What Is the Identity and Access Provisioning Lifecycle?"},{"content":"Hey everyone, hope everything is well.\nOne of the significant challenges in handling data comes from the massive amount of data your organization is most likely generating constantly. Having the ability to react quickly and accurately turns into a differential item among competitors.\nMore and more companies are becoming aware of the need to handle data wisely.\nThey\u0026rsquo;re looking for ways of getting the most out of their information. And this is probably where they first hear about data warehouses.\nI want to present data warehouses and their components in this article.\nTo start, you\u0026rsquo;ll get an overview of what a data warehouse is. Then we\u0026rsquo;ll go over some considerations that I find mandatory when building a data warehouse.\nTo conclude, I\u0026rsquo;ll do a quick recap and then share some other concerns and clarifications.\nThe basics of data warehousing\nData warehouse databases (DWs for short) are a decision support system. Once you understand how they can make the analysis of historical data much easier, it\u0026rsquo;s easy to see why people call them warehouses.\nAs the name suggests, this decision support system helps in the decision-making process by using available data from within the company.\nDW is a result of solving questions around specific subjects.\nIt has a different approach from the typical operational applications, such as billing or human resource systems. The latter focuses on operational needs such as pay and resource allocation, for example.\nAnd due to DW systems\u0026rsquo; peculiar characteristics, such as being time-sensitive and not having detailed information such as phone numbers, they aren\u0026rsquo;t a good option for operational reports.\nStaying safe during data warehouse implementation\nI can\u0026rsquo;t stress enough the importance of having a solid security policy for each phase of your data warehouse rollout.\nSimply put, the more closely you allow your users to the source systems so they can explore your data, the more they could potentially access without the correct permissions. And no one wants a data leak.\nOn top of your existing policies, your data warehouse must follow different requests.\nDW databases can solve critical decision support tasks; however, in some cases, the only place this information exists is within sensitive data. In the best \u0026ldquo;for your eyes only\u0026rdquo; style, sometimes it\u0026rsquo;s good to restrain those who don\u0026rsquo;t truly need access.\nThe data warehouse fun begins\nNow we\u0026rsquo;ve got our bases covered in terms of what a data warehouse is, it\u0026rsquo;s time for us to define who will be its users and what subjects it\u0026rsquo;ll cover.\nIt\u0026rsquo;s always a good starting point to inform department stakeholders in the early stages of your data warehouse development. After all, they\u0026rsquo;ll know who\u0026rsquo;s responsible for or interested in the resulting key performance indicators, or KPIs for short.\nRemember that they can help you solve problems like \u0026ldquo;Where can I find data that could help me?\u0026rdquo; or \u0026ldquo;What will we use to analyze that?\u0026rdquo;\nWhich DW building technique should you use?\nIt\u0026rsquo;s also important to choose the best technique to follow as you build your DW.\nThe most famous one is the Kimball approach. (I\u0026rsquo;ll describe its phases a bit later in this article.) Ralph Kimball is considered the father of data warehousing. The Inmon method is the oldest approach, but it\u0026rsquo;s a solid choice. I also want to mention data vault modeling and CDP/DMP if you\u0026rsquo;re involved in AdTech. I always like to clarify that although some characteristics of a customer data platform (CDP) make it seem like a data warehouse, it isn\u0026rsquo;t.\nA CDP can work as a data lake for your data lakehouse, though. To find out more, check out this CDP guide.\nGiving your data warehouse layers\nSo far, we\u0026rsquo;ve clarified who your team is and what your needs are. Now, let\u0026rsquo;s see the layers (or phases) on how your data flows from your applications into your data warehouse.\nYour OLTP layer\nAt this stage, you should start by defining what you\u0026rsquo;d use as a data source to create your dashboards.\nThe goal is to build your online transaction processing layer, also known as the staging area, landing zone area or by its acronym OLTP. It\u0026rsquo;s where you define the retention plan for this data.\nKeep in mind that this stage shouldn\u0026rsquo;t have any data transformation, as the data remains available if some reload is needed on the next phases.\nThis area lets you complete concurrent processes while the source applications are running. It\u0026rsquo;s a data source for the next layer, the ODS area.\nYour ODS layer\nOK, let\u0026rsquo;s say you\u0026rsquo;ve got your data sources configured and pumping data into your staging area. Great! But you\u0026rsquo;ll need to make some sense of your disparate systems.\nThink about it: How can you certify that the tables have the same kind of information? Will you need to have a conformed format for your data?\nHere\u0026rsquo;s where the operational data store (or ODS) layer is helpful.\nThe ODS puts all your data sources in order in the correct format. It also removes duplicated data.\nIn other words, it helps you develop the best and most unified operational database possible. Cube models and data granularity\nOnce your sources are busy creating useful data for your analysts, you\u0026rsquo;re ready to create your cube models. Here\u0026rsquo;s where you\u0026rsquo;ll use techniques such as iceberg queries and windows queries.\nAt this point, you\u0026rsquo;ve advanced far beyond the operational nature of your data.\nThese techniques transform your data, allowing you and your team members different levels of analysis. What we call data granularity helps you gain more specific, more targeted \u0026amp; more useful insights from the same dataset.\nLet\u0026rsquo;s imagine that you want to look for an address of a retailer. That information is of high granularity. In other words, it has a specific value to look for (the store address).\nBut what if you want to see how many stores are in a specific county?\nIn that case, you\u0026rsquo;ll have data of low granularity, and you\u0026rsquo;ll need to count your addresses to produce this outcome.\nYou can find out more about data granularity on this link. Setting up the infrastructure, in the cloud or on-premises\nThe idea of business intelligence comes from a simpler era regarding data structures and interest in extracting information from the data.\nMost data sources originated in structured formats from relational databases or semi-structured formats from web services (such as the XML format of the good old SOA services or JSON).\nIn the past, people needed to rely on heavily regular expressions to extract data from a single field; therefore, it made sense to invest in big guns to store your data, as it was supposed to increase over time exponentially.\nWhat\u0026rsquo;s called the public cloud presented an attractive option, eliminating the need for these expensive investments.\nCloud providers, including AWS and Azure, are acquiring more evangelists on their data storage solutions.\nThe serverless approach is exciting and saves time. However, the configuration and management of these new services might be cumbersome to your existing environment.\nFor this reason, I advise you to go easy on migrating and rebuilding your existing pipelines into new ones. Instead, choose less critical ones to start with and then evolve into the cloud. (Some call this the hybrid approach.) What we\u0026rsquo;ve covered so far about data warehousing\nComing up with the correct design for your data warehouse is challenging all by itself.\nEven for the most experienced data architect, focusing on unclear requirements and discovering more complex ones during development makes having a simple dashboard a distant dream.\nTherefore, let\u0026rsquo;s consider the concerns you should have in mind. Other approaches besides Kimball\nIn this article, I focused on the Kimball approach because it\u0026rsquo;s the most used one. But what if you\u0026rsquo;re interested in different techniques?\nYou can investigate another old and reliable framework that we mentioned earlier — the Inmon approach.\nAnother approach worth finding out about is the data vault. It\u0026rsquo;s an excellent option for fast-changing schemas. Also, it\u0026rsquo;s my preferred method when modeling for streaming data.\nFinding the right tools for your data warehouse\nSometimes it\u0026rsquo;s hard to find the right tools to handle all your data sources and their pipeline ingestions. In part because data warehouse techniques are technology agnostic by their nature. But with a good foundation about the nature of your data you would be on a better position to understand your data.\nSee you guys next time!\n","permalink":"https://www.dawrlog.com/posts/1/","summary":"Hey everyone, hope everything is well.\nOne of the significant challenges in handling data comes from the massive amount of data your organization is most likely generating constantly. Having the ability to react quickly and accurately turns into a differential item among competitors.\nMore and more companies are becoming aware of the need to handle data wisely.\nThey\u0026rsquo;re looking for ways of getting the most out of their information. And this is probably where they first hear about data warehouses.","title":"Data Warehouse Design: The good, the bad, the ugly"},{"content":"Hello all!\nToday I want to discuss one of the dataviz tools that is getting more and more adoption; Power BI. Power BI increases its adoption among the most experienced users each day. Thanks to being a user-friendly tool with various data sources to connect to, it helps analyze distinct data sources within the same console.\nIn this post, I\u0026rsquo;ll show you how to import your data stored on PostgreSQL with a desktop version of Power BI.\nI\u0026rsquo;ll start by explaining what Power BI is and what it does. Then I\u0026rsquo;ll offer an option to spin up a local instance to explore your PostgreSQL data with Power BI.\nI suggest skipping the \u0026ldquo;Configure your local environment\u0026rdquo; section if you already have PostgreSQL and Power BI running or are familiar with Vagrant boxes.\nHowever, I encourage you to look at it if you have some spare time. Leveraging Vagrant can be a valuable option to create your proofs of concept smoothly with automation tools.\nOnce we handle the infrastructure, we\u0026rsquo;ll configure Power BI to connect to your existing PostgreSQL instance, explore some data on your Power BI instance, and transform it before its final load, creating a chart from your newly imported data.\nWhat is Power BI?\nPower BI is a data visualization tool created by Microsoft; it has different versions focused on other use cases. You can try PowerBI for free on Azure, the Microsoft option for public cloud, to interact with your existing services such as the datawarehouse as service Azure Synapse.\nAnother option is to have it installed on your \u0026ldquo;on-premises services\u0026rdquo; using an on-premises data gateway.\nAs Power BI grows more mature, it\u0026rsquo;s naturally increasing in popularity, most of which is thanks to some neat capabilities offered to its users.\nFor example, it\u0026rsquo;s quick and sweet to load your structured data as the source on Power BI. That lets you understand the trends of your data and have the answers you need faster than you\u0026rsquo;re used to, making even the most demanding users happy as it\u0026rsquo;s possible to explore more data sources.\nAnd remember: better data gives better results!\nSo, with that said, if you\u0026rsquo;re still struggling to consume your data correctly, you would like to audit your data pipelines. You would need to clean your data, which consists of treating all bad aspects for your analysis. As the data format from its source might not be ready for analysis.\nNow that we understand what Power BI is: let\u0026rsquo;s see it in action. We will dive right into connecting it to PostgreSQL, one of the most used relational databases options.\nConnecting PostgreSQL to Power BI, configure your local environment\nIf you already have a PostgreSQL instance, skip to the next section Create a database connection. For those who don\u0026rsquo;t have an instance, let\u0026rsquo;s start by setting one up.\nFirst, download PostgreSQL and Power BI (Windows only). But wait, suppose you\u0026rsquo;re like me and are not that into having a Windows machine?\nI suggest using Vagrant.\nHaven\u0026rsquo;t heard of or used Vagrant before? No problem! Take a look at this quick introduction for a good crash course.\nAfter that, or if you\u0026rsquo;re already familiar with Vagrant, you\u0026rsquo;re ready to follow these steps:\nFor starters, you\u0026rsquo;ll need to download Vagrant and the Windows box used as the base image. Once you have Vagrant set up, you can download Power BI Desktop from inside; a VirtualBox client can do the trick. To follow along, download the database that we\u0026rsquo;ll be using from here. By doing that, you\u0026rsquo;ll follow along with a local Power BI instance on your machine. Create a database connection\nTo get started, select the Get Data option after opening Power BI; this will open a window where you\u0026rsquo;ll specify what kind of storage or database your data is stored in. The popup will show all data sources that Power BI can access.\nFor our exercise, we\u0026rsquo;ll select the PostgreSQL database, as shown below.\nPostgreSQL database option on \u0026ldquo;Get Data\u0026rdquo; window\nOnce selected, Power BI will ask you to confirm the username and password the first time you connect to this database. There\u0026rsquo;s no need to repeat this validation when you log in as the same user within the same server already used.\nJust ensure that the specified PostgreSQL user has the permissions required on the database where your table resides.\nOnce authenticated on the server, you\u0026rsquo;ll need to specify the server and the database. To do so, it would simply need to add your PowerBI server address, or localhost if you have a local PostgreSQL instance, to access the database where you\u0026rsquo;ll select your tables. We\u0026rsquo;ll select the Import option for the Data Connectivity mode since we\u0026rsquo;ll be transforming some data before using it.\nThe DirectConnect option, as the name suggests, doesn\u0026rsquo;t create this needed cache; I always recommend using Import instead of DirectQuery. One of the most significant flaws of DirectQuery is that it doesn\u0026rsquo;t allow data transformation during the load.\nPostgreSQL database gets data screen\nTo keep it short and sweet, leave the advanced options unchanged and click OK.\nPower BI will inform you before continuing in the case where your connection is not encrypted. If you click Cancel, it will stop the import. I encourage you to explore the SQL Statement option more, as it gives more choices to transform your data while loading it.\nYou can find more details and even a working example on this page about native database queries from Microsoft.\nTransform the data while loading it into your datastore\nNow that we have taken care of our connection, it\u0026rsquo;s time to make it work for us. On the next screen, you can cherry-pick your tables. Once you select it, you\u0026rsquo;ll have the option to either load it as is or transform it before using it. In this tutorial, I\u0026rsquo;ll split the first word from the remaining others using the leftmost delimiter; By chosing select transform, and a new window (like the one below), will appear.\nIn the Select or enter delimiter field, leave the remaining options with space delimiter and quote character unchanged. Doing so will transform the string field into two child fields.\nSplit options for data import\nOnce you confirm the change, you will then go back to the previous screen. It will now reflect the changes you applied on the Split Columns screen.\nIt\u0026rsquo;s good to remember that you can still remove these changes, even after the previous confirmation.\nThese results aren\u0026rsquo;t final on the data sources used by the dashboards; instead, they\u0026rsquo;re on temporary components. So, you can still go back if you want to add or change some extra details.\nOnce all your settings are as desired, select the Close \u0026amp; Apply option to commit your changes, as shown below.\nPreview Get Data after transformation changes.\nCongratulations! You just imported your data from PostgreSQL into Power BI.\nStart your data journey\nToday we scratched the surface of integrating Power BI and PostgreSQL and breaking down how Power BI could enhance your data exploration on PostgreSQL databases thanks to its very user-friendly interface.\nPower BI enables you to interact with the data in its early stages. Why not enrich your data directly from the source replication right? Just remember what your real needs are.\nIt might be hard to have a complete view of your historical data trend analysis.\nHaving said that, a good thing to have in mind is to avoid, or at least restrict when possible, using Power BI on untreated data.\nThe data in its raw state is more for operational purposes. For time-sensitive analytical purposes, you should always choose to use treated information that gives better insights from application data in its raw state.\nNow, go and have fun!\n","permalink":"https://www.dawrlog.com/posts/7/","summary":"Hello all!\nToday I want to discuss one of the dataviz tools that is getting more and more adoption; Power BI. Power BI increases its adoption among the most experienced users each day. Thanks to being a user-friendly tool with various data sources to connect to, it helps analyze distinct data sources within the same console.\nIn this post, I\u0026rsquo;ll show you how to import your data stored on PostgreSQL with a desktop version of Power BI.","title":"How To Analyze PostgreSQL Data in Power BI."},{"content":"Hey everyone, today I would like to present you how to explore your raw data stored by AWS S3 by using AWS managed analytical managed services. Today we will cover both AWS Redshift Spectrum and AWS Athena on our tutorial.\nOne of the significant challenges to data-driven companies comes when integrating different application systems.\nMajor public cloud providers, such as Amazon Web Services or Google Cloud Platform, offer robust products that are ready for your analytics needs. But when it comes to exploring your data, things might not be that simple.\nYour source data often comes from files with unknown formats of data, which makes an analyst\u0026rsquo;s job a nightmare.\nSome cases could be more fluid in this integration, such as when your data has nested values. Exploding complex structures, such as JSON files, into a tabular format, can consume most of your time when you\u0026rsquo;re exploring new data.\nThat\u0026rsquo;s where AWS Redshift Spectrum and AWS Athena shine. They let you use SQL to analyze data without changing it from the source. There\u0026rsquo;s no need for complex Python code if you don\u0026rsquo;t want to use it on initial data profiling tasks.\nPretty neat, right?\nThis article will show you how to explore your data on Amazon S3 using Athena and Redshift Spectrum. Below, you\u0026rsquo;ll find the necessary steps to create a table on the AWS Glue catalog and use it to access your data in Amazon S3. How do Redshift Spectrum, AWS Athena \u0026amp; AWS S3 fit together?\nWhile Amazon\u0026rsquo;s data products aren\u0026rsquo;t quite as extensive as its famous ecommerce shop, there\u0026rsquo;s still a lot going on. Redshift Spectrum Spectrum is an Amazon Redshift component that allows you to query files stored in Amazon S3, this is done by creating a new database pointing to your AWS S3 storage bucket.\nYour team can narrow its search by querying only the necessary columns for your analysis. Also, it\u0026rsquo;s possible to consult existing tables from your Redshift cluster, which means that instead of querying the full table all the time, you can select the required columns for your report using SQL.\nSo when you\u0026rsquo;re querying your data, you get only the needed columns from your data instead of returning unnecessary fields and rows. The also opens up the possibility of querying data stored directly on Amazon S3. AWS Athena Athena makes it easier to create shareable SQL queries among your teams—unlike Spectrum, which needs Redshift. You can then create and run your workbooks without any cluster configuration.\nAthena makes it possible to achieve more with less, and it\u0026rsquo;s cheaper to explore your data with less management than Redshift Spectrum.\nAWS S3 AWS S3 is the managed object storage option that Amazon offers. It\u0026rsquo;s the best option to store your semi-structured data, such as server logs from your applications.\nS3 also allows delete protection and version control of your objects, making your data safer and easier to track back to its original source.\nHow to create tables from files Now that you have an overall idea of each product, it\u0026rsquo;s time to get your hands dirty and create some tables!\nWe\u0026rsquo;ll use a sample CSV dataset for our tutorial, which you can download here.\nWe\u0026rsquo;ll also assume that you have your Redshift cluster ready with the necessary IAM roles attached to it (when using Redshift Spectrum).\nYou\u0026rsquo;ll also need your AWS bucket set up with your data configured and the required permissions to create your data catalog; more details can be found at Athena IAM Data Catalog policy page.\nOkay, so far, so good! Let\u0026rsquo;s move on to creating the tables.\nFirst, you need to create the database where the tables will be stored.\nFor this tutorial, we\u0026rsquo;ll count on AWS Glue Data Catalog for this job. Just remember that other options are available, such as the Hive metastore.\nAWS Glue Data Catalog is a better option if you want to have fluid integration with additional data sources without starting extra services.\nHere is how to create database on AWS Athena:\nCREATE DATABASE IF NOT EXISTS ext_data_suppliers COMMENT \u0026#39;Landing Zone for S3 buckets loaded by external Data Suppliers\u0026#39; LOCATION \u0026#39;s3://test-12343210/\u0026#39;; Here\u0026rsquo;s the Redshift Spectrum version of it:\ncreate external schema ext_data_suppliers from data catalog database \u0026#39;ext_data_suppliers\u0026#39; iam_role \u0026#39;arn:aws:iam::123456789012:role/RSRoleApiData\u0026#39; create external database if not exists; As you can see in both cases, your code will create a Glue catalog database if one doesn\u0026rsquo;t exist already.\nOnce you have that, you\u0026rsquo;ll need the table definition, which will let you query the data directly from the file.\nAt this stage, I recommend not doing any transformations on the data because a minor modification. As on this data layer we want to be as close to its data source as possible, even a simple datatype conversion can result in the loss of data. So let\u0026rsquo;s avoid that, especially in the early stages.\nNow, let\u0026rsquo;s create a table definition that\u0026rsquo;ll contain the data.\nBelow, you can see the Athena version:\nCREATE EXTERNAL TABLE IF NOT EXISTS ext_data_suppliers.zillow_sample_file ( `index` int, `liv_space_in_sqft` int, `beds` int, `baths` int, `zip` int, `year` int, `list_price_in_usd` int ) ROW FORMAT SERDE \u0026#39;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\u0026#39; WITH SERDEPROPERTIES ( \u0026#39;serialization.format\u0026#39; = \u0026#39;,\u0026#39;, \u0026#39;field.delim\u0026#39; = \u0026#39;,\u0026#39;) LOCATION \u0026#39;s3://test-12343210/\u0026#39; TBLPROPERTIES (\u0026#39;has_encrypted_data\u0026#39;=\u0026#39;false\u0026#39;); And here\u0026rsquo;s the Spectrum version:\nCREATE EXTERNAL TABLE ext_data_suppliers.zillow_sample_file ( index int, liv_space_in_sqft int, beds int, baths int, zip int, year int, list_price_in_usd int ) ROW FORMAT SERDE \u0026#39;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\u0026#39; WITH SERDEPROPERTIES ( \u0026#39;serialization.format\u0026#39; = \u0026#39;,\u0026#39;, \u0026#39;field.delim\u0026#39; = \u0026#39;,\u0026#39;) LOCATION \u0026#39;s3://test-12343210/\u0026#39;; So, which should you choose: Spectrum or Athena? Most data from data suppliers isn\u0026rsquo;t optimized to answer the questions you may have. As this data is in a raw state, tools like AWS Athena or AWS Redshift Spectrum will make your exploration tasks much more straightforward.\nBoth tools allow you to explore your data without loading it into a database. All you need to do is say what your data structure is and where it resides. After that, you\u0026rsquo;re good to go—no more delays on your data pipelines to start creating your dashboards. As your data becomes available on your Amazon S3 bucket, your team can consume it right away.\nYou can run federated queries on both services. These queries allow the use of the same SQL script to correlate data from structured relational sources, such as MySQL and Postgres.\nMy advice? Choose Athena if you don\u0026rsquo;t have a Redshift cluster already in place.\nWith Athena, it becomes easier to create shareable queries among your team without managing extra services and raising your cloud bill unnecessarily. Summing it up and going deeper\nTo recap, we\u0026rsquo;ve covered 2 important topics:\nThe benefits of having a data exploration tool that allows your analysts to run SQL commands on top of your object-storage-type solution. Running SQL commands on files stored in Amazon S3, using Athena and Redshift Spectrum.\nAs you saw, both scripts are very similar. In both, we used serialization/deserialization (SerDe for short) to create a table-like structure correctly. This structure lets you access the data in the CSV format without loading it into native tables.\nHere\u0026rsquo;s another thing I\u0026rsquo;d like you to remember: Grant only the necessary permissions to services. Narrowing access to your services will help you sleep better.\nFor further help transforming your data, you can reach out to us with any questions you have and even discover new info you weren\u0026rsquo;t aware of with a better insight from better understanding your data.\nSee you guys next time!\n","permalink":"https://www.dawrlog.com/posts/11/","summary":"Hey everyone, today I would like to present you how to explore your raw data stored by AWS S3 by using AWS managed analytical managed services. Today we will cover both AWS Redshift Spectrum and AWS Athena on our tutorial.\nOne of the significant challenges to data-driven companies comes when integrating different application systems.\nMajor public cloud providers, such as Amazon Web Services or Google Cloud Platform, offer robust products that are ready for your analytics needs.","title":"AWS Redshift Spectrum, Athena, and S3: Everything You Need to Know"},{"content":"🐧\nCurrent AWS Work:\nI\u0026rsquo;m currently working on AWS environments with Github runners hosted on EC2 instances for some clients; and the current workload are splited by instance fleets on EC2 and EKS clusters. It is stored behind a public load balancer and a NAT Gateway for private instances. I also provided IAM policies to restrain external accesses to those services. The Helm charts and Docker images are limited by the AWS Organization regional policies and stored on ECR repositories. Our data pipelines are orchestrated by Step-functions on most of them; however there are some clients which explore clickstream data with the help of Eventbridge from AWS S3 buckets. The code are mostly run on AWS Lambda functions on Python code; but some EMR might be necessary depending on the data flow expected.\nCloud Architect | SRE - Collibra Montreal, CA ●●●●●●●●● December 2021 - Actual\nPresent to level-C customers the benefit of Collibra Data Quality on distributed cloud environments; Provide PoC on public, private and hybrid clouds. Training the internal team with best practices on Cloud environments based on AWS cloud well-architected framework and the CBK disciplines. Providing guidance for non technical users on technical benefits and drawbacks on technology choices. Providing coaching sessions on technical deployments for customer technical teams on troubleshooting their automated deployment of the core Collibra DQ components. Technical Writer on best practices articles as well as technical hands on deployment documents for customers related to Data Engineering, Cloud Security, DataOps and DevOps topics. Creating best practice documents on a Security standpoint while deploying the Collibra Data Quality application. Developed and maintained CI/CD pipelines using open-source tools and configuring them on internal cloud environments. Technical Writer - Hit Subscribe Montreal, CA ●●●●●●●●● May 2021 - Actual\nProvided technical whitepapers for third-party clients on Cloud native products.\nProvided blog content to improve SEO rating due to organic traffic.\nElaborated topics around:\nCloud Native tools (Kubernetes, Docker)\nStorage best practices (SQL and NoSQL database types)\nCloud Computing Best Practices on public cloud providers (AWS/GCP)\nPublic cloud providers best practices AWS\nTechnical Leader Cloud, KEYRUS MONTREAL, CA ●●●●●●●●● October 2020 – December 2021\nHelped the internal team with best practices on which technological approach to take based on a cloud well-architected framework. Provided internal presentations on Cloud deployments using automation tools. Presented benefits of cloud migration to IT Managers based on the CBK disciplines. Worked with the internal team to implement new data ingestion pipelines to populate the customer\u0026rsquo;s data lake. Focused on data quality, reliability and automation of their cloud environment. Worked close to Business Analysts to provide Business Intelligence self-served platforms, allowing the customer to query non-relational data with SQL statements. Worked with internal teams on the development of internal products. Worked as Technical Writer while helping to build a better brand while maintaining a professional blog with articles related to Data Science, Cloud Security and DevOps implementations. Developed conformed Kubernetes applications based on open-source projects. Configured the CI/CD pipelines using open-source tools and configuring them on GCP and AWS environments. Converted monolithic legacy applications into Python microservices applications. Deployed successfully internal projects following the best practices on DevSecOps, DataOps and MLOps. TECHNICAL LEAD/CLOUD SPECIALIST, MEDIAGRIF LONGUEUIL, CA ●●●●●●●●● December 2019 – October 2020\nProvided hands-on training on Python to experienced ETL Developers/Business Analysts. Gave guidance to the DBA team to migrate on-prem data servers into public cloud providers. Worked close to Business Analysts to provide Business Intelligence self-served platforms. Brought value to Artificial Intelligence projects with a solid architecture for data consumption; enhancing the accuracy of Machine Learning models with proper data. Brought new insights to Vice presidents and CTO with a new analytical platform based on AWS components based on the CBK disciplines and the AWS Well architecture framework. Acted as solution architect for migration of existing legacy systems into microservices using Kubernetes. Refactored existing SSIS ETL pipelines into AWS Glue jobs, orchestrated by Airflow instances. TECHNICAL LEAD BI | CLOUD SOLUTIONS ARCHITECT, MAVEN WAVE PARTNERS LLC Montreal, CA ●●●●●●●●● December 2018 – December 2019\nProvided guidance and training at cloud solutions for Data Warehouse on one of the biggest sports clothing retailers in the US. Delivered as Lead developer to our client in the healthcare industry a data streaming pipeline which consumed data from health tracking devices into an Analytical database. Lead a team of developers at one of the leading retailing companies in the US, implemented a new pattern of data consumption based on google cloud products using a correlation of Informatica Intelligent Cloud Services. This pattern could migrate the previously existing code into the latest technology without reworking on the mappings, saving the data pipelines\u0026rsquo; downtime. SOLUTIONS ARCHITECT | DATA ENGINEER Montreal, CA ●●●●●●●●● March 2018 – December 2018\nDelivered containerized data pipelines, simplifying the process of data consumption and helping the users to have their data faster. Designed the client consent project pipeline under the MCP Program. Focused on strengthening relationships with existing customers with the privacy-focused initiative. Implementing explicit consent on the data owned by the Bank from their customers. Worked as integration architect to build up the 360 databases with transactional data from NBC clients. SENIOR DEVELOPER, VIDEOTRON MONTREAL, CA ●●●●●●●●● May 2017 – March 2018\nIdentified data integrity problems and bottlenecks, giving a better understanding of their customer data. Enforced a change of mindset from traditional Business Intelligence towards Big Data analysis, unlocking the real-time analysis. TECHNICAL LEAD BI | DATA ENGINEER, REITMANS MONTREAL, CA ●●●●●●●●● May 2016 – May 2017\nLed a proof-of-concept project to understand the customer behavior with the help of Machine Learning algorithms, raising the productivity of the data analysts. Acted as Data Scientist on a work with the Director of Analytics on improving their attribution modelling based on google analytics data. Having a precision gain on their marketing campaigns. DATA INTEGRATION CONSULTANT, EXIA MONTREAL, CA ●●●●●●●●● Nov 2014 – May 2016\nEnhanced business processes and technical specifications critical for company operations, Lowering the response window for critical issues at data consumption. Realized a process improvement with a new pattern of data consumption on a self-served data pipeline environment. As a result, increased the utilization from business areas due to reduced complexity. DATA INTEGRATION SPECIALIST/SOLUTIONS ARCHITECT, INFORMATICA RIO DE JANEIRO, BR ●●●●●●●●● Sept 2011 – May 2014\nImplemented an analytical platform from scratch for two major players on Brazil\u0026rsquo;s insurance market, providing detailed information on components to make antidotes on a laboratory client, the costs drastically decreased. Led the technical team on one of the biggest banks in Brazil, providing guidance and hands-on training on the implementation of data generated in real-time, making it possible to be more reactive to adapt to new trends. Trained and guided a team of experienced developers to build up an analytical platform for one of the significant government laboratory entities, enabling the board to track all costs related to anti- dotes. Implemented a New Data Warehouse under a contract with Subsecretaria de Planejamento e Orçamento do Rio de Janeiro. Training their analysts and developer team to find new metrics, whereas enforcing the business gains to the finance director. BUSINESS INTELLIGENCE CONSULTANT, ACCENTURE RIO DE JANEIRO, BR ●●●●●●●●● Sept 2011 – May 2014\nOptimized existing data pipelines to consume less infrastructure, saving budget with third parties consultants and providers. Led the technical IT team to implement a new marketing campaign, increasing the conversion rate for telecom data clients. BUSINESS INTELLIGENCE CONSULTANT, NTT DATA RIO DE JANEIRO, BR ●●●●●●●●● Jan 2006 - Dec 2009\nMade substantial process improvements on the analytical dashboard, generating new business leads due to a better data comprehension. Provided internal training, hands-on training of data warehouse, increasing the in-house expertise, decreasing the need of outsourcing the development due to unskilled staff. COURSE INSTRUCTOR – CONCORDIA CONTINUING EDUCATION MONTREAL, CA ●●●●●●●●● January 2019 – May 2019\nTaught hands-on course for new grads and experienced professionals on big data storage (CEBD-1250). Served as consultant to enhance existing course outline to be more precise with what can be found on the market ","permalink":"https://www.dawrlog.com/resume/","summary":"🐧\nCurrent AWS Work:\nI\u0026rsquo;m currently working on AWS environments with Github runners hosted on EC2 instances for some clients; and the current workload are splited by instance fleets on EC2 and EKS clusters. It is stored behind a public load balancer and a NAT Gateway for private instances. I also provided IAM policies to restrain external accesses to those services. The Helm charts and Docker images are limited by the AWS Organization regional policies and stored on ECR repositories.","title":""}]