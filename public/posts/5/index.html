<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Argo Workflow ETL Examples. | Dataware Logistics</title>
<meta name="keywords" content="Kubernetes, Argo, Development, Data Warehouse">
<meta name="description" content="This post explores using Argo Workflows to orchestrate your data pipelines. To start, let&rsquo;s refresh on what ETL is while designing our work&rsquo;s high-level architecture. I&rsquo;ll demonstrate how to set up your data pipelines to follow the structure more naturally. Then, we&rsquo;ll see how to achieve the same result using directed acyclic graphs (DAGs). Last, I&rsquo;ll summarize what we saw and present reasons to guide your choice of approach based on your project complexity.">
<meta name="author" content="Daniel Paes">
<link rel="canonical" href="https://pipekit.io/blog/argo-workflows-etl-examples">
<meta name="google-site-verification" content="4599922566">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<link rel="icon" href="https://www.dawrlog.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://www.dawrlog.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://www.dawrlog.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://www.dawrlog.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://www.dawrlog.com/apple-touch-icon.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://www.dawrlog.com/posts/5/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Argo Workflow ETL Examples." />
<meta property="og:description" content="This post explores using Argo Workflows to orchestrate your data pipelines. To start, let&rsquo;s refresh on what ETL is while designing our work&rsquo;s high-level architecture. I&rsquo;ll demonstrate how to set up your data pipelines to follow the structure more naturally. Then, we&rsquo;ll see how to achieve the same result using directed acyclic graphs (DAGs). Last, I&rsquo;ll summarize what we saw and present reasons to guide your choice of approach based on your project complexity." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.dawrlog.com/posts/5/" />
<meta property="og:image" content="https://user-images.githubusercontent.com/78096758/217079803-11a4c93a-7ae1-4273-b067-499a0d67023a.png" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-08-14T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-08-14T00:00:00+00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://user-images.githubusercontent.com/78096758/217079803-11a4c93a-7ae1-4273-b067-499a0d67023a.png" />
<meta name="twitter:title" content="Argo Workflow ETL Examples."/>
<meta name="twitter:description" content="This post explores using Argo Workflows to orchestrate your data pipelines. To start, let&rsquo;s refresh on what ETL is while designing our work&rsquo;s high-level architecture. I&rsquo;ll demonstrate how to set up your data pipelines to follow the structure more naturally. Then, we&rsquo;ll see how to achieve the same result using directed acyclic graphs (DAGs). Last, I&rsquo;ll summarize what we saw and present reasons to guide your choice of approach based on your project complexity."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://www.dawrlog.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Argo Workflow ETL Examples.",
      "item": "https://www.dawrlog.com/posts/5/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Argo Workflow ETL Examples.",
  "name": "Argo Workflow ETL Examples.",
  "description": "This post explores using Argo Workflows to orchestrate your data pipelines. To start, let\u0026rsquo;s refresh on what ETL is while designing our work\u0026rsquo;s high-level architecture. I\u0026rsquo;ll demonstrate how to set up your data pipelines to follow the structure more naturally. Then, we\u0026rsquo;ll see how to achieve the same result using directed acyclic graphs (DAGs). Last, I\u0026rsquo;ll summarize what we saw and present reasons to guide your choice of approach based on your project complexity.",
  "keywords": [
    "Kubernetes", "Argo", "Development", "Data Warehouse"
  ],
  "articleBody": "This post explores using Argo Workflows to orchestrate your data pipelines. To start, let’s refresh on what ETL is while designing our work’s high-level architecture. I’ll demonstrate how to set up your data pipelines to follow the structure more naturally. Then, we’ll see how to achieve the same result using directed acyclic graphs (DAGs). Last, I’ll summarize what we saw and present reasons to guide your choice of approach based on your project complexity. So, let’s start.\nWhat Does ETL Stand for? First, let’s remember what ETL is before starting our examples. Extract, transform and load consists of tasks to clean your data, and it wrangles the data from your applications into a conformed database. Imagine this conformed database as the single source of truth of your data. This centralized repository helps you gain insights into your products and your customers.\nHowever, each application has its structure to handle the data. Each ETL task makes the application data more palatable for analysis, having explicit dependencies as your data wrangling process becomes more robust. Our code will deploy four ETL tasks and their relationship, as reflected by the image below:\nHigh-level architecture of ETL tasks\nUnderstanding Each ETL Task This workflow supports two different data formats: column or row-based. These different formats require different parsers: parquet for columns and avro for rows. We can also refer to them as batch or stream, respectively.\nSo, the workflow starts with a request handler that identifies the data type. Based on the value of a data type flag, it passes the job to either the batch or stream parsing tags.\nThe parsers pass their output to the Load Data task, where it is loaded in persistent storage, based on the source and data type.\nWhile it seems like this is a single workflow with two different code paths, there are advantages to processing both data types in the same set of tasks. Sharing a loading tasks between the data streams makes it easier to manage overlaps and relationships. For example, the columnar data store may need to be updated with foreign keys from new row-based data. It’s also easier to share a single ETL workflow between different teams\nNow that we know what we’ll be building, let’s start by seeing how to implement it using Argo Workflows steps.\nBuild an ETL Pipeline Using steps in Argo Workflows In this approach, the data pipeline will follow a list of steps to clean and treat the data from your data sources. Your ETL code becomes even more robust when we use conditionals to inform which ETL flow your data should take. Sounds good, right? So, let’s get our hands dirty and see how it works in practice.\nThe code below will create a workflow in a namespace called argo. This namespace must exist before this workflow is executed with argo submit. Doing so will avoid security issues, such as your user not having permission to create namespaces. It will also prevent error messages warning you not to break your Kubernetes deployment. For our example, we’ll generate a random value on a Linux machine and load the upcoming data based on this value.\nWhile both parser steps are triggered simultaneously, only the one informed by the handle requests step will execute. Using automated code like this will reduce the chances of having problems with our ETL data flow. Automation on your workflow steps handles common errors such as mismatch types in your database.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: stream-batch-parser- namespace: argo spec: entrypoint: sbp templates: - name: sbp steps: - - name: request-handler template: edge-input - - name: parser-stream template: stream when: \"{{steps.handle-requests.outputs.result}} \u003c= 163883\" - name: parser-batch template: parquet when: \"{{steps.handle-requests.outputs.result}} \u003e 163883\" - name: stream steps: - - name: avro-parser template: avro - - name: wrapper template: wrapper - name: batch steps: - - name: parquet-parser template: parquet - - name: wrapper template: wrapper - name: edge-input container: image: alpine:3.6 command: [sh, -c] args: [\"echo ${RANDOM}\"] - name: parquet container: image: alpine:3.6 command: [sh, -c] args: [\"echo \\\"code to parse Parquet\\\"\"] - name: avro container: image: alpine:3.6 command: [sh, -c] args: [\"echo \\\"code to parse Avro\\\"\"] - name: wrapper container: image: alpine:3.6 command: [sh, -c] args: [\"echo \\\"code to load into Staging\\\"\"] Save the file above as `etl_steps.yml` and start your workflow with this command: argo submit -n argo etl_steps.yml We can now get our workflow status by executing the following argo get command:\nargo get -n argo stream-batch-parser-xxvks ‍The last five digits will differ in each environment. And by running the previous command, your output log should be similar to the image below; as stated, our workflow will execute the parser stream task based on the value returned by the handle requests task.\nArgo workflow etl steps get output ETL steps returned by running argo get command Now that we’ve seen how to build an ETL with tasks, let’s explore how to use DAGs for your ETL.\nBuilding an ETL Pipeline with DAGs Instead of Steps Now, let’s explore how to achieve the same work using DAG templates instead of steps in Argo Workflows. Even though the DSL looks similar at first glance, DAGs give you more power to specify dependencies between steps and run tasks in parallel.\nIn a DAG, any task can run when its dependencies are satisfied. If more than one task’s dependencies are fulfilled, all of them will run in parallel. If a task has no dependencies, it will run as soon as the workflow is started. DAGs are excellent for processing ETL, and I strongly suggest you familiarize yourself with all options that a DAG task can provide by looking at the Argo official documentation.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: dag-orchestrate- namespace: argo spec: entrypoint: sbp archiveLogs: true templates: - name: sbp dag: tasks: - name: request-handler template: edge-input - name: stream-flow template: stream when: \"{{tasks.handle-requests.outputs.result}} \u003c= 163883\" depends: handle-requests - name: batch-flow template: batch when: \"{{tasks.handle-requests.outputs.result}} \u003e 163883\" depends: handle-requests - name: stream steps: - - name: avro-parser template: avro - - name: wrapper template: wrapper - name: batch steps: - - name: parquet-parser template: parquet - - name: wrapper template: wrapper - name: edge-input container: image: alpine:3.6 command: [sh, -c] args: [\"echo ${RANDOM}\"] - name: parquet container: image: alpine:3.6 command: [sh, -c] args: [\"echo \\\"code to parse Parquet\\\"\"] - name: avro container: image: alpine:3.6 command: [sh, -c] args: [\"echo \\\"code to parse Avro\\\"\"] - name: wrapper container: image: alpine:3.6 command: [sh, -c] args: [\"echo \\\"code to load into Staging\\\"\"] Save the file above as etl_dag.yml and submit your workflow to start it:\nargo submit -n argo etl_dag.yml As demonstrated below, you can check its evolution with argo get:\nargo get -n argo dag-orchestrate-ctpkl ‍In this scenario, our workflow executed the batch stream task instead of the stream-flow based on the value returned by the handle requests task.\nCongratulations on your work! You can now design your ETL data flows using a DAG or structured list of steps within Argo Workflows.\nDon’t forget to clean your environment with argo delete -n argo your-workflow, where you should inform the desired workflow as your-workflow.\nargo delete -n argo your-workflow Conclusion While it’s commonly used for infrastructure management, Argo Workflows can also orchestrate your ETL tasks. Using it like this removes the need for different tools to achieve the same goal, i.e. Argo for CI/CD and Airflow for ETL jobs.\nThe DAG approach is often better than the steps approach for running ETL pipelines. For starters, DAG task processing is optimized at runtime. You’ll have fewer decision points for some of your pipelines simply by informing the desired data flow.\nFor simple tasks, sequential flows (as you get with the steps approach in Argo Workflows) work fine. However, they become harder to maintain in cases where you need to target a subset of your data flow and manage complex dependencies over time.\nAnother perk of using DAGs is to specify the exact step at runtime. Running it gives you more liberty to create conditional code with fewer indented loops while optimizing the code and the infrastructure resources.\nI urge you to go deeper into Argo Workflows’ documentation around DAGs. Mastering how DAGs work can increase the quality of your ETL pipelines, allowing you to manage your ETL tasks more dynamically compared to the steps method.\nFor more optimized ways of managing your Kubernetes resources, explore how Pipekit can help you orchestrate your whole Argo Workflows deployment.\nSee you next time!\n",
  "wordCount" : "1411",
  "inLanguage": "en",
  "image":"https://user-images.githubusercontent.com/78096758/217079803-11a4c93a-7ae1-4273-b067-499a0d67023a.png","datePublished": "2022-08-14T00:00:00Z",
  "dateModified": "2022-08-14T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Daniel Paes"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://www.dawrlog.com/posts/5/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Dataware Logistics",
    "logo": {
      "@type": "ImageObject",
      "url": "https://www.dawrlog.com/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://www.dawrlog.com/" accesskey="h" title="Dataware Logistics (Alt + H)">Dataware Logistics</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="https://www.dawrlog.com/fr/" title="French"
                            aria-label=":fr:">🇫🇷</a>
                    </li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://www.dawrlog.com/about" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://www.dawrlog.com/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://calendly.com/dawrlog" title="Contact">
                    <span>Contact</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://www.dawrlog.com/">Home</a>&nbsp;»&nbsp;<a href="https://www.dawrlog.com/posts/">Posts</a></div>
    <h1 class="post-title">
      Argo Workflow ETL Examples.
    </h1>
    <div class="post-meta"><span title='2022-08-14 00:00:00 +0000 UTC'>August 14, 2022</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Daniel Paes

</div>
  </header> 
<figure class="entry-cover"><img loading="lazy" src="https://user-images.githubusercontent.com/78096758/217079803-11a4c93a-7ae1-4273-b067-499a0d67023a.png" alt="">
        
</figure><div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#h2what-does-etl-stand-for-h2-br-"><h2>What Does ETL Stand for? </h2> <br /></a></li>
        <li><a href="#h2understanding-each-etl-task-h2br-"><h2>Understanding Each ETL Task </h2><br /></a></li>
        <li><a href="#h2build-an-etl-pipeline-using-steps-in-argo-workflowsh2-br-"><h2>Build an ETL Pipeline Using steps in Argo Workflows</h2> <br /></a></li>
        <li><a href="#h2building-an-etl-pipeline-with-dags-instead-of-steps-h2br-"><h2>Building an ETL Pipeline with DAGs Instead of Steps </h2><br /></a></li>
        <li><a href="#h2conclusion-h2br-"><h2>Conclusion </h2><br /></a></li>
      </ul>
    </li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><p>This post explores using <a href="https://argoproj.github.io/argo-workflows/">Argo Workflows</a> to orchestrate your data pipelines. To start, let&rsquo;s refresh on what ETL is while designing our work&rsquo;s high-level architecture. I&rsquo;ll demonstrate how to set up your data pipelines to follow the structure more naturally. Then, we&rsquo;ll see how to achieve the same result using <a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">directed acyclic graphs</a> (DAGs). Last, I&rsquo;ll summarize what we saw and present reasons to guide your choice of approach based on your project complexity. So, let&rsquo;s start.</p>
<h3 id="h2what-does-etl-stand-for-h2-br-"><h2>What Does ETL Stand for? </h2> <br /><a hidden class="anchor" aria-hidden="true" href="#h2what-does-etl-stand-for-h2-br-">#</a></h3>
<p>First, let&rsquo;s remember what ETL is before starting our examples. <a href="https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/etl-architecture-34-subsystems/">Extract, transform and load</a> consists of tasks to clean your data, and it wrangles the data from your applications into a conformed database. Imagine this conformed database as the single source of truth of your data. This centralized repository helps you gain insights into your products and your customers.</p>
<p>However, each application has its structure to handle the data. Each ETL task makes the application data more palatable for analysis, having explicit dependencies as your <a href="https://en.wikipedia.org/wiki/Data_wrangling">data wrangling</a> process becomes more robust. Our code will deploy four ETL tasks and their relationship, as reflected by the image below:</p>
<p><img loading="lazy" src="https://user-images.githubusercontent.com/78096758/217080436-2a286a21-2aff-40ab-9d83-1b874c8e5c6b.png#center" alt="etl examples"  />

<em>High-level architecture of ETL tasks</em></p>
<h3 id="h2understanding-each-etl-task-h2br-"><h2>Understanding Each ETL Task </h2><br /><a hidden class="anchor" aria-hidden="true" href="#h2understanding-each-etl-task-h2br-">#</a></h3>
<p>This workflow supports two different data formats: column or row-based. These different formats require different parsers: parquet for columns and avro for rows. We can also refer to them as batch or stream, respectively.</p>
<p>So, the workflow starts with a request handler that identifies the data type. Based on the value of a data type flag, it passes the job to either the batch or stream parsing tags.</p>
<p>The parsers pass their output to the Load Data task, where it is loaded in persistent storage, based on the source and data type.</p>
<p>While it seems like this is a single workflow with two different code paths, there are advantages to processing both data types in the same set of tasks. Sharing a loading tasks between the data streams makes it easier to manage overlaps and relationships. For example, the columnar data store may need to be updated with foreign keys from new row-based data. It’s also easier to share a single ETL workflow between different teams</p>
<p>Now that we know what we&rsquo;ll be building, let&rsquo;s start by seeing how to implement it using Argo Workflows steps.</p>
<p><img loading="lazy" src="https://user-images.githubusercontent.com/78096758/217080713-bbd06f86-f41d-45bf-87d6-c1981b97776c.png#center" alt="Remember"  />
</p>
<br />
<h3 id="h2build-an-etl-pipeline-using-steps-in-argo-workflowsh2-br-"><h2>Build an ETL Pipeline Using steps in Argo Workflows</h2> <br /><a hidden class="anchor" aria-hidden="true" href="#h2build-an-etl-pipeline-using-steps-in-argo-workflowsh2-br-">#</a></h3>
<p>In this approach, the data pipeline will follow a list of steps to clean and treat the data from your data sources. Your ETL code becomes even more robust when we use conditionals to inform which ETL flow your data should take. Sounds good, right? So, let&rsquo;s get our hands dirty and see how it works in practice.</p>
<p>The code below will create a workflow in a namespace called <code>argo</code>. This namespace must exist before this workflow is executed with <code>argo submit</code>. Doing so will avoid security issues, such as your user not having permission to create namespaces. It will also prevent error messages warning you not to break your Kubernetes deployment. For our example, we&rsquo;ll generate a random value on a Linux machine and load the upcoming data based on this value.</p>
<p>While both parser steps are triggered simultaneously, only the one informed by the handle requests step will execute. Using automated code like this will reduce the chances of having problems with our ETL data flow. Automation on your workflow steps handles common errors such as mismatch types in your database.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">apiVersion: argoproj.io/v1alpha1 
</span></span><span class="line"><span class="cl">kind: Workflow 
</span></span><span class="line"><span class="cl">metadata: 
</span></span><span class="line"><span class="cl">  generateName: stream-batch-parser-  
</span></span><span class="line"><span class="cl">  namespace: argo
</span></span><span class="line"><span class="cl">spec: 
</span></span><span class="line"><span class="cl">  entrypoint: sbp 
</span></span><span class="line"><span class="cl">  templates: 
</span></span><span class="line"><span class="cl">  - name: sbp 
</span></span><span class="line"><span class="cl">    steps: 
</span></span><span class="line"><span class="cl">    - - name: request-handler 
</span></span><span class="line"><span class="cl">        template: edge-input 
</span></span><span class="line"><span class="cl">    - - name: parser-stream 
</span></span><span class="line"><span class="cl">        template: stream 
</span></span><span class="line"><span class="cl">        when: <span class="s2">&#34;{{steps.handle-requests.outputs.result}} &lt;= 163883&#34;</span> 
</span></span><span class="line"><span class="cl">      - name: parser-batch 
</span></span><span class="line"><span class="cl">        template: parquet 
</span></span><span class="line"><span class="cl">        when: <span class="s2">&#34;{{steps.handle-requests.outputs.result}} &gt; 163883&#34;</span> 
</span></span><span class="line"><span class="cl">  - name: stream 
</span></span><span class="line"><span class="cl">    steps: 
</span></span><span class="line"><span class="cl">    - - name: avro-parser 
</span></span><span class="line"><span class="cl">        template: avro 
</span></span><span class="line"><span class="cl">    - - name: wrapper
</span></span><span class="line"><span class="cl">        template: wrapper 
</span></span><span class="line"><span class="cl">  - name: batch 
</span></span><span class="line"><span class="cl">    steps: 
</span></span><span class="line"><span class="cl">    - - name: parquet-parser 
</span></span><span class="line"><span class="cl">        template: parquet 
</span></span><span class="line"><span class="cl">    - - name: wrapper
</span></span><span class="line"><span class="cl">        template: wrapper 
</span></span><span class="line"><span class="cl">  - name: edge-input
</span></span><span class="line"><span class="cl">    container: 
</span></span><span class="line"><span class="cl">      image: alpine:3.6 
</span></span><span class="line"><span class="cl">      command: <span class="o">[</span>sh, -c<span class="o">]</span>
</span></span><span class="line"><span class="cl">      args: <span class="o">[</span><span class="s2">&#34;echo </span><span class="si">${</span><span class="nv">RANDOM</span><span class="si">}</span><span class="s2">&#34;</span><span class="o">]</span> 
</span></span><span class="line"><span class="cl">  - name: parquet 
</span></span><span class="line"><span class="cl">    container: 
</span></span><span class="line"><span class="cl">      image: alpine:3.6 
</span></span><span class="line"><span class="cl">      command: <span class="o">[</span>sh, -c<span class="o">]</span>
</span></span><span class="line"><span class="cl">      args: <span class="o">[</span><span class="s2">&#34;echo \&#34;code to parse Parquet\&#34;&#34;</span><span class="o">]</span> 
</span></span><span class="line"><span class="cl">  - name: avro 
</span></span><span class="line"><span class="cl">    container: 
</span></span><span class="line"><span class="cl">      image: alpine:3.6 
</span></span><span class="line"><span class="cl">      command: <span class="o">[</span>sh, -c<span class="o">]</span>
</span></span><span class="line"><span class="cl">      args: <span class="o">[</span><span class="s2">&#34;echo \&#34;code to parse Avro\&#34;&#34;</span><span class="o">]</span>
</span></span><span class="line"><span class="cl">  - name: wrapper
</span></span><span class="line"><span class="cl">    container: 
</span></span><span class="line"><span class="cl">      image: alpine:3.6 
</span></span><span class="line"><span class="cl">      command: <span class="o">[</span>sh, -c<span class="o">]</span>
</span></span><span class="line"><span class="cl">      args: <span class="o">[</span><span class="s2">&#34;echo \&#34;code to load into Staging\&#34;&#34;</span><span class="o">]</span> 
</span></span></code></pre></div><br />
Save the file above as `etl_steps.yml` and start your workflow with this command:
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">argo submit -n argo etl_steps.yml
</span></span></code></pre></div><p>We can now get our workflow status by executing the following <code>argo get</code> command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">argo get -n argo stream-batch-parser-xxvks
</span></span></code></pre></div><p>‍The last five digits will differ in each environment. And by running the previous command, your output log should be similar to the image below; as stated, our workflow will execute the parser stream task based on the value returned by the handle requests task.</p>
<p>Argo workflow etl steps get output
ETL steps returned by running <code>argo get</code> command
Now that we&rsquo;ve seen how to build an ETL with tasks, let&rsquo;s explore how to use DAGs for your ETL.</p>
<h3 id="h2building-an-etl-pipeline-with-dags-instead-of-steps-h2br-"><h2>Building an ETL Pipeline with DAGs Instead of Steps </h2><br /><a hidden class="anchor" aria-hidden="true" href="#h2building-an-etl-pipeline-with-dags-instead-of-steps-h2br-">#</a></h3>
<p>Now, let&rsquo;s explore how to achieve the same work using DAG templates instead of steps in Argo Workflows. Even though the DSL looks similar at first glance, DAGs give you more power to specify dependencies between steps and run tasks in parallel.</p>
<p>In a DAG, any task can run when its dependencies are satisfied. If more than one task’s dependencies are fulfilled, all of them will run in parallel. If a task has no dependencies, it will run as soon as the workflow is started. DAGs are excellent for processing ETL, and I strongly suggest you familiarize yourself with all options that a DAG task can provide by looking at the <a href="https://argoproj.github.io/argo-workflows/fields/#dagtask">Argo official documentation</a>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">apiVersion: argoproj.io/v1alpha1
</span></span><span class="line"><span class="cl">kind: Workflow
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">  generateName: dag-orchestrate-
</span></span><span class="line"><span class="cl">  namespace: argo
</span></span><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">  entrypoint: sbp
</span></span><span class="line"><span class="cl">  archiveLogs: <span class="nb">true</span>
</span></span><span class="line"><span class="cl">  templates:
</span></span><span class="line"><span class="cl">    - name: sbp
</span></span><span class="line"><span class="cl">      dag:
</span></span><span class="line"><span class="cl">        tasks:
</span></span><span class="line"><span class="cl">          - name: request-handler
</span></span><span class="line"><span class="cl">            template: edge-input
</span></span><span class="line"><span class="cl">          - name: stream-flow
</span></span><span class="line"><span class="cl">            template: stream
</span></span><span class="line"><span class="cl">            when: <span class="s2">&#34;{{tasks.handle-requests.outputs.result}} &lt;= 163883&#34;</span> 
</span></span><span class="line"><span class="cl">            depends: handle-requests
</span></span><span class="line"><span class="cl">          - name: batch-flow
</span></span><span class="line"><span class="cl">            template: batch
</span></span><span class="line"><span class="cl">            when: <span class="s2">&#34;{{tasks.handle-requests.outputs.result}} &gt; 163883&#34;</span> 
</span></span><span class="line"><span class="cl">            depends: handle-requests
</span></span><span class="line"><span class="cl">    - name: stream 
</span></span><span class="line"><span class="cl">      steps: 
</span></span><span class="line"><span class="cl">      - - name: avro-parser 
</span></span><span class="line"><span class="cl">          template: avro 
</span></span><span class="line"><span class="cl">      - - name: wrapper
</span></span><span class="line"><span class="cl">          template: wrapper 
</span></span><span class="line"><span class="cl">    - name: batch 
</span></span><span class="line"><span class="cl">      steps: 
</span></span><span class="line"><span class="cl">      - - name: parquet-parser 
</span></span><span class="line"><span class="cl">          template: parquet 
</span></span><span class="line"><span class="cl">      - - name: wrapper
</span></span><span class="line"><span class="cl">          template: wrapper             
</span></span><span class="line"><span class="cl">    - name: edge-input
</span></span><span class="line"><span class="cl">      container: 
</span></span><span class="line"><span class="cl">        image: alpine:3.6 
</span></span><span class="line"><span class="cl">        command: <span class="o">[</span>sh, -c<span class="o">]</span>
</span></span><span class="line"><span class="cl">        args: <span class="o">[</span><span class="s2">&#34;echo </span><span class="si">${</span><span class="nv">RANDOM</span><span class="si">}</span><span class="s2">&#34;</span><span class="o">]</span> 
</span></span><span class="line"><span class="cl">    - name: parquet 
</span></span><span class="line"><span class="cl">      container: 
</span></span><span class="line"><span class="cl">        image: alpine:3.6 
</span></span><span class="line"><span class="cl">        command: <span class="o">[</span>sh, -c<span class="o">]</span>
</span></span><span class="line"><span class="cl">        args: <span class="o">[</span><span class="s2">&#34;echo \&#34;code to parse Parquet\&#34;&#34;</span><span class="o">]</span> 
</span></span><span class="line"><span class="cl">    - name: avro 
</span></span><span class="line"><span class="cl">      container: 
</span></span><span class="line"><span class="cl">        image: alpine:3.6 
</span></span><span class="line"><span class="cl">        command: <span class="o">[</span>sh, -c<span class="o">]</span>
</span></span><span class="line"><span class="cl">        args: <span class="o">[</span><span class="s2">&#34;echo \&#34;code to parse Avro\&#34;&#34;</span><span class="o">]</span>   
</span></span><span class="line"><span class="cl">    - name: wrapper
</span></span><span class="line"><span class="cl">      container: 
</span></span><span class="line"><span class="cl">        image: alpine:3.6 
</span></span><span class="line"><span class="cl">        command: <span class="o">[</span>sh, -c<span class="o">]</span>
</span></span><span class="line"><span class="cl">        args: <span class="o">[</span><span class="s2">&#34;echo \&#34;code to load into Staging\&#34;&#34;</span><span class="o">]</span> 
</span></span></code></pre></div><p>Save the file above as <code>etl_dag.yml</code> and submit your workflow to start it:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">argo submit -n argo etl_dag.yml
</span></span></code></pre></div><p>As demonstrated below, you can check its evolution with argo get:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">argo get -n argo dag-orchestrate-ctpkl
</span></span></code></pre></div><p>‍In this scenario, our workflow executed the batch stream task instead of the stream-flow based on the value returned by the handle requests task.</p>
<br />
<p><img loading="lazy" src="https://user-images.githubusercontent.com/78096758/217081047-efbc4d79-5f38-4e9f-8c2c-d00be60521ea.png#center" alt="etl examples"  />

<br /></p>
<p>Congratulations on your work! You can now design your ETL data flows using a DAG or structured list of steps within Argo Workflows.</p>
<p>Don&rsquo;t forget to clean your environment with <code>argo delete -n argo your-workflow</code>, where you should inform the desired workflow as <code>your-workflow</code>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">argo delete -n argo your-workflow
</span></span></code></pre></div><h3 id="h2conclusion-h2br-"><h2>Conclusion </h2><br /><a hidden class="anchor" aria-hidden="true" href="#h2conclusion-h2br-">#</a></h3>
<p>While it&rsquo;s commonly used for infrastructure management, Argo Workflows can also orchestrate your ETL tasks. Using it like this removes the need for different tools to achieve the same goal, i.e. Argo for CI/CD and Airflow for ETL jobs.</p>
<p>The DAG approach is often better than the steps approach for running ETL pipelines. For starters, DAG task processing is optimized at runtime. You&rsquo;ll have fewer decision points for some of your pipelines simply by informing the desired data flow.</p>
<p>For simple tasks, sequential flows (as you get with the steps approach in Argo Workflows) work fine. However, they become harder to maintain in cases where you need to target a subset of your data flow and manage complex dependencies over time.</p>
<p>Another perk of using DAGs is to specify the exact step at runtime. Running it gives you more liberty to create conditional code with fewer indented loops while optimizing the code and the infrastructure resources.</p>
<p>I urge you to go deeper into Argo Workflows&rsquo; documentation around DAGs. Mastering how DAGs work can increase the quality of your ETL pipelines, allowing you to manage your ETL tasks more dynamically compared to the steps method.</p>
<p>For more optimized ways of managing your Kubernetes resources, explore how Pipekit can help you orchestrate your whole Argo Workflows deployment.</p>
<p>See you next time!</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://www.dawrlog.com/tags/kubernetes/">Kubernetes</a></li>
      <li><a href="https://www.dawrlog.com/tags/argo/">Argo</a></li>
      <li><a href="https://www.dawrlog.com/tags/development/">Development</a></li>
      <li><a href="https://www.dawrlog.com/tags/data-warehouse/">Data Warehouse</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://www.dawrlog.com/posts/13/">
    <span class="title">« Prev</span>
    <br>
    <span>gRPC vs REST: Which Is Right for Your Application and Why?</span>
  </a>
  <a class="next" href="https://www.dawrlog.com/posts/6/">
    <span class="title">Next »</span>
    <br>
    <span>Top 10 Argo Workflows Examples.</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Argo Workflow ETL Examples. on twitter"
        href="https://twitter.com/intent/tweet/?text=Argo%20Workflow%20ETL%20Examples.&amp;url=https%3a%2f%2fwww.dawrlog.com%2fposts%2f5%2f&amp;hashtags=Kubernetes%2cArgo%2cDevelopment%2cDataWarehouse">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Argo Workflow ETL Examples. on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fwww.dawrlog.com%2fposts%2f5%2f&amp;title=Argo%20Workflow%20ETL%20Examples.&amp;summary=Argo%20Workflow%20ETL%20Examples.&amp;source=https%3a%2f%2fwww.dawrlog.com%2fposts%2f5%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Argo Workflow ETL Examples. on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fwww.dawrlog.com%2fposts%2f5%2f&title=Argo%20Workflow%20ETL%20Examples.">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Argo Workflow ETL Examples. on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fwww.dawrlog.com%2fposts%2f5%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Argo Workflow ETL Examples. on whatsapp"
        href="https://api.whatsapp.com/send?text=Argo%20Workflow%20ETL%20Examples.%20-%20https%3a%2f%2fwww.dawrlog.com%2fposts%2f5%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Argo Workflow ETL Examples. on telegram"
        href="https://telegram.me/share/url?text=Argo%20Workflow%20ETL%20Examples.&amp;url=https%3a%2f%2fwww.dawrlog.com%2fposts%2f5%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://www.dawrlog.com/">Dataware Logistics</a>; <br /> created with 🧡 on 🐧 machines running ☸️ pods.</span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a><script src='https://storage.ko-fi.com/cdn/scripts/overlay-widget.js'></script>
<script>
  kofiWidgetOverlay.draw('dawrlog', {
    'type': 'floating-chat',
    'floating-chat.donateButton.text': 'Tip jar',
    'floating-chat.donateButton.background-color': '#794bc4',
    'floating-chat.donateButton.text-color': '#fff'
  });
</script>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
