<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Lean Agile on Dataware Logistics</title>
    <link>https://www.dawrlog.com/categories/lean-agile/</link>
    <description>Recent content in Lean Agile on Dataware Logistics</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Wed, 24 Nov 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://www.dawrlog.com/categories/lean-agile/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>How to Value Stream DataOps?</title>
      <link>https://www.dawrlog.com/posts/12/</link>
      <pubDate>Wed, 24 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.dawrlog.com/posts/12/</guid>
      <description>Hello everyone, today we will check how to properly value stream your DataOps initiative.
Enhancements on data ingestion made evident the amount of data lost when generating insights. However, without guidance from methodologies like The DataOps Manifesto, some companies are still struggling to blend data pipelines from legacy databases, such as an ADABAS database, with new ones, such as MongoDB or Neo4j.
And it’s always good to point out that these legacy systems aren’t easy to let go of.</description>
    </item>
    
  </channel>
</rss>
